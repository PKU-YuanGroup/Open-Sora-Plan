<!DOCTYPE html>
<html lang="Chinese"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Open-Sora-Plan-CN</title>
<meta name="generator" content="Jekyll v3.9.4" />
<meta property="og:title" content="Open-Sora-Plan" />
<meta name="author" content="Bin Lin, Shenghai Yuan, Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Liuhan Chen,  Yang Ye, Bin Zhu, Yunyang Ge, Xing Zhou, Yonghong Tian, Li Yuan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="" />
<meta property="og:description" content="" />
<!-- <link rel="canonical" href="https://llava-vl.github.io/blog/2024-01-30-llava-next/" />
<meta property="og:url" content="https://llava-vl.github.io/blog/2024-01-30-llava-next/" /> -->
<meta property="og:site_name" content="Open Sora" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-30T12:33:38-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Open Sora Plan" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Bin Lin, Shenghai Yuan, Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Liuhan Chen,  Yang Ye, Bin Zhu, Yunyang Ge, Xing Zhou, Yonghong Tian, Li Yuan"},"dateModified":"2024-01-30T12:33:38-06:00","datePublished":"2024-01-30T12:33:38-06:00","description":"","headline":"","mainEntityOfPage":{"@type":"WebPage","@id":"https://llava-vl.github.io/blog/2024-01-30-llava-next/"},"url":"https://llava-vl.github.io/blog/2024-01-30-llava-next/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="./assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://llava-vl.github.io/blog/feed.xml" title="LLaVA" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">中文</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Open Sora Plan</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-01-30T12:33:38-06:00" itemprop="datePublished">Mar 1, 2024
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">林彬, 袁盛海, 唐振宇, 张俊武, 程鑫华, 陈柳汉, 叶阳, 朱斌, 葛云阳, 周星, 董少灵, 史业民, 田永鸿, 袁粒</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!-- for mathjax support -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
      },
      TeX: {
        equationNumbers: { autoNumber: "AMS" }
      }
    });
  </script>

<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<strong><a href="https://pku-yuangroup.github.io/Open-Sora-Plan/" target="_blank">[英文]</a> </strong> 
<strong><a href="https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html" target="_blank">[中文]</a> </strong> 
<strong><a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan" target="_blank">[Github]</a> </strong> 
  <!-- <li><a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v16" target="_blank">Model</a></li> --> </li>


<p>在2024年2月, OpenAI最近推出了<a href="https://openai.com/sora">Sora</a>, 这是一个开创性的文本到视频模型, 代表了视频生成技术的一个重大飞跃。Sora能够将简短的文本描述转换成详细的、高清的视频片段, 这些片段最长可达一分钟。所提出的<a href="https://openai.com/sora">Sora</a>推进了AI技术, 并在视频制作中提供了新的创造潜能。</p>

<p><strong>
  今天, 我们非常兴奋地推出我们的Open Sora项目, 旨在复现OpenAI的视频生成模型, 得到国内外网友的一致期待。</strong></p>
<p align="center">
    <img width="80%" src="./assets/twitter.png" />
    <br />
  </p>
<p><strong>下面, 我们将介绍我们的框架, 它由以下组成部分组成。</strong></p>

<ol>
  <li><strong>Video VQ-VAE</strong>. </li>
  <li><strong>Denoising Diffusion Transformer</strong>.</li>
  <li><strong>Condition Encoder</strong>. </li>
</ol>
<p align="center">
  <img width="90%" src="./assets/framework.png" />
  <br />
  <em>框架图</em>
</p>
<!-- <p>Along with performance improvements, <strong>LLaVA-NeXT maintains the minimalist design and data efficiency of LLaVA-1.5</strong>. It re-uses the pretrained connector of LLaVA-1.5, and still uses less than 1M visual instruction tuning samples. The largest 34B variant finishes training in ~1 day with 32 A100s. <strong>Code, data, model will be made publicly available.</strong></p> -->

<!-- <h2 id="open-source-release">Open-Source Release</h2> -->

<!-- <p>We open-source the LLaVA-NeXT to facilitate future development of LMM in the community. Code, data, model will be made publicly available.</p> -->

<!-- <ul>
  <li><strong><a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan" target="_blank">Code</a></strong></li>
  <li><a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v16" target="_blank">Model</a></li> -->
  <!-- <li><strong>Data: Coming soon.</strong></li>
</ul> -->

<!-- <h2 id="results">Results</h2>

<div>
  <span style="display: inline-block; padding: 4px 10px; margin: 5px; border-radius: 3px; background-color: rgba(249, 242, 248, 1);">Open-Source</span>
  <span style="display: inline-block; padding: 4px 10px; margin: 5px; border-radius: 3px; background-color: rgba(117, 209, 215, 0.1);">Proprietary</span>
</div>

<table>
  <tr><th>Data (PT)</th><th>Data (IT)</th><th>Model</th><th>MMMU (val)</th><th>Math-Vista</th><th>MMB-ENG</th><th>MMB-CN</th><th>MM-Vet</th><th>LLaVA-Wild</th><th>SEED-IMG</th></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>N/A</td><td>N/A</td><td>GPT-4V</td><td>56.8</td><td>49.9</td><td>75.8</td><td>73.9</td><td>67.6</td><td>-</td><td>71.6</td></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>N/A</td><td>N/A</td><td>Gemini Ultra</td><td>59.4</td><td>53</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>N/A</td><td>N/A</td><td>Gemini Pro</td><td>47.9</td><td>45.2</td><td>73.6</td><td>74.3</td><td>64.3</td><td>-</td><td>70.7</td></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>1.4B</td><td>50M</td><td>Qwen-VL-Plus</td><td>45.2</td><td>43.3</td><td>-</td><td>-</td><td>55.7</td><td>-</td><td>65.7</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>1.5B</td><td>5.12M</td><td>CogVLM-30B</td><td>32.1</td><td>-</td><td>-</td><td>-</td><td>56.8</td><td>-</td><td>-</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>125M</td><td>~1M</td><td>Yi-VL-34B</td><td>45.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>558K</td><td>665K</td><td>LLaVA-1.5-13B</td><td>36.4</td><td>27.6</td><td>67.8</td><td>63.3</td><td>36.3</td><td>72.5</td><td>68.2</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>558K</td><td>760K</td><td>LLaVA-NeXT-34B</td><td>51.1</td><td>46.5</td><td>79.3</td><td>79</td><td>57.4</td><td>89.6</td><td>75.9</td></tr>
</table> --> 

<!-- <details>
<summary><b><em>For more results, please unfold to see expanded tables</em></b></summary>

<i>More benchmarks will be added soon.</i>

<div>
  <span style="display: inline-block; padding: 4px 10px;">$^\dagger$specialist</span>
  <span style="display: inline-block; padding: 4px 10px; color: gray">Grey: supervised finetuned</span>
  <span style="display: inline-block; padding: 4px 10px;">$^*$training <u>image</u> observed</span>
  <span style="display: inline-block; padding: 4px 10px;">Normal text: zero-shot</span>
</div>

<table>
  <tr><th>Model</th><th>VQAv2</th><th>GQA</th><th>VisWiz</th><th>TextVQA</th><th>ScienceQA</th></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>GPT-4V</td><td>77.2</td><td>-</td><td>-</td><td>78.0</td><td>-</td></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>Gemini Ultra</td><td>77.8</td><td>-</td><td>-</td><td>82.3</td><td>-</td></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>Gemini Pro</td><td>71.2</td><td>-</td><td>-</td><td>74.6</td><td>-</td></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>PALI-X</td><td style="color: gray">86.0$^\dagger$</td><td>-</td><td style="color: gray">70.9$^\dagger$</td><td style="color: gray">71.4$^\dagger$</td><td>-</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>CogVLM-30B</td><td style="color: gray">83.4 (84.7$^\dagger$)</td><td style="color: gray">65.2$^\dagger$</td><td style="color: gray">76.4$^\dagger$</td><td style="color: gray">68.1 (69.3$^\dagger$)</td><td style="color: gray">92.7$^\dagger$</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>LLaVA-1.5-13B</td><td style="color: gray">80</td><td style="color: gray">63.3</td><td>53.6</td><td>61.3$^*$</td><td>71.6</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>LLaVA-NeXT-Vicuna-7B</td><td style="color: gray">81.8</td><td style="color: gray">64.2</td><td>57.6</td><td>64.9</td><td>70.1</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>LLaVA-NeXT-Vicuna-13B</td><td style="color: gray">82.8</td><td style="color: gray">65.4</td><td>60.5</td><td>67.1</td><td>73.6</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>LLaVA-NeXT-Mistral-7B</td><td style="color: gray">82.2</td><td style="color: gray">64.8</td><td>60.0</td><td>65.7</td><td>72.8</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>LLaVA-NeXT-34B</td><td style="color: gray">83.7</td><td style="color: gray">67.1</td><td>63.8</td><td>69.5</td><td>81.8</td></tr>
</table>


<table>
  <tr><th>Data (PT)</th><th>Data (IT)</th><th>Model</th><th>MMMU (val)</th><th>MMMU (test)</th><th>MathVista</th><th>MMB-ENG</th><th>MMB-CN</th><th>MM-Vet</th><th>LLaVA-Wild</th><th>SEED-IMG</th><th>MME</th><th>POPE</th></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>N/A</td><td>N/A</td><td>GPT-4V</td><td>56.8</td><td>55.7</td><td>49.9</td><td>75.8</td><td>73.9</td><td>67.6</td><td>-</td><td>71.6</td><td>-</td><td>-</td></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>N/A</td><td>N/A</td><td>Gemini Ultra</td><td>59.4</td><td>-</td><td>53</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>N/A</td><td>N/A</td><td>Gemini Pro</td><td>47.9</td><td>-</td><td>45.2</td><td>73.6</td><td>74.3</td><td>64.3</td><td>-</td><td>70.7</td><td>-</td><td>-</td></tr>
   <tr style="background-color: rgba(117, 209, 215, 0.1);"><td>1.4B</td><td>50M</td><td>Qwen-VL-Plus</td><td>45.2</td><td>40.8</td><td>43.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr> 
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>1.5B</td><td>5.12M</td><td>CogVLM-30B</td><td>32.1</td><td>30.1</td><td>-</td><td>-</td><td>-</td><td>56.8</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>558K</td><td>665K</td><td>LLaVA-1.5-13B (336$^2$)</td><td>36.4</td><td>33.6</td><td>27.6</td><td>67.8</td><td>63.3</td><td>36.3</td><td>72.5</td><td>68.2</td><td>1531/295</td><td>85.93</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>558K</td><td>760K</td><td>LLaVA-NeXT-Vicuna-7B (672$^2$)</td><td>35.8</td><td>-</td><td>34.6</td><td>67.4</td><td>60.6</td><td>43.9</td><td>81.6</td><td>70.2</td><td>1519/332</td><td>86.53</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>558K</td><td>760K</td><td>LLaVA-NeXT-Mistral-7B (672$^2$)</td><td>35.3</td><td>-</td><td>37.7</td><td>68.7</td><td>61.2</td><td>47.3</td><td>83.2</td><td>72.2</td><td>1498/321</td><td>86.73</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>558K</td><td>760K</td><td>LLaVA-NeXT-13B (672$^2$)</td><td>36.2</td><td>-</td><td>35.3</td><td>70</td><td>64.4</td><td>48.4</td><td>87.3</td><td>71.9</td><td>1575/326</td><td>86.23</td></tr>
  <tr style="background-color: rgba(249, 242, 248, 1);"><td>558K</td><td>760K</td><td>LLaVA-NeXT-34B (672$^2$)</td><td>51.1</td><td>44.7</td><td>46.5</td><td>79.3</td><td>79</td><td>57.4</td><td>89.6</td><td>75.9</td><td>1631/397</td><td>87.73</td></tr>
</table>

</details> -->

<!-- <div>&nbsp;</div> -->

<!-- <p>✨ Highlights:</p>
<ul>
  <li><strong>SoTA Performance!</strong> LLaVA-NeXT achieves the best performance compared with open-source LMMs such as <a href="https://github.com/THUDM/CogVLM" target="_blank">CogVLM</a> or <a href="https://huggingface.co/01-ai/Yi-VL-34B" target="_blank">Yi-VL</a>. Compared with commercial ones, it catches up to Gemini Pro and outperforms <a href="https://huggingface.co/spaces/Qwen/Qwen-VL-Plus" target="_blank">Qwen-VL-Plus</a> on selected benchmarks.</li>
  <li><strong>Zero-shot Chinese capability</strong>. LLaVA-NeXT’s Chinese capability is an emerging zero-shot capability (i.e., only English multimodal data is considered). Its performance on Chinese multimodal scenarios is surprisingly good, e.g., SoTA on MMBench-CN.</li>
  <li><strong>Low Training Cost</strong>. LLaVA-NeXT is trained with 32 GPUs for ~1 day, with 1.3M data samples in total. The compute / training data cost is 100-1000 times smaller than others.</li>
</ul> -->

<!-- <h3 id="qualitative-results">Qualitative Results</h3> -->

<!-- <details>
<summary><b>Mark Zuckerberg talks about Llama-3 and 600K H100s</b></summary>

<div style="display: block; padding: 4px 10px; margin: 5px; border-radius: 3px; background-color: rgba(117, 209, 215, 0.1);">

  <h4>User: Explain this to me in detail.</h4>

  <img width="90%" src="/blog/assets/images/llava-1-6/zuck_llama3.jpg" />

</div>

<div style="display: block; padding: 4px 10px; margin: 5px; border-radius: 3px; background-color: rgba(249, 242, 248, 1);">

  <h4>LLaVA-NeXT-34B</h4>

  <p>The image you've provided appears to be a screenshot of a social media post, specifically from Instagram, featuring a man speaking and a text overlay. The man in the image is wearing a black t-shirt and appears to be in the middle of a conversation or presentation. The text overlay is a quote from a post by a user named "zuck," which is presumably a reference to Mark Zuckerberg, the CEO of Meta (formerly known as Facebook).</p>

  <p>The text overlay reads:</p>

  <p>"Some updates on our AI efforts. Our long term vision is to build general intelligence, open source it responsibly, and make it widely available so everyone can benefit. We're bringing our two major AI research efforts (FAIR and GenAI) closer together to support this. We're currently training our next-gen model Llama 3, and we're building massive compute infrastructure to support our future roadmap, including 35k H100s by the end of this year — and overall almost 600k H100s equivalents of compute if you include other GPUs. Also really excited about our progress building new AI-centric computing devices like Ray Ban Meta smart glasses. Lots more to come."</p>

  <p>This text provides insights into Meta's artificial intelligence (AI) initiatives. It mentions that the company's long-term goal is to develop general intelligence and make it accessible to everyone. The post also discusses the integration of Meta's two AI research efforts, FAIR (Facebook AI Research) and GenAI, to support this goal.</p>

  <p>The text also mentions the training of a next-generation AI model called Llama 3 and the construction of massive compute infrastructure to support Meta's future plans. It highlights the company's investment in NVIDIA's H100 GPUs, with a total of 35,000 H100s by the end of the year and almost 600,000 H100s equivalents of compute if other GPUs are included.</p>

  <p>Lastly, the post expresses excitement about Meta's progress in developing AI-centric computing devices, specifically Ray Ban Meta smart glasses. The post concludes with a statement that there is more to come, suggesting that Meta has additional AI-related announcements or developments in the pipeline.</p>

  <p>The image of the man speaking could be from a video or a live stream where he is discussing these AI updates. The context of the image suggests that it is part of a larger conversation or announcement related to Meta's AI strategy and advancements.</p>

</div>

</details> -->

<!-- <details>
<summary><b>When to leave to pickup based on flight info?</b></summary>

<div style="display: block; padding: 4px 10px; margin: 5px; border-radius: 3px; background-color: rgba(117, 209, 215, 0.1);">

  <h4>User: I need to pick up my wife. I live in San Jose. When should I leave?</h4>

  <img width="90%" src="/blog/assets/images/llava-1-6/ewr_sfo_flight.png" />

</div>

<div style="display: block; padding: 4px 10px; margin: 5px; border-radius: 3px; background-color: rgba(249, 242, 248, 1);">

  <h4>LLaVA-NeXT-34B</h4>

  <p>Based on the information provided in the image, the flight is scheduled to arrive at 11:51 AM at San Francisco International Airport (SFO). If you live in San Jose, you should consider the travel time between San Jose and San Francisco, which is approximately 45-60 minutes depending on traffic conditions.</p>

  <p>To ensure you have enough time to pick up your wife, you should leave San Jose no later than 11:00 AM to account for traffic and any unexpected delays. However, it's always a good idea to leave earlier than necessary to ensure you arrive on time.</p>

</div>

</details> -->
<p><br /></p>

<h2 id="detailed-technical-improvement">实现细节</h2>

<!-- <p>We detail the findings we have and improvements we make to LLaVA-1.5. <em>More implementation details will be released and documented here in the coming days.</em></p> -->

<!-- <h3 id="1-dynamic-high-resolution">(1) Dynamic High Resolution</h3>

<p>We design our model at high resolution with an aim to <strong>preserve its data efficiency</strong>.
When provided with high-resolution images and representations that preserve these details, the model’s capacity to perceive intricate details in an image is significantly improved. It reduces the model hallucination that conjectures the imagined visual content when confronted with low-resolution images. Our ‘AnyRes’ technique is designed to accommodate images of various high resolutions. We employ a grid configuration of $\{2 \times 2, 1 \times \{2,3,4\}, \{2,3,4\} \times 1\}$, balancing performance efficiency with operational costs. See our <a href="https://static.hliu.cc/files/llava/improved_llava.pdf">updated LLaVA-1.5 technical report</a> for more details.</p>

<p align="center">
  <img width="90%" src="/blog/assets/images/llava-1-6/high_res_arch_v2.png" />
  <br />
  <em>Illustration of dynamic high resolution scheme: a grid configuration of $\\{2 \times 2\\}$</em>
</p> -->

<h3 id="1-Variable Aspect Ratios">(1)可变长宽比</h3>
<p>我们参考<a href="https://arxiv.org/pdf/2402.12376.pdf">FIT</a>实施了一种动态掩码策略, 以并行批量训练的同时保持灵活的长宽比。具体来说, 我们将高分辨率视频在保持长宽比的同时下采样至最长边为256像素, 然后在右侧和底部用零填充至一致的256x256分辨率。这样便于videovae以批量编码视频, 以及便于扩散模型使用注意力掩码对批量潜变量进行去噪。</p>
<p align="center">
  <img width="80%" src="./assets/mask.png" />
  <br />
  <em>动态训练策略</em>
</p>
<!-- <ul>
  <li><strong>High-quality User Instruct Data</strong>. We implement a dynamic masking stategy for batch training in parallel while maintaining flexible aspect ratios. Specifically, we resize high-resolution videos to make their longest side 256 pixels, maintaining aspect ratios, and then pad them with zeros on the right and bottom to achieve a consistent 256x256 resolution. This facilitates videovae to encode videos in batches and diffusion model to denoise batches of latents with their own attention masks.</li>
  <li><strong>Multimodal Document/Chart Data</strong>. During inferencing, we use Position Interpolation[xx] to enable variable resolution sampling, despite training on a fixed 256x256 resolution. We downscale the position indices of the variable-resolution noist latents from [0, seq_length-1] to [0, 255] to aligning them with the pretrained range. This adjustment enables the attention-based diffusion model to handle sequences of higher resolutions.
  </li>
</ul> -->

<h3 id="2-Variable Resolutions">(2) 可变分辨率</h3>
<p>在推理过程中, 尽管我们在固定的256x256分辨率上进行训练, 但我们使用<a href="https://arxiv.org/pdf/2306.15595.pdf">位置插值</a>可以实现可变分辨率采样。我们将可变分辨率噪声潜变量的位置索引从[0, seq_length-1]下调到[0, 255]，以使其与预训练范围对齐。这种调整使得基于注意力的扩散模型能够处理更高分辨率的序列。</p>
<!-- <ul>
  <li><strong>High-quality User Instruct Data</strong>. We implement a dynamic masking stategy for batch training in parallel while maintaining flexible aspect ratios. Specifically, we resize high-resolution videos to make their longest side 256 pixels, maintaining aspect ratios, and then pad them with zeros on the right and bottom to achieve a consistent 256x256 resolution. This facilitates videovae to encode videos in batches and diffusion model to denoise batches of latents with their own attention masks.</li>
  <li><strong>Multimodal Document/Chart Data</strong>. During inferencing, we use Position Interpolation[xx] to enable variable resolution sampling, despite training on a fixed 256x256 resolution. We downscale the position indices of the variable-resolution noist latents from [0, seq_length-1] to [0, 255] to aligning them with the pretrained range. This adjustment enables the attention-based diffusion model to handle sequences of higher resolutions.
  </li>
</ul> -->

<h3 id="2-Variable durations">(3) 可变时长 </h3>

<!-- <p>In addition to Vicuna-1.5 (<a href="https://huggingface.co/lmsys/vicuna-7b-v1.5">7B</a> and <a href="https://huggingface.co/lmsys/vicuna-13b-v1.5">13B</a>), we consider more LLMs, including <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral-7B</a> and <a href="https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B">Nous-Hermes-2-Yi-34B</a>. These LLMs possess nice properties, flexible commercial use terms, strong bilingual support and larger language model capacity. It allows LLaVA to support a wider spectrum of users and more scenarios in the community. The LLaVA recipe works well with various LLMs, and scales up smoothly with the LLM up to 34B.</p> -->
<p>我们使用<a href="https://arxiv.org/abs/2104.10157">VideoGPT</a>中的Video VQ-VAE, 将视频压缩至潜在空间, 并且支持变时长生成。同时, 我们扩展空间位置插值至时空维度, 实现对变时长视频的处理。</p>

<div class="container">
  <div class="video-container">
    <video controls loop>
      <source src="./assets/10s.mp4" type="video/mp4" >
    </video>
    <!-- <video width="80%" src="./assets/8s.mp4" autoplay loop playsinline muted type="video/mp4"></video> -->
    <p class="caption">10s视频重建(256x)</p>
  </div>
  <div class="video-container">
    <video controls loop>
      <source src="./assets/new.mp4" type="video/mp4"  >
    </video>
  <!-- <video width="80%" src="./assets/8s.mp4" autoplay loop playsinline muted type="video/mp4"></video> -->
  <p class="caption">18s视频重建(196x)</p>
   </div>
</p>
</div>

<!-- <h2 id="model-card">Model Card</h2>

<table>
  <tr><th colspan="2">Name</th><th>LLaVA-NeXT-7B</th><th>LLaVA-NeXT-13B</th><th>LLaVA-NeXT-34B</th></tr>
  <tr><th rowspan="4">Model Size</th><td>Total</td><td><b>7.06B</b></td><td><b>13.35B</b></td><td><b>34.75B</b></td></tr>
  <tr><td>Vision Encoder</td><td>303.5M</td><td>303.5M</td><td>303.5M</td></tr>
  <tr><td>Connector</td><td>21M</td><td>31.5M</td><td>58.7M</td></tr>
  <tr><td>LLM</td><td>6.74B</td><td>13B</td><td>34.39B</td></tr>
  <tr><th colspan="2">Resolution</th><td colspan="3">336 x [(2,2), (1,2), (2,1), (1,3), (3,1), (1,4), (4,1)]</td></tr>
  <tr><th>Stage-1</th><th>Training Data</th><td colspan="3">558K</td></tr>
  <tr><th></th><th>Trainable Module</th><td colspan="3">Connector</td></tr>
  <tr><th>Stage-2</th><th>Training Data</th><td colspan="3">760K</td></tr>
  <tr><th></th><th>Trainable Module</th><td colspan="3">Full model</td></tr>
  <tr><th colspan="2">Compute (#GPU x #Hours)</th><td>8x20</td><td>16x24</td><td>32x30</td></tr>
  <tr><th colspan="2">Training Data (#Samples)</th><td colspan="3">1318K</td></tr>
</table> -->

<h2 id="team">团队</h2> 

<ul>
  <li>林彬: 北京大学</li>
  <li>袁盛海: 北京大学</li>
  <li>唐振宇: 北京大学</li>
  <li>张俊武: 北京大学</li>
  <li>程鑫华: 北京大学</li>
  <li>陈柳汉: 北京大学</li>
  <li>叶阳: 北京大学</li>
  <li>朱斌: 北京大学</li>
  <li>葛云阳: 北京大学</li>
  <li>周星: 兔展AI</li>
  <li>董少灵: 兔展AI</li>
  <li>史业民: 兔展AI顾问</li>
  <li>田永鸿: 北京大学</li>
  <li><a href="https://yuanli2333.github.io/">袁粒</a>: 北京大学</li>
</ul>
    
<h2 id="acknowledgement">贡献者</h2>
<p>这个项目旨在创建一个简单且可扩展的仓库, 以尽可能复现<a href="https://openai.com/sora">Sora</a>。然而, 我们的资源有限, 我们深切希望开源社区能够为这个复现项目做出贡献。 </p>
<div class="centered-images">
  <img src="./assets/we_want_you.jpg" alt="图片1" style="width: 200px;">
  <img src="./assets/sora.png" alt="图片2" style="width: 200px;">
</div>
    
<!-- <h2 id="acknowledgement">Acknowledgement</h2> -->

<!-- <ul>
  <li>A16Z Open Source AI Grants Program.</li>
  <li>We thank Lianmin Zheng, Ying Sheng, Shiyi Cao for the integration of LLaVA to SGLang.</li>
  <li>This work was supported in part by NSF CAREER IIS2150012, Microsoft Accelerate Foundation Models Research, and Institute of Information &amp; communications Technology Planning &amp; Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training).</li>
</ul> -->

<!-- <h2 id="citation">Citation</h2> -->

<!-- <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">openai</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Sora: Creating video from text}</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">{https://openai.com/sora}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>

<span class="nc">@misc</span><span class="p">{</span><span class="nl">peebles2023scalable</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Scalable diffusion models with transformers}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Peebles, William and Xie, Saining}</span><span class="p">,</span>
      <span class="na">publisher</span><span class="p">=</span><span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span>

<span class="nc">@misc</span><span class="p">{</span><span class="nl">chen2023extending</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Extending context window of large language models via positional interpolation}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong}</span><span class="p">,</span>
      <span class="na">publisher</span><span class="p">=</span><span class="s">{arXiv:2306.15595}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span>

<span class="nc">@misc</span><span class="p">{</span><span class="nl">yan2021videogpt</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Videogpt: Video generation using vq-vae and transformers}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Yan, Wilson and Zhang, Yunzhi and Abbeel, Pieter and Srinivas, Aravind}</span><span class="p">,</span>
      <span class="na">publisher</span><span class="p">=</span><span class="s">{arXiv:2104.10157}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span> -->
</code></pre></div></div>

  </div><a class="u-url" href="/blog/2024-01-30-llava-next/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Open-Sora</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Open-Sora</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
