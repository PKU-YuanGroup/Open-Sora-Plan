[2024-09-05 19:39:28,530] torch.distributed.run: [WARNING] 
[2024-09-05 19:39:28,530] torch.distributed.run: [WARNING] *****************************************
[2024-09-05 19:39:28,530] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-05 19:39:28,530] torch.distributed.run: [WARNING] *****************************************
[2024-09-05 19:39:34,260] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:209: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
[2024-09-05 19:39:34,466] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-09-05 19:39:34,501] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-05 19:39:34,542] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-05 19:39:34,598] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-05 19:39:34,666] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-09-05 19:39:34,677] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-05 19:39:34,718] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
pid 400's current affinity list: 0-191
pid 400's new affinity list: 48-71
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
The npu_config.on_npu is True
pid 398's current affinity list: 0-191
pid 398's new affinity list: 0-23
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
pid 403's current affinity list: 0-191
pid 403's new affinity list: 120-143
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
pid 404's current affinity list: 0-191
pid 404's new affinity list: 144-167
pid 401's current affinity list: 0-191
pid 401's new affinity list: 72-95
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
pid 402's current affinity list: 0-191
pid 402's new affinity list: 96-119
pid 405's current affinity list: 0-191
pid 405's new affinity list: 168-191
skip replace _has_inf_or_nan
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
pid 399's current affinity list: 0-191
pid 399's new affinity list: 24-47
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-0]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=8, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1_swap', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-05 19:39:47,954] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-05 19:39:47,954] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-05 19:39:47,954] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
09/05/2024 19:39:47 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 0
Local process index: 0
Device: npu:0

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-3]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=8, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1_swap', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-05 19:39:48,151] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-05 19:39:48,151] [INFO] [comm.py:637:init_distributed] cdb=None
09/05/2024 19:39:48 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 3
Local process index: 3
Device: npu:3

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-5]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=8, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1_swap', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-05 19:39:48,226] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-05 19:39:48,226] [INFO] [comm.py:637:init_distributed] cdb=None
09/05/2024 19:39:48 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 5
Local process index: 5
Device: npu:5

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-6]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=8, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1_swap', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-05 19:39:48,568] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-05 19:39:48,568] [INFO] [comm.py:637:init_distributed] cdb=None
09/05/2024 19:39:48 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 6
Local process index: 6
Device: npu:6

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
[RANK-4]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=8, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1_swap', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-05 19:39:48,606] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-05 19:39:48,607] [INFO] [comm.py:637:init_distributed] cdb=None
09/05/2024 19:39:48 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 4
Local process index: 4
Device: npu:4

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-7]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=8, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1_swap', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-05 19:39:48,722] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-05 19:39:48,722] [INFO] [comm.py:637:init_distributed] cdb=None
09/05/2024 19:39:48 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 7
Local process index: 7
Device: npu:7

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-2]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=8, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1_swap', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-05 19:39:49,410] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-05 19:39:49,410] [INFO] [comm.py:637:init_distributed] cdb=None
09/05/2024 19:39:49 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 2
Local process index: 2
Device: npu:2

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-1]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=8, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1_swap', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-05 19:39:50,913] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-05 19:39:50,913] [INFO] [comm.py:637:init_distributed] cdb=None
09/05/2024 19:39:50 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 1
Local process index: 1
Device: npu:1

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
  0%|          | 0/478625 [00:00<?, ?it/s][A
  1%|          | 3251/478625 [00:00<00:14, 32490.05it/s][A
  1%|         | 6501/478625 [00:00<00:14, 32025.38it/s][A
  2%|         | 9840/478625 [00:00<00:14, 32638.48it/s][A
  3%|         | 13204/478625 [00:00<00:14, 33030.86it/s][A
  3%|         | 16508/478625 [00:00<00:14, 32286.15it/s][A
  4%|         | 19874/478625 [00:00<00:14, 32742.51it/s][A
  5%|         | 23152/478625 [00:00<00:14, 32173.65it/s][A
  6%|         | 26519/478625 [00:00<00:13, 32638.90it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
  6%|         | 29886/478625 [00:00<00:13, 32953.46it/s][A
  1%|          | 3283/478625 [00:00<00:14, 32823.38it/s][A
  7%|         | 33184/478625 [00:01<00:13, 32351.19it/s][A
  1%|         | 6566/478625 [00:00<00:14, 32342.98it/s][A
  8%|         | 36526/478625 [00:01<00:13, 32671.28it/s][A
  2%|         | 9914/478625 [00:00<00:14, 32853.43it/s][A
  8%|         | 39797/478625 [00:01<00:13, 32021.81it/s][A
  3%|         | 13268/478625 [00:00<00:14, 33119.53it/s][A
  9%|         | 43159/478625 [00:01<00:13, 32492.61it/s][A
  3%|         | 16581/478625 [00:00<00:14, 32189.47it/s][A
 10%|         | 46494/478625 [00:01<00:13, 32744.78it/s][A
  4%|         | 19961/478625 [00:00<00:14, 32722.97it/s][A
 10%|         | 49772/478625 [00:01<00:13, 32223.13it/s][ALoading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...

  5%|         | 23238/478625 [00:00<00:14, 32140.06it/s][A
 11%|         | 53133/478625 [00:01<00:13, 32629.93it/s][A
  6%|         | 26597/478625 [00:00<00:13, 32589.26it/s][A
 12%|        | 56465/478625 [00:01<00:12, 32832.76it/s][A
  6%|         | 29954/478625 [00:00<00:13, 32889.01it/s][A
 12%|        | 59751/478625 [00:01<00:13, 32212.64it/s][A
  7%|         | 33246/478625 [00:01<00:13, 32351.35it/s][A
 13%|        | 63097/478625 [00:01<00:12, 32578.64it/s][A
  8%|         | 36594/478625 [00:01<00:13, 32688.78it/s][A
 14%|        | 66359/478625 [00:02<00:12, 32136.45it/s][A
  8%|         | 39866/478625 [00:01<00:13, 32124.55it/s][A
 15%|        | 69733/478625 [00:02<00:12, 32605.80it/s][A
  9%|         | 43239/478625 [00:01<00:13, 32598.57it/s][A
 15%|        | 73035/478625 [00:02<00:12, 32725.93it/s][A
 10%|         | 46583/478625 [00:01<00:13, 32847.91it/s][A
 16%|        | 76311/478625 [00:02<00:12, 32222.12it/s][A
 10%|         | 49871/478625 [00:01<00:13, 32257.25it/s][A
 17%|        | 79659/478625 [00:02<00:12, 32590.76it/s][A
 11%|         | 53203/478625 [00:01<00:13, 32567.66it/s][A
 17%|        | 82921/478625 [00:02<00:12, 32070.33it/s][A
 12%|        | 56464/478625 [00:01<00:13, 32088.75it/s][A
 18%|        | 86262/478625 [00:02<00:12, 32462.77it/s][A
 12%|        | 59817/478625 [00:01<00:12, 32509.86it/s][A
 19%|        | 89641/478625 [00:02<00:11, 32852.13it/s][A
 13%|        | 63175/478625 [00:01<00:12, 32824.58it/s][A
 19%|        | 92929/478625 [00:02<00:11, 32164.75it/s][A
 14%|        | 66461/478625 [00:02<00:12, 32330.60it/s][A
 20%|        | 96253/478625 [00:02<00:11, 32478.88it/s][A
 15%|        | 69832/478625 [00:02<00:12, 32736.44it/s][A
 21%|        | 99505/478625 [00:03<00:11, 32066.83it/s][A
 15%|        | 73109/478625 [00:02<00:12, 32133.77it/s][A
 21%|       | 102799/478625 [00:03<00:11, 32319.95it/s][A
 16%|        | 76473/478625 [00:02<00:12, 32573.07it/s][A
 22%|       | 106121/478625 [00:03<00:11, 32583.83it/s][A
 17%|        | 79824/478625 [00:02<00:12, 32848.29it/s][A
 23%|       | 109382/478625 [00:03<00:11, 32102.49it/s][A
 17%|        | 83112/478625 [00:02<00:12, 32298.31it/s][A
 24%|       | 112716/478625 [00:03<00:11, 32465.14it/s][A
 18%|        | 86456/478625 [00:02<00:12, 32631.32it/s][A
 24%|       | 115966/478625 [00:03<00:11, 31931.62it/s][A
 19%|        | 89728/478625 [00:02<00:12, 32004.69it/s][A
 25%|       | 119288/478625 [00:03<00:11, 32308.30it/s][A
 19%|        | 93086/478625 [00:02<00:11, 32464.39it/s][A
 26%|       | 122641/478625 [00:03<00:10, 32667.67it/s][A
 20%|        | 96421/478625 [00:02<00:11, 32722.66it/s][A
 26%|       | 125911/478625 [00:03<00:10, 32112.50it/s][A
 21%|        | 99697/478625 [00:03<00:11, 32236.14it/s][A
 27%|       | 129234/478625 [00:03<00:10, 32439.16it/s][A
 22%|       | 103041/478625 [00:03<00:11, 32588.35it/s][A
 28%|       | 132577/478625 [00:04<00:10, 32730.58it/s][A
 22%|       | 106387/478625 [00:03<00:11, 32845.84it/s][A
 28%|       | 135853/478625 [00:04<00:10, 32219.83it/s][A
 23%|       | 109675/478625 [00:03<00:11, 32270.95it/s][A
 29%|       | 139191/478625 [00:04<00:10, 32559.00it/s][A
 24%|       | 113030/478625 [00:03<00:11, 32644.78it/s][A
 30%|       | 142450/478625 [00:04<00:10, 32087.24it/s][A
 24%|       | 116298/478625 [00:03<00:11, 32122.90it/s][A
 30%|       | 145757/478625 [00:04<00:10, 32373.64it/s][A
 25%|       | 119648/478625 [00:03<00:11, 32525.61it/s][A
 31%|       | 149096/478625 [00:04<00:10, 32673.27it/s][A
 26%|       | 122994/478625 [00:03<00:10, 32799.95it/s][A
 32%|      | 152366/478625 [00:04<00:10, 32095.14it/s][A
 26%|       | 126277/478625 [00:03<00:10, 32183.13it/s][A
 33%|      | 155723/478625 [00:04<00:09, 32526.29it/s][A
 27%|       | 129642/478625 [00:03<00:10, 32612.02it/s][A
 33%|      | 158979/478625 [00:04<00:09, 32002.31it/s][A
 28%|       | 132907/478625 [00:04<00:10, 32196.10it/s][A
 34%|      | 162232/478625 [00:05<00:09, 32155.78it/s][A
 28%|       | 136257/478625 [00:04<00:10, 32579.18it/s][A
 35%|      | 165592/478625 [00:05<00:09, 32580.34it/s][A
 29%|       | 139620/478625 [00:04<00:10, 32888.66it/s][A
 35%|      | 168853/478625 [00:05<00:09, 32007.20it/s][A
 30%|       | 142912/478625 [00:04<00:10, 32279.68it/s][A
 36%|      | 172172/478625 [00:05<00:09, 32353.70it/s][A
 31%|       | 146245/478625 [00:04<00:10, 32587.08it/s][A
 37%|      | 175411/478625 [00:05<00:09, 31880.44it/s][A
 31%|       | 149507/478625 [00:04<00:10, 32115.68it/s][A
 37%|      | 178742/478625 [00:05<00:09, 32297.77it/s][A
 32%|      | 152863/478625 [00:04<00:10, 32538.83it/s][A
 38%|      | 182007/478625 [00:05<00:09, 32399.05it/s][A
 33%|      | 156221/478625 [00:04<00:09, 32845.67it/s][A
 39%|      | 185250/478625 [00:05<00:09, 31926.93it/s][A
 33%|      | 159509/478625 [00:04<00:09, 32201.63it/s][A
 39%|      | 188571/478625 [00:05<00:08, 32303.00it/s][A
 34%|      | 162815/478625 [00:05<00:09, 32451.55it/s][A
 40%|      | 191804/478625 [00:05<00:09, 31774.14it/s][A
 35%|      | 166064/478625 [00:05<00:09, 32021.19it/s][A
 41%|      | 195139/478625 [00:06<00:08, 32236.69it/s][A
 35%|      | 169404/478625 [00:05<00:09, 32424.20it/s][A
 41%|     | 198463/478625 [00:06<00:08, 32531.65it/s][A
 36%|      | 172746/478625 [00:05<00:09, 32715.76it/s][A
 42%|     | 201719/478625 [00:06<00:08, 31991.68it/s][A
 37%|      | 176021/478625 [00:05<00:09, 32209.60it/s][A
 43%|     | 205009/478625 [00:06<00:08, 32256.59it/s][A
 37%|      | 179365/478625 [00:05<00:09, 32569.53it/s][A
 44%|     | 208329/478625 [00:06<00:08, 32534.28it/s][A
 38%|      | 182625/478625 [00:05<00:09, 31831.18it/s][A
 44%|     | 211585/478625 [00:06<00:08, 31980.45it/s][A
 39%|      | 185988/478625 [00:05<00:09, 32354.09it/s][A
 45%|     | 214915/478625 [00:06<00:08, 32347.85it/s][A
 40%|      | 189354/478625 [00:05<00:08, 32737.96it/s][A
 46%|     | 218153/478625 [00:06<00:08, 31907.49it/s][A
 40%|      | 192632/478625 [00:05<00:08, 32253.81it/s][A
 46%|     | 221457/478625 [00:06<00:07, 32239.41it/s][A
 41%|      | 195970/478625 [00:06<00:08, 32583.89it/s][A
 47%|     | 224795/478625 [00:06<00:07, 32575.49it/s][A
 42%|     | 199306/478625 [00:06<00:08, 32811.26it/s][A
 48%|     | 228055/478625 [00:07<00:07, 32057.00it/s][A
 42%|     | 202590/478625 [00:06<00:08, 32180.23it/s][A
 48%|     | 231289/478625 [00:07<00:07, 32137.60it/s][A
 43%|     | 205859/478625 [00:06<00:08, 32327.44it/s][A
 49%|     | 234506/478625 [00:07<00:07, 31687.41it/s][A
 44%|     | 209095/478625 [00:06<00:08, 31930.44it/s][A
 50%|     | 237832/478625 [00:07<00:07, 32146.80it/s][A
 44%|     | 212434/478625 [00:06<00:08, 32359.18it/s][A
 50%|     | 241158/478625 [00:07<00:07, 32474.77it/s][A
 45%|     | 215779/478625 [00:06<00:08, 32678.91it/s][A
 51%|     | 244408/478625 [00:07<00:07, 31921.41it/s][ALoading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...

 46%|     | 219050/478625 [00:06<00:08, 32187.41it/s][A
 52%|    | 247721/478625 [00:07<00:07, 32276.24it/s][A
 46%|     | 222408/478625 [00:06<00:07, 32594.69it/s][A
 52%|    | 250952/478625 [00:07<00:07, 31661.65it/s][ALoading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...

 47%|     | 225671/478625 [00:06<00:07, 32027.31it/s][A
 53%|    | 254291/478625 [00:07<00:06, 32167.04it/s][A
 48%|     | 228996/478625 [00:07<00:07, 32385.39it/s][A
 54%|    | 257587/478625 [00:07<00:06, 32399.46it/s][A
 49%|     | 232293/478625 [00:07<00:07, 32556.39it/s][A
 54%|    | 260831/478625 [00:08<00:06, 31622.30it/s][A
 49%|     | 235552/478625 [00:07<00:07, 32008.96it/s][ALoading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...

 55%|    | 264147/478625 [00:08<00:06, 32063.91it/s][ALoading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...

 50%|     | 238896/478625 [00:07<00:07, 32427.97it/s][A
 56%|    | 267473/478625 [00:08<00:06, 32414.72it/s][A
 51%|     | 242142/478625 [00:07<00:07, 32006.34it/s][A
 57%|    | 270719/478625 [00:08<00:06, 31762.52it/s][A
 51%|    | 245482/478625 [00:07<00:07, 32415.93it/s][A
 57%|    | 274063/478625 [00:08<00:06, 32252.05it/s][A
 52%|    | 248789/478625 [00:07<00:07, 32606.49it/s][A
 58%|    | 277293/478625 [00:08<00:06, 31763.23it/s][A
 53%|    | 252052/478625 [00:07<00:07, 32054.03it/s][A
 59%|    | 280626/478625 [00:08<00:06, 32220.65it/s][A
 53%|    | 255402/478625 [00:07<00:06, 32478.10it/s][A
 59%|    | 283952/478625 [00:08<00:05, 32525.23it/s][A
 54%|    | 258653/478625 [00:07<00:06, 31856.25it/s][A
 60%|    | 287208/478625 [00:08<00:05, 32010.93it/s][A
 55%|    | 261935/478625 [00:08<00:06, 32137.36it/s][A
 61%|    | 290548/478625 [00:08<00:05, 32416.22it/s][A
 55%|    | 265263/478625 [00:08<00:06, 32473.74it/s][A
 61%|   | 293793/478625 [00:09<00:05, 31589.42it/s][A
 56%|    | 268514/478625 [00:08<00:06, 31917.60it/s][A
 62%|   | 297139/478625 [00:09<00:05, 32134.09it/s][A
 57%|    | 271830/478625 [00:08<00:06, 32280.92it/s][A
 63%|   | 300471/478625 [00:09<00:05, 32480.46it/s][A
 57%|    | 275159/478625 [00:08<00:06, 32577.68it/s][A
 63%|   | 303724/478625 [00:09<00:05, 31932.09it/s][A
 58%|    | 278420/478625 [00:08<00:06, 31957.02it/s][A
 64%|   | 307068/478625 [00:09<00:05, 32336.66it/s][A
 59%|    | 281767/478625 [00:08<00:06, 32398.16it/s][A
 65%|   | 310306/478625 [00:09<00:05, 31851.35it/s][A
 60%|    | 285011/478625 [00:08<00:06, 31957.20it/s][A
 66%|   | 313621/478625 [00:09<00:05, 32229.28it/s][ALoading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...

 60%|    | 288365/478625 [00:08<00:05, 32421.87it/s][A
 66%|   | 316948/478625 [00:09<00:04, 32535.86it/s][A
 61%|    | 291611/478625 [00:08<00:05, 32430.32it/s][A
 67%|   | 320205/478625 [00:09<00:04, 31983.01it/s][A
 62%|   | 294857/478625 [00:09<00:05, 31954.29it/s][A
 68%|   | 323505/478625 [00:10<00:04, 32279.81it/s][A
 62%|   | 298199/478625 [00:09<00:05, 32383.41it/s][A
 68%|   | 326763/478625 [00:10<00:04, 31656.42it/s][A
 63%|   | 301441/478625 [00:09<00:05, 31910.07it/s][A
 69%|   | 329997/478625 [00:10<00:04, 31855.38it/s][A
 64%|   | 304785/478625 [00:09<00:05, 32358.00it/s][A
 70%|   | 333329/478625 [00:10<00:04, 32286.25it/s][A
 64%|   | 308103/478625 [00:09<00:05, 32600.54it/s][A
 70%|   | 336561/478625 [00:10<00:04, 31755.18it/s][A
 65%|   | 311366/478625 [00:09<00:05, 32049.03it/s][A
 71%|   | 339884/478625 [00:10<00:04, 32186.21it/s][A
 66%|   | 314725/478625 [00:09<00:05, 32501.56it/s][A
 72%|  | 343188/478625 [00:10<00:04, 32436.39it/s][A
 66%|   | 317979/478625 [00:09<00:05, 31947.73it/s][A
 72%|  | 346435/478625 [00:10<00:04, 31861.30it/s][A
 67%|   | 321312/478625 [00:09<00:04, 32352.62it/s][A
 73%|  | 349766/478625 [00:10<00:03, 32284.63it/s][A
 68%|   | 324666/478625 [00:10<00:04, 32701.41it/s][A
 74%|  | 352998/478625 [00:10<00:03, 31736.49it/s][A
 69%|   | 327940/478625 [00:10<00:04, 31849.43it/s][A
 74%|  | 356350/478625 [00:11<00:03, 32258.28it/s][A
 69%|   | 331279/478625 [00:10<00:04, 32298.56it/s][A
 75%|  | 359705/478625 [00:11<00:03, 32639.09it/s][A
 70%|   | 334515/478625 [00:10<00:04, 31800.09it/s][A
 76%|  | 362973/478625 [00:11<00:03, 32019.61it/s][A
 71%|   | 337855/478625 [00:10<00:04, 32267.10it/s][A
 77%|  | 366293/478625 [00:11<00:03, 32363.61it/s][A
 71%|  | 341218/478625 [00:10<00:04, 32668.12it/s][A
 77%|  | 369534/478625 [00:11<00:03, 31620.98it/s][A
 72%|  | 344489/478625 [00:10<00:04, 31940.32it/s][A
 78%|  | 372834/478625 [00:11<00:03, 32021.77it/s][A
 73%|  | 347842/478625 [00:10<00:04, 32403.03it/s][A
 79%|  | 376164/478625 [00:11<00:03, 32395.11it/s][A
 73%|  | 351181/478625 [00:10<00:03, 32691.74it/s][A
 79%|  | 379408/478625 [00:11<00:03, 31880.52it/s][A
 74%|  | 354455/478625 [00:10<00:03, 32128.10it/s][A
 80%|  | 382773/478625 [00:11<00:02, 32399.01it/s][A
 75%|  | 357803/478625 [00:11<00:03, 32524.17it/s][A
 81%|  | 386017/478625 [00:11<00:02, 31903.17it/s][A
 75%|  | 361060/478625 [00:11<00:03, 32074.43it/s][A
 81%| | 389359/478625 [00:12<00:02, 32347.68it/s][Amissing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!

 76%|  | 364402/478625 [00:11<00:03, 32469.25it/s][A
 82%| | 392735/478625 [00:12<00:02, 32763.45it/s][A
 77%|  | 367714/478625 [00:11<00:03, 32660.77it/s][A
 83%| | 396015/478625 [00:12<00:02, 32083.38it/s][A
 78%|  | 370983/478625 [00:11<00:03, 31879.86it/s][A
 83%| | 399344/478625 [00:12<00:02, 32436.44it/s][A
 78%|  | 374330/478625 [00:11<00:03, 32342.34it/s][A
 84%| | 402688/478625 [00:12<00:02, 32731.28it/s][A
 79%|  | 377570/478625 [00:11<00:03, 31859.53it/s][A
 85%| | 405965/478625 [00:12<00:02, 32088.09it/s][A
 80%|  | 380904/478625 [00:11<00:03, 32258.68it/s][A
 86%| | 409307/478625 [00:12<00:02, 32477.90it/s][A
 80%|  | 384257/478625 [00:11<00:02, 32632.31it/s][A
 86%| | 412559/478625 [00:12<00:02, 31832.48it/s][A
 81%|  | 387524/478625 [00:11<00:02, 32108.02it/s][A
 87%| | 415789/478625 [00:12<00:01, 31967.39it/s][A
 82%| | 390859/478625 [00:12<00:02, 32471.49it/s][A
 88%| | 419128/478625 [00:12<00:01, 32383.76it/s][A
 82%| | 394110/478625 [00:12<00:02, 32033.34it/s][A
 88%| | 422370/478625 [00:13<00:01, 31879.00it/s][A
 83%| | 397440/478625 [00:12<00:02, 32404.75it/s][A
 89%| | 425710/478625 [00:13<00:01, 32325.07it/s][A
 84%| | 400774/478625 [00:12<00:02, 32680.28it/s][A
 90%| | 428946/478625 [00:13<00:01, 31796.07it/s][A
 84%| | 404045/478625 [00:12<00:02, 32208.41it/s][A
 90%| | 432253/478625 [00:13<00:01, 32167.46it/s][A
 85%| | 407366/478625 [00:12<00:02, 32500.49it/s][A
 91%| | 435618/478625 [00:13<00:01, 32603.51it/s][A
 86%| | 410619/478625 [00:12<00:02, 31982.15it/s][A
 92%|| 438882/478625 [00:13<00:01, 32012.27it/s][A
 86%| | 413942/478625 [00:12<00:01, 32348.40it/s][A
 92%|| 442225/478625 [00:13<00:01, 32425.38it/s][A
 87%| | 417180/478625 [00:12<00:01, 32284.58it/s][A
 93%|| 445472/478625 [00:13<00:01, 31895.03it/s][A
 88%| | 420411/478625 [00:12<00:01, 31844.22it/s][A
 94%|| 448824/478625 [00:13<00:00, 32368.81it/s][A
 89%| | 423778/478625 [00:13<00:01, 32381.85it/s][A
 94%|| 452137/478625 [00:14<00:00, 32593.15it/s][A
 89%| | 427116/478625 [00:13<00:01, 32675.61it/s][A
 95%|| 455400/478625 [00:14<00:00, 32029.06it/s][A
 90%| | 430386/478625 [00:13<00:01, 32124.07it/s][A
 96%|| 458746/478625 [00:14<00:00, 32447.34it/s][A
 91%| | 433734/478625 [00:13<00:01, 32522.78it/s][A
 97%|| 461995/478625 [00:14<00:00, 31840.94it/s][A
 91%|| 436990/478625 [00:13<00:01, 32089.53it/s][A
 97%|| 465295/478625 [00:14<00:00, 32177.89it/s][A
 92%|| 440329/478625 [00:13<00:01, 32469.69it/s][A
 98%|| 468517/478625 [00:14<00:00, 32111.94it/s][A
 93%|| 443674/478625 [00:13<00:01, 32758.55it/s][A
 99%|| 471731/478625 [00:14<00:00, 31604.72it/s][A
 93%|| 446953/478625 [00:13<00:00, 32239.27it/s][A
 99%|| 475087/478625 [00:14<00:00, 32177.51it/s][A
 94%|| 450290/478625 [00:13<00:00, 32571.00it/s][A
100%|| 478437/478625 [00:14<00:00, 32566.37it/s][A100%|| 478625/478625 [00:14<00:00, 32244.93it/s]
100%|| 1/1 [00:21<00:00, 21.07s/it]100%|| 1/1 [00:21<00:00, 21.07s/it]

 95%|| 453550/478625 [00:14<00:00, 31939.75it/s][A
 95%|| 456900/478625 [00:14<00:00, 32394.66it/s][A
 96%|| 460268/478625 [00:14<00:00, 32771.74it/s][A
 97%|| 463549/478625 [00:14<00:00, 32041.30it/s][A
 98%|| 466784/478625 [00:14<00:00, 32128.82it/s][A
 98%|| 470001/478625 [00:14<00:00, 31803.25it/s][A
 99%|| 473329/478625 [00:14<00:00, 32236.00it/s][Atime 21.760502099990845
n_elements: 474899
data length: 474899

100%|| 476679/478625 [00:14<00:00, 32609.13it/s][A100%|| 478625/478625 [00:14<00:00, 32351.36it/s]
100%|| 1/1 [00:21<00:00, 21.01s/it]100%|| 1/1 [00:21<00:00, 21.01s/it]
time 21.66859006881714
n_elements: 474899
data length: 474899
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
09/05/2024 19:41:57 - INFO - __main__ - optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: False
    fused: None
    lr: 1e-05
    maximize: False
    weight_decay: 0.01
)
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
  0%|          | 0/1 [00:00<?, ?it/s]09/05/2024 19:42:05 - INFO - opensora.dataset.t2v_datasets - Building /home/image_data/captions/TV01_clips_final_478625_llavanext_217405_aes478625.json...
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

  0%|          | 0/478625 [00:00<?, ?it/s][A
  1%|          | 3313/478625 [00:00<00:14, 33125.06it/s][A
  1%|         | 6626/478625 [00:00<00:14, 32474.06it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
  2%|         | 9996/478625 [00:00<00:14, 33024.73it/s][A
  3%|         | 13366/478625 [00:00<00:13, 33288.44it/s][A
  3%|         | 16696/478625 [00:00<00:14, 32443.20it/s][A
  4%|         | 20075/478625 [00:00<00:13, 32888.71it/s][A
  5%|         | 23368/478625 [00:00<00:14, 32372.55it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
  6%|         | 26754/478625 [00:00<00:13, 32833.92it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
  6%|         | 30131/478625 [00:00<00:13, 33120.14it/s][A
  7%|         | 33446/478625 [00:01<00:13, 32451.60it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
  8%|         | 36829/478625 [00:01<00:13, 32862.46it/s][A
  8%|         | 40119/478625 [00:01<00:13, 32267.72it/s][A
  9%|         | 43487/478625 [00:01<00:13, 32684.39it/s][A/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

 10%|         | 46850/478625 [00:01<00:13, 32964.82it/s][A
 10%|         | 50150/478625 [00:01<00:13, 32440.52it/s][A
 11%|         | 53530/478625 [00:01<00:12, 32840.00it/s][A
 12%|        | 56818/478625 [00:01<00:13, 32285.96it/s][A
 13%|        | 60198/478625 [00:01<00:12, 32727.35it/s][A
 13%|        | 63578/478625 [00:01<00:12, 33042.05it/s][A
 14%|        | 66886/478625 [00:02<00:12, 32491.49it/s][A
 15%|        | 70247/478625 [00:02<00:12, 32818.80it/s][A
 15%|        | 73533/478625 [00:02<00:12, 32251.40it/s][A
 16%|        | 76929/478625 [00:02<00:12, 32751.55it/s][A
 17%|        | 80278/478625 [00:02<00:12, 32967.39it/s][A
 17%|        | 83578/478625 [00:02<00:12, 32447.99it/s][A
 18%|        | 86956/478625 [00:02<00:11, 32837.12it/s][A
 19%|        | 90339/478625 [00:02<00:11, 33127.77it/s][A
 20%|        | 93655/478625 [00:02<00:11, 32466.89it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
 20%|        | 97021/478625 [00:02<00:11, 32816.55it/s][A
 21%|        | 100307/478625 [00:03<00:11, 32305.66it/s][A
 22%|       | 103682/478625 [00:03<00:11, 32726.50it/s][A
 22%|       | 107056/478625 [00:03<00:11, 33024.41it/s][A
 23%|       | 110362/478625 [00:03<00:11, 32433.74it/s][A
 24%|       | 113691/478625 [00:03<00:11, 32684.62it/s][A
 24%|       | 116963/478625 [00:03<00:11, 32287.75it/s][A
 25%|       | 120336/478625 [00:03<00:10, 32711.11it/s][A
 26%|       | 123710/478625 [00:03<00:10, 33012.32it/s][A
 27%|       | 127014/478625 [00:03<00:10, 32414.60it/s][A
 27%|       | 130396/478625 [00:03<00:10, 32825.32it/s][A
 28%|       | 133682/478625 [00:04<00:10, 32364.12it/s][A
 29%|       | 137054/478625 [00:04<00:10, 32760.03it/s][A
 29%|       | 140434/478625 [00:04<00:10, 33066.54it/s][A
 30%|       | 143744/478625 [00:04<00:10, 32396.58it/s][A
 31%|       | 147094/478625 [00:04<00:10, 32719.67it/s][A
 31%|      | 150370/478625 [00:04<00:10, 32226.21it/s][A
 32%|      | 153734/478625 [00:04<00:09, 32640.75it/s][A
 33%|      | 157134/478625 [00:04<00:09, 33041.79it/s][A
 34%|      | 160442/478625 [00:04<00:09, 32386.85it/s][A
 34%|      | 163783/478625 [00:05<00:09, 32685.56it/s][A
 35%|      | 167056/478625 [00:05<00:09, 32298.42it/s][A
 36%|      | 170418/478625 [00:05<00:09, 32685.64it/s][A
 36%|      | 173789/478625 [00:05<00:09, 32987.82it/s][A
 37%|      | 177091/478625 [00:05<00:09, 32446.91it/s][A
 38%|      | 180441/478625 [00:05<00:09, 32732.21it/s][A
 38%|      | 183718/478625 [00:05<00:09, 32150.71it/s][A
 39%|      | 187058/478625 [00:05<00:08, 32515.34it/s][A
 40%|      | 190460/478625 [00:05<00:08, 32959.27it/s][A
 40%|      | 193759/478625 [00:05<00:08, 32456.05it/s][A
 41%|      | 197123/478625 [00:06<00:08, 32801.59it/s][A
 42%|     | 200407/478625 [00:06<00:08, 32281.21it/s][A
 43%|     | 203768/478625 [00:06<00:08, 32669.04it/s][A
 43%|     | 207109/478625 [00:06<00:08, 32887.17it/s][A
 44%|     | 210401/478625 [00:06<00:08, 32344.12it/s][A
 45%|     | 213763/478625 [00:06<00:08, 32718.69it/s][A
 45%|     | 217038/478625 [00:06<00:08, 32240.71it/s][A
 46%|     | 220430/478625 [00:06<00:07, 32733.52it/s][A
 47%|     | 223820/478625 [00:06<00:07, 33077.23it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
 47%|     | 227131/478625 [00:06<00:07, 32449.12it/s][A
  1%|          | 3324/478625 [00:00<00:14, 33236.94it/s][A
 48%|     | 230424/478625 [00:07<00:07, 32587.43it/s][A
  1%|         | 6648/478625 [00:00<00:14, 32531.88it/s][A
 49%|     | 233802/478625 [00:07<00:07, 32938.23it/s][A
  2%|         | 9903/478625 [00:00<00:14, 32191.73it/s][A
 50%|     | 237099/478625 [00:07<00:07, 32398.99it/s][A
  3%|         | 13181/478625 [00:00<00:14, 32417.46it/s][A
 50%|     | 240468/478625 [00:07<00:07, 32777.68it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
  3%|         | 16424/478625 [00:00<00:14, 31346.46it/s][A
 51%|     | 243749/478625 [00:07<00:07, 32220.33it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
  1%|          | 3175/478625 [00:00<00:14, 31740.80it/s][A
  4%|         | 19672/478625 [00:00<00:14, 31719.72it/s][A
 52%|    | 247123/478625 [00:07<00:07, 32665.55it/s][A
  1%|          | 3300/478625 [00:00<00:14, 32993.74it/s][A
  5%|         | 22878/478625 [00:00<00:14, 31435.92it/s][A
  1%|         | 6350/478625 [00:00<00:15, 29866.97it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
 52%|    | 250483/478625 [00:07<00:06, 32938.62it/s][A
  1%|         | 6600/478625 [00:00<00:14, 32493.47it/s][A
  5%|         | 26177/478625 [00:00<00:14, 31919.06it/s][A
  2%|         | 9597/478625 [00:00<00:15, 31010.92it/s][A
  1%|          | 3214/478625 [00:00<00:14, 32136.20it/s][A
 53%|    | 253780/478625 [00:07<00:06, 32368.76it/s][A
  2%|         | 9980/478625 [00:00<00:14, 33083.20it/s][A
  6%|         | 29557/478625 [00:00<00:13, 32496.93it/s][A
  3%|         | 12896/478625 [00:00<00:14, 31774.66it/s][A
  1%|         | 6428/478625 [00:00<00:14, 31674.22it/s][A
 54%|    | 257123/478625 [00:07<00:06, 32680.39it/s][A
  3%|         | 13334/478625 [00:00<00:13, 33261.10it/s][A
  7%|         | 32810/478625 [00:01<00:13, 32059.10it/s][A
  3%|         | 16080/478625 [00:00<00:14, 31165.62it/s][A
  2%|         | 9739/478625 [00:00<00:14, 32323.00it/s][A
 54%|    | 260395/478625 [00:07<00:06, 32059.46it/s][A
  3%|         | 16661/478625 [00:00<00:14, 32430.21it/s][A
  8%|         | 36213/478625 [00:01<00:13, 32649.60it/s][A
  4%|         | 19371/478625 [00:00<00:14, 31744.71it/s][A
  3%|         | 13062/478625 [00:00<00:14, 32678.80it/s][A
 55%|    | 263736/478625 [00:08<00:06, 32452.83it/s][A
  4%|         | 20054/478625 [00:00<00:13, 32926.57it/s][A
  8%|         | 39482/478625 [00:01<00:13, 32557.33it/s][A
  5%|         | 22663/478625 [00:00<00:14, 32120.40it/s][A
  3%|         | 16331/478625 [00:00<00:14, 31820.29it/s][A
 56%|    | 267099/478625 [00:08<00:06, 32798.12it/s][A
  5%|         | 23351/478625 [00:00<00:14, 32323.88it/s][A
  9%|         | 42740/478625 [00:01<00:13, 32090.21it/s][A
  5%|         | 25879/478625 [00:00<00:14, 31498.58it/s][A
  4%|         | 19636/478625 [00:00<00:14, 32227.85it/s][A
 56%|    | 270383/478625 [00:08<00:06, 32274.68it/s][A
  6%|         | 26725/478625 [00:00<00:13, 32764.01it/s][A
 10%|         | 46103/478625 [00:01<00:13, 32520.35it/s][A
  6%|         | 29173/478625 [00:00<00:14, 31938.19it/s][A
  5%|         | 22882/478625 [00:00<00:14, 31677.91it/s][A
 57%|    | 273742/478625 [00:08<00:06, 32659.21it/s][A
  6%|         | 30108/478625 [00:00<00:13, 33089.10it/s][A
 10%|         | 49358/478625 [00:01<00:13, 31642.28it/s][A
  7%|         | 32371/478625 [00:01<00:14, 31420.22it/s][A
  5%|         | 26226/478625 [00:00<00:14, 32224.66it/s][A
 58%|    | 277012/478625 [00:08<00:06, 32139.80it/s][A
  7%|         | 33420/478625 [00:01<00:13, 32546.24it/s][A
 11%|         | 52686/478625 [00:01<00:13, 32119.76it/s][A
  7%|         | 35659/478625 [00:01<00:13, 31854.20it/s][A
  6%|         | 29530/478625 [00:00<00:13, 32475.23it/s][A
 59%|    | 280371/478625 [00:08<00:06, 32565.25it/s][A
  8%|         | 36795/478625 [00:01<00:13, 32904.56it/s][A
 12%|        | 55911/478625 [00:01<00:13, 32157.36it/s][A
  8%|         | 38952/478625 [00:01<00:13, 32174.05it/s][A
  7%|         | 32781/478625 [00:01<00:13, 31852.81it/s][A
 59%|    | 283773/478625 [00:08<00:05, 32994.74it/s][A
  8%|         | 40089/478625 [00:01<00:13, 32294.93it/s][A
 12%|        | 59131/478625 [00:01<00:13, 31705.04it/s][A
  9%|         | 42173/478625 [00:01<00:13, 31534.32it/s][A
  8%|         | 36099/478625 [00:01<00:13, 32249.95it/s][A
 60%|    | 287076/478625 [00:08<00:05, 32442.34it/s][A
  9%|         | 43460/478625 [00:01<00:13, 32713.45it/s][A
 13%|        | 62467/478625 [00:01<00:12, 32190.21it/s][A
  9%|         | 45448/478625 [00:01<00:13, 31891.12it/s][A
  8%|         | 39390/478625 [00:01<00:13, 32445.75it/s][A
 61%|    | 290448/478625 [00:08<00:05, 32817.40it/s][A
 10%|         | 46825/478625 [00:01<00:13, 32990.29it/s][A
 10%|         | 48641/478625 [00:01<00:13, 31484.20it/s][A
 14%|        | 65690/478625 [00:02<00:13, 31465.72it/s][A
  9%|         | 42638/478625 [00:01<00:13, 31815.36it/s][A
 61%|   | 293734/478625 [00:09<00:05, 32087.36it/s][A
 10%|         | 50128/478625 [00:01<00:13, 32444.84it/s][A
 11%|         | 51911/478625 [00:01<00:13, 31840.63it/s][A
 14%|        | 69009/478625 [00:02<00:12, 31966.84it/s][A
 10%|         | 45969/478625 [00:01<00:13, 32255.36it/s][A
 62%|   | 297134/478625 [00:09<00:05, 32644.87it/s][A
 11%|         | 53480/478625 [00:01<00:12, 32759.25it/s][A
 12%|        | 55215/478625 [00:01<00:13, 32193.32it/s][A
 15%|        | 72272/478625 [00:02<00:12, 32161.32it/s][A
 10%|         | 49199/478625 [00:01<00:13, 31739.62it/s][A
 63%|   | 300511/478625 [00:09<00:05, 32975.40it/s][A
 12%|        | 56760/478625 [00:01<00:13, 32223.37it/s][A
 12%|        | 58437/478625 [00:01<00:13, 31613.49it/s][A
 16%|        | 75492/478625 [00:02<00:12, 31439.15it/s][A
 11%|         | 52518/478625 [00:01<00:13, 32165.41it/s][A
 63%|   | 303813/478625 [00:09<00:05, 32365.29it/s][A
 13%|        | 60157/478625 [00:01<00:12, 32736.43it/s][A
 13%|        | 61734/478625 [00:01<00:13, 32011.79it/s][A
 16%|        | 78727/478625 [00:02<00:12, 31704.71it/s][A
 12%|        | 55865/478625 [00:01<00:12, 32550.71it/s][A
 64%|   | 307176/478625 [00:09<00:05, 32734.05it/s][A
 13%|        | 63541/478625 [00:01<00:12, 33060.70it/s][A
 14%|        | 65027/478625 [00:02<00:12, 32281.30it/s][A
 17%|        | 81982/478625 [00:02<00:12, 31195.84it/s][A
 12%|        | 59124/478625 [00:01<00:13, 31888.16it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
 65%|   | 310454/478625 [00:09<00:05, 32283.78it/s][A
 14%|        | 66851/478625 [00:02<00:12, 32486.24it/s][A
 14%|        | 68258/478625 [00:02<00:12, 31701.61it/s][A
 18%|        | 85273/478625 [00:02<00:12, 31694.08it/s][A
 13%|        | 62459/478625 [00:01<00:12, 32316.94it/s][A
  1%|          | 3224/478625 [00:00<00:14, 32231.96it/s][A
 66%|   | 313821/478625 [00:09<00:05, 32688.97it/s][A
 15%|        | 70237/478625 [00:02<00:12, 32889.71it/s][A
 15%|        | 71536/478625 [00:02<00:12, 32018.49it/s][A
 19%|        | 88603/478625 [00:02<00:12, 32163.68it/s][A
 14%|        | 65695/478625 [00:02<00:12, 31830.35it/s][A
  1%|         | 6448/478625 [00:00<00:14, 31629.49it/s][A
 66%|   | 317180/478625 [00:09<00:04, 32951.65it/s][A
 15%|        | 73530/478625 [00:02<00:12, 32371.61it/s][A
 16%|        | 74742/478625 [00:02<00:12, 31512.16it/s][A
 19%|        | 91824/478625 [00:02<00:12, 30741.50it/s][A
 14%|        | 69018/478625 [00:02<00:12, 32238.01it/s][A
  2%|         | 9743/478625 [00:00<00:14, 32222.68it/s][A
 67%|   | 320478/478625 [00:09<00:04, 32415.84it/s][A
 16%|        | 76915/478625 [00:02<00:12, 32805.31it/s][A
 16%|        | 77897/478625 [00:02<00:12, 31321.54it/s][A
 20%|        | 95061/478625 [00:02<00:12, 31208.86it/s][A
 15%|        | 72283/478625 [00:02<00:12, 32357.86it/s][A
  3%|         | 13040/478625 [00:00<00:14, 32512.17it/s][A
 68%|   | 323869/478625 [00:09<00:04, 32853.09it/s][A
 17%|        | 80272/478625 [00:02<00:12, 33030.19it/s][A
 17%|        | 81195/478625 [00:02<00:12, 31807.64it/s][A
 21%|        | 98212/478625 [00:03<00:12, 31295.65it/s][A
 16%|        | 75522/478625 [00:02<00:12, 31855.41it/s][A
  3%|         | 16293/478625 [00:00<00:14, 31676.00it/s][A
 68%|   | 327158/478625 [00:10<00:04, 32218.00it/s][A
 17%|        | 83578/478625 [00:02<00:12, 32467.33it/s][A
 18%|        | 84379/478625 [00:02<00:12, 31330.03it/s][A
 21%|        | 101351/478625 [00:03<00:12, 30987.48it/s][A
 16%|        | 78818/478625 [00:02<00:12, 32179.64it/s][A
  4%|         | 19586/478625 [00:00<00:14, 32091.79it/s][A
 69%|   | 330435/478625 [00:10<00:04, 32378.12it/s][A
 18%|        | 86934/478625 [00:02<00:11, 32786.53it/s][A
 18%|        | 87646/478625 [00:02<00:12, 31722.04it/s][A
 22%|       | 104631/478625 [00:03<00:11, 31517.73it/s][A
 17%|        | 82039/478625 [00:02<00:12, 31678.11it/s][A
  5%|         | 22872/478625 [00:00<00:14, 32336.40it/s][A
 70%|   | 333802/478625 [00:10<00:04, 32758.04it/s][A
 19%|        | 90216/478625 [00:02<00:12, 32313.43it/s][A
 19%|        | 90821/478625 [00:02<00:12, 31272.39it/s][A
 23%|       | 107789/478625 [00:03<00:11, 31258.31it/s][A
 18%|        | 85368/478625 [00:02<00:12, 32149.54it/s][A
  5%|         | 26109/478625 [00:00<00:14, 31555.97it/s][A

 70%|   | 337081/478625 [00:10<00:04, 32251.34it/s][A 20%|        | 93536/478625 [00:02<00:11, 32572.58it/s][A
 20%|        | 94085/478625 [00:02<00:12, 31673.98it/s][A
 23%|       | 111058/478625 [00:03<00:11, 31678.36it/s][A
 19%|        | 88698/478625 [00:02<00:12, 32488.60it/s][A
  6%|         | 29390/478625 [00:00<00:14, 31935.65it/s][A

 71%|   | 340411/478625 [00:10<00:04, 32557.94it/s][A 20%|        | 96893/478625 [00:02<00:11, 32864.78it/s][A
 20%|        | 97256/478625 [00:03<00:12, 31515.76it/s][A
 24%|       | 114232/478625 [00:03<00:11, 31694.13it/s][A
 19%|        | 91950/478625 [00:02<00:12, 31781.55it/s][A
  7%|         | 32588/478625 [00:01<00:14, 31223.25it/s][A
 72%|  | 343670/478625 [00:10<00:04, 32044.66it/s][A
 21%|        | 100182/478625 [00:03<00:11, 32331.40it/s][A
 21%|        | 100410/478625 [00:03<00:12, 31144.20it/s][A
 25%|       | 117404/478625 [00:03<00:11, 31095.57it/s][A
 20%|        | 95267/478625 [00:02<00:11, 32188.04it/s][A
  7%|         | 35790/478625 [00:01<00:14, 31457.40it/s][A
 73%|  | 347044/478625 [00:10<00:04, 32541.77it/s]
[A 22%|       | 103516/478625 [00:03<00:11, 32627.98it/s][A
 22%|       | 103685/478625 [00:03<00:11, 31616.34it/s][A
 25%|       | 120687/478625 [00:03<00:11, 31602.89it/s][A
 21%|        | 98594/478625 [00:03<00:11, 32505.37it/s][A
  8%|         | 39075/478625 [00:01<00:13, 31839.91it/s][A
 73%|  | 350409/478625 [00:10<00:03, 32867.45it/s][A
 22%|       | 106782/478625 [00:03<00:11, 32063.14it/s][A
 22%|       | 106945/478625 [00:03<00:11, 31906.82it/s][A
 26%|       | 124012/478625 [00:03<00:11, 32088.32it/s][A
 21%|       | 101849/478625 [00:03<00:11, 31816.53it/s][A
  9%|         | 42263/478625 [00:01<00:13, 31312.76it/s][A
 74%|  | 353699/478625 [00:10<00:03, 32295.19it/s]
[A 23%|       | 110135/478625 [00:03<00:11, 32490.16it/s][A
 23%|       | 110138/478625 [00:03<00:11, 31397.64it/s][A
 27%|       | 127225/478625 [00:04<00:11, 31232.08it/s][A
 22%|       | 105147/478625 [00:03<00:11, 32156.64it/s][A
 10%|         | 45513/478625 [00:01<00:13, 31661.99it/s][A
 75%|  | 357063/478625 [00:10<00:03, 32689.59it/s][A
 24%|       | 113508/478625 [00:03<00:11, 32854.15it/s][A
 24%|       | 113438/478625 [00:03<00:11, 31869.42it/s][A
 27%|       | 130491/478625 [00:04<00:11, 31641.33it/s][A
 23%|       | 108367/478625 [00:03<00:11, 31608.82it/s][A
 10%|         | 48683/478625 [00:01<00:13, 31284.27it/s][A
 75%|  | 360449/478625 [00:11<00:03, 33032.50it/s][A
 24%|       | 116797/478625 [00:03<00:11, 32264.93it/s][A
 24%|       | 116628/478625 [00:03<00:11, 31083.19it/s][A
 28%|       | 133661/478625 [00:04<00:11, 30799.89it/s][A
 23%|       | 111662/478625 [00:03<00:11, 32000.81it/s][A
 11%|         | 51956/478625 [00:01<00:13, 31708.28it/s][A
 76%|  | 363756/478625 [00:11<00:03, 32470.23it/s][A
 25%|       | 120157/478625 [00:03<00:10, 32655.67it/s][A
 25%|       | 119918/478625 [00:03<00:11, 31611.42it/s][A
 29%|       | 136974/478625 [00:04<00:10, 31474.48it/s][A
 24%|       | 114977/478625 [00:03<00:11, 32337.35it/s][A
 12%|        | 55248/478625 [00:01<00:13, 32064.05it/s][A
 77%|  | 367090/478625 [00:11<00:03, 32725.40it/s][A
 26%|       | 123444/478625 [00:03<00:11, 32169.72it/s][A
 26%|       | 123187/478625 [00:03<00:11, 31927.36it/s][A
 29%|       | 140255/478625 [00:04<00:10, 31864.52it/s][A
 25%|       | 118215/478625 [00:03<00:11, 31713.74it/s][A
 12%|        | 58457/478625 [00:01<00:13, 31427.84it/s][A
 26%|       | 126808/478625 [00:03<00:10, 32598.46it/s][A
 77%|  | 370366/478625 [00:11<00:03, 31956.41it/s][A
 26%|       | 126384/478625 [00:03<00:11, 31395.68it/s][A
 30%|       | 143448/478625 [00:04<00:10, 31387.56it/s][A
 25%|       | 121538/478625 [00:03<00:11, 32158.17it/s][A
 13%|        | 61687/478625 [00:01<00:13, 31683.93it/s][A
 27%|       | 130148/478625 [00:03<00:10, 32831.78it/s][A
 78%|  | 373750/478625 [00:11<00:03, 32504.40it/s][A
 27%|       | 129666/478625 [00:04<00:10, 31811.55it/s][A
 31%|       | 146631/478625 [00:04<00:10, 31514.21it/s][A
 26%|       | 124758/478625 [00:03<00:11, 31651.47it/s][A
 14%|        | 64959/478625 [00:02<00:12, 31987.57it/s][A
 79%|  | 377113/478625 [00:11<00:03, 32834.58it/s][A
 28%|       | 133434/478625 [00:04<00:10, 32332.13it/s][A
 28%|       | 132852/478625 [00:04<00:11, 31389.35it/s][A
 31%|      | 149787/478625 [00:04<00:10, 31144.64it/s][A
 27%|       | 128023/478625 [00:03<00:10, 31941.70it/s][A
 14%|        | 68161/478625 [00:02<00:13, 31470.57it/s][A
 29%|       | 136812/478625 [00:04<00:10, 32718.20it/s][A
 79%|  | 380401/478625 [00:11<00:03, 32208.27it/s][A
 28%|       | 136100/478625 [00:04<00:10, 31677.88it/s][A
 32%|      | 153087/478625 [00:04<00:10, 31689.99it/s][A
 27%|       | 131357/478625 [00:04<00:10, 32351.76it/s][A
 15%|        | 71377/478625 [00:02<00:12, 31672.82it/s][A
 29%|       | 140183/478625 [00:04<00:10, 33009.71it/s][A
 80%|  | 383768/478625 [00:11<00:02, 32634.65it/s][A
 29%|       | 139388/478625 [00:04<00:10, 32030.08it/s][A
 33%|      | 156426/478625 [00:04<00:10, 32191.86it/s][A
 28%|       | 134596/478625 [00:04<00:10, 31784.11it/s][A
 16%|        | 74548/478625 [00:02<00:12, 31236.56it/s][A
 30%|       | 143487/478625 [00:04<00:10, 32403.59it/s][A
 81%|  | 387036/478625 [00:11<00:02, 32223.70it/s][A
 30%|       | 142594/478625 [00:04<00:10, 31474.37it/s][A
 33%|      | 159649/478625 [00:05<00:10, 31761.61it/s][A
 29%|       | 137924/478625 [00:04<00:10, 32220.35it/s][A
 16%|        | 77851/478625 [00:02<00:12, 31760.94it/s][A
 31%|       | 146831/478625 [00:04<00:10, 32707.38it/s][A
 82%| | 390401/478625 [00:11<00:02, 32641.74it/s][A
 30%|       | 145870/478625 [00:04<00:10, 31850.98it/s][A
 34%|      | 162941/478625 [00:05<00:09, 32101.45it/s][A
 29%|       | 141150/478625 [00:04<00:10, 31694.18it/s][A
 17%|        | 81143/478625 [00:02<00:12, 32100.66it/s][A
 82%| | 393765/478625 [00:12<00:02, 32936.17it/s][A
 31%|      | 150105/478625 [00:04<00:10, 32231.96it/s][A
 31%|       | 149151/478625 [00:04<00:10, 32132.31it/s][A
 35%|      | 166154/478625 [00:05<00:09, 31824.32it/s][A
 30%|       | 144404/478625 [00:04<00:10, 31941.37it/s][A
 18%|        | 84356/478625 [00:02<00:12, 31494.69it/s][A
 32%|      | 153466/478625 [00:04<00:09, 32635.02it/s][A
 83%| | 397062/478625 [00:12<00:02, 32359.55it/s][A
 32%|      | 152367/478625 [00:04<00:10, 31478.80it/s][A
 35%|      | 169339/478625 [00:05<00:09, 31326.70it/s][A
 31%|       | 147723/478625 [00:04<00:10, 32307.41it/s][A
 18%|        | 87631/478625 [00:02<00:12, 31861.73it/s][A
 33%|      | 156850/478625 [00:04<00:09, 32990.42it/s][A
 84%| | 400420/478625 [00:12<00:02, 32715.93it/s][A
 33%|      | 155668/478625 [00:04<00:10, 31925.34it/s][A
 36%|      | 172602/478625 [00:05<00:09, 31708.73it/s][A
 32%|      | 150957/478625 [00:04<00:10, 31705.75it/s][A
 19%|        | 90821/478625 [00:02<00:12, 31355.97it/s][A
 33%|      | 160152/478625 [00:04<00:09, 32389.13it/s][A
 84%| | 403695/478625 [00:12<00:02, 32222.93it/s][A
 33%|      | 158865/478625 [00:05<00:10, 31471.44it/s][A
 37%|      | 175776/478625 [00:05<00:09, 31382.79it/s][A
 32%|      | 154274/478625 [00:04<00:10, 32133.18it/s][A
 20%|        | 94104/478625 [00:02<00:12, 31786.06it/s][A
 34%|      | 163495/478625 [00:05<00:09, 32694.20it/s][A
 85%| | 407054/478625 [00:12<00:02, 32623.28it/s][A
 34%|      | 162116/478625 [00:05<00:09, 31773.50it/s][A
 37%|      | 179043/478625 [00:05<00:09, 31760.25it/s][A
 33%|      | 157595/478625 [00:04<00:09, 32447.55it/s][A
 20%|        | 97322/478625 [00:03<00:11, 31901.46it/s][A
 86%| | 410421/478625 [00:12<00:02, 32931.86it/s][A
 35%|      | 166768/478625 [00:05<00:09, 32251.59it/s][A
 35%|      | 165403/478625 [00:05<00:09, 32095.96it/s][A
 38%|      | 182222/478625 [00:05<00:09, 31375.59it/s][A
 34%|      | 160843/478625 [00:05<00:10, 31761.43it/s][A
 21%|        | 100515/478625 [00:03<00:12, 31444.02it/s][A
 36%|      | 170104/478625 [00:05<00:09, 32575.84it/s][A
 86%| | 413717/478625 [00:12<00:02, 32246.46it/s][A
 35%|      | 168616/478625 [00:05<00:09, 31468.88it/s][A
 39%|      | 185362/478625 [00:05<00:09, 31104.32it/s][A
 34%|      | 164138/478625 [00:05<00:09, 32107.89it/s][A
 22%|       | 103777/478625 [00:03<00:11, 31745.90it/s][A
 36%|      | 173468/478625 [00:05<00:09, 32888.52it/s][A
 87%| | 416947/478625 [00:12<00:01, 32247.54it/s][A
 36%|      | 171900/478625 [00:05<00:09, 31869.08it/s][A
 39%|      | 188671/478625 [00:05<00:09, 31685.54it/s][A
 35%|      | 167353/478625 [00:05<00:09, 31666.68it/s][A
 22%|       | 107075/478625 [00:03<00:11, 32109.45it/s][A
 37%|      | 176760/478625 [00:05<00:09, 32392.82it/s][A
 88%| | 420175/478625 [00:12<00:01, 31957.87it/s][A
 37%|      | 175091/478625 [00:05<00:09, 31371.84it/s][A
 40%|      | 191842/478625 [00:06<00:09, 30549.39it/s][A
 36%|      | 170644/478625 [00:05<00:09, 32029.95it/s][A
 23%|       | 110289/478625 [00:03<00:11, 31544.40it/s][A
 38%|      | 180111/478625 [00:05<00:09, 32720.45it/s][A
 88%| | 423547/478625 [00:12<00:01, 32475.52it/s][A
 37%|      | 178377/478625 [00:05<00:09, 31804.64it/s][A
 41%|      | 195061/478625 [00:06<00:09, 31023.82it/s][A
 36%|      | 173976/478625 [00:05<00:09, 32408.36it/s][A
 24%|       | 113545/478625 [00:03<00:11, 31841.33it/s][A
 89%| | 426904/478625 [00:13<00:01, 32799.11it/s][A
 38%|      | 183386/478625 [00:05<00:09, 32044.64it/s][A
 38%|      | 181649/478625 [00:05<00:09, 32072.27it/s][A
 41%|     | 198176/478625 [00:06<00:09, 31059.69it/s][A
 37%|      | 177220/478625 [00:05<00:09, 31809.78it/s][A
 24%|       | 116733/478625 [00:03<00:11, 31365.82it/s][A
 90%| | 430187/478625 [00:13<00:01, 32300.82it/s][A
 39%|      | 186758/478625 [00:05<00:08, 32534.22it/s][A
 39%|      | 184860/478625 [00:05<00:09, 31306.49it/s][A
 42%|     | 201288/478625 [00:06<00:08, 30917.45it/s][A
 38%|      | 180488/478625 [00:05<00:09, 32063.84it/s][A
 25%|       | 120023/478625 [00:03<00:11, 31813.66it/s][A
 91%| | 433571/478625 [00:13<00:01, 32751.90it/s][A
 40%|      | 190145/478625 [00:05<00:08, 32926.38it/s][A
 39%|      | 188147/478625 [00:05<00:09, 31761.66it/s][A
 43%|     | 204573/478625 [00:06<00:08, 31486.92it/s][A
 38%|      | 183698/478625 [00:05<00:09, 31482.48it/s][A
 26%|       | 123322/478625 [00:03<00:11, 32158.89it/s][A
 91%|| 436849/478625 [00:13<00:01, 32269.04it/s][A
 40%|      | 193442/478625 [00:05<00:08, 32352.89it/s][A
 40%|      | 191448/478625 [00:06<00:08, 32128.77it/s][A
 43%|     | 207832/478625 [00:06<00:08, 31812.89it/s][A
 39%|      | 187005/478625 [00:05<00:09, 31944.38it/s][A
 26%|       | 126541/478625 [00:03<00:11, 31503.12it/s][A
 92%|| 440219/478625 [00:13<00:01, 32688.13it/s][A
 41%|      | 196793/478625 [00:06<00:08, 32691.67it/s][A
 41%|      | 194665/478625 [00:06<00:09, 31518.04it/s][A
 44%|     | 211017/478625 [00:06<00:08, 31428.31it/s][A
 40%|      | 190348/478625 [00:05<00:08, 32379.89it/s][A
 27%|       | 129781/478625 [00:04<00:10, 31738.24it/s][A
 93%|| 443604/478625 [00:13<00:01, 33030.02it/s][A
 42%|     | 200066/478625 [00:06<00:08, 32259.70it/s][A
 41%|     | 197956/478625 [00:06<00:08, 31925.63it/s][A
 45%|     | 214248/478625 [00:06<00:08, 31686.32it/s][A
 40%|      | 193590/478625 [00:06<00:08, 31808.87it/s][A
 28%|       | 132959/478625 [00:04<00:11, 31276.39it/s][A
 93%|| 446910/478625 [00:13<00:00, 32506.89it/s][A
 42%|     | 203407/478625 [00:06<00:08, 32596.53it/s][A
 42%|     | 201153/478625 [00:06<00:08, 31356.80it/s][A
 45%|     | 217419/478625 [00:06<00:08, 31336.90it/s][A
 41%|      | 196887/478625 [00:06<00:08, 32148.69it/s][A
 28%|       | 136141/478625 [00:04<00:10, 31434.73it/s][A
 94%|| 450276/478625 [00:13<00:00, 32842.94it/s][A
 43%|     | 206705/478625 [00:06<00:08, 32707.35it/s][A
 43%|     | 204403/478625 [00:06<00:08, 31690.56it/s][A
 46%|     | 220752/478625 [00:06<00:08, 31922.34it/s][A
 42%|     | 200125/478625 [00:06<00:08, 31731.60it/s][A
 29%|       | 139431/478625 [00:04<00:10, 31865.92it/s][A

 44%|     | 209978/478625 [00:06<00:08, 32202.63it/s][A 95%|| 453564/478625 [00:13<00:00, 32290.08it/s][A
 43%|     | 207646/478625 [00:06<00:08, 31906.07it/s][A
 47%|     | 224068/478625 [00:07<00:07, 32256.17it/s][A
 43%|     | 203444/478625 [00:06<00:08, 32158.85it/s][A
 30%|       | 142621/478625 [00:04<00:10, 31334.33it/s][A

 45%|     | 213363/478625 [00:06<00:08, 32686.94it/s][A 95%|| 456917/478625 [00:14<00:00, 32653.83it/s][A
 44%|     | 210840/478625 [00:06<00:08, 31407.25it/s][A
 47%|     | 227296/478625 [00:07<00:07, 31704.30it/s][A
 43%|     | 206706/478625 [00:06<00:08, 32293.12it/s][A
 30%|       | 145876/478625 [00:04<00:10, 31690.66it/s][A
 96%|| 460313/478625 [00:14<00:00, 33037.95it/s][A
 45%|     | 216635/478625 [00:06<00:08, 32145.41it/s][A
 45%|     | 214125/478625 [00:06<00:08, 31828.77it/s][A
 48%|     | 230543/478625 [00:07<00:07, 31927.56it/s][A
 44%|     | 209938/478625 [00:06<00:08, 31741.81it/s][A
 31%|       | 149140/478625 [00:04<00:10, 31971.20it/s][A
 46%|     | 220019/478625 [00:06<00:07, 32642.89it/s][A
 97%|| 463620/478625 [00:14<00:00, 32330.39it/s][A
 45%|     | 217312/478625 [00:06<00:08, 31271.03it/s][A
 49%|     | 233760/478625 [00:07<00:07, 31998.55it/s][A
 45%|     | 213256/478625 [00:06<00:08, 32162.04it/s][A
 32%|      | 152340/478625 [00:04<00:10, 31439.82it/s][A
 47%|     | 223402/478625 [00:06<00:07, 32990.73it/s][A
 98%|| 466877/478625 [00:14<00:00, 32399.69it/s][A
 46%|     | 220595/478625 [00:06<00:08, 31727.81it/s][A
 50%|     | 236962/478625 [00:07<00:07, 31612.36it/s][A
 45%|     | 216548/478625 [00:06<00:08, 32385.33it/s][A
 33%|      | 155650/478625 [00:04<00:10, 31925.50it/s][A
 47%|     | 226705/478625 [00:06<00:07, 32477.16it/s][A
 98%|| 470222/478625 [00:14<00:00, 32072.73it/s][A
 47%|     | 223890/478625 [00:07<00:07, 32088.23it/s][A
 50%|     | 240247/478625 [00:07<00:07, 31974.53it/s][A
 46%|     | 219790/478625 [00:06<00:08, 31876.56it/s][A
 33%|      | 158846/478625 [00:05<00:10, 31484.97it/s][A
 48%|     | 230049/478625 [00:07<00:07, 32758.14it/s][A
 99%|| 473601/478625 [00:14<00:00, 32573.75it/s][A
 47%|     | 227102/478625 [00:07<00:07, 31561.65it/s][A
 51%|     | 243447/478625 [00:07<00:07, 31530.28it/s][A
 47%|     | 223114/478625 [00:06<00:07, 32277.39it/s][A
 34%|      | 162017/478625 [00:05<00:10, 31550.42it/s][A
100%|| 476967/478625 [00:14<00:00, 32892.91it/s][A
 49%|     | 233328/478625 [00:07<00:07, 32069.81it/s][A
 48%|     | 230331/478625 [00:07<00:07, 31774.13it/s][A
 52%|    | 246652/478625 [00:07<00:07, 31682.20it/s][A100%|| 478625/478625 [00:14<00:00, 32603.48it/s]
100%|| 1/1 [00:21<00:00, 21.28s/it]100%|| 1/1 [00:21<00:00, 21.28s/it]

 47%|     | 226345/478625 [00:07<00:07, 31802.35it/s][A
 35%|      | 165267/478625 [00:05<00:09, 31829.96it/s][A
 49%|     | 236697/478625 [00:07<00:07, 32542.26it/s][A
 49%|     | 233592/478625 [00:07<00:07, 32020.58it/s][A
 52%|    | 249942/478625 [00:07<00:07, 32039.86it/s][A09/05/2024 19:42:26 - INFO - opensora.dataset.t2v_datasets - no_cap: 0, too_long: 3711, too_short: 2, no_resolution: 0, resolution_mismatch: 0, Counter(sample_size): Counter({'93x160x320': 84930, '29x160x320': 73201, '45x160x320': 68295, '61x160x320': 44578, '77x160x320': 38630, '93x128x320': 17805, '29x128x320': 16948, '93x224x320': 16403, '93x192x320': 15259, '45x128x320': 14788, '61x128x320': 9795, '29x224x320': 8615, '29x192x320': 8528, '45x224x320': 8477, '45x192x320': 8309, '77x128x320': 7730, '61x224x320': 6211, '61x192x320': 5983, '77x224x320': 5788, '77x192x320': 5268, '93x256x320': 3164, '45x256x320': 1510, '29x256x320': 1480, '61x256x320': 1152, '77x256x320': 1090, '93x96x320': 282, '45x96x320': 200, '29x96x320': 169, '61x96x320': 163, '77x96x320': 148}), cnt_movie: 0, cnt_img: 0, before filter: 478625, after filter: 474899

 48%|     | 229657/478625 [00:07<00:07, 32188.03it/s][A
 35%|      | 168452/478625 [00:05<00:09, 31375.95it/s][A
 50%|     | 240089/478625 [00:07<00:07, 32946.92it/s][A
 49%|     | 236797/478625 [00:07<00:07, 31477.68it/s][A
 53%|    | 253148/478625 [00:07<00:07, 31523.85it/s][A
 49%|     | 232899/478625 [00:07<00:07, 32255.09it/s][A
 36%|      | 171712/478625 [00:05<00:09, 31734.98it/s][A
 51%|     | 243388/478625 [00:07<00:07, 32361.64it/s][A
 50%|     | 240088/478625 [00:07<00:07, 31898.21it/s][A
 54%|    | 256433/478625 [00:08<00:06, 31912.06it/s][A
 49%|     | 236127/478625 [00:07<00:07, 31756.84it/s][A
 37%|      | 174888/478625 [00:05<00:09, 31299.48it/s][A
 52%|    | 246723/478625 [00:07<00:07, 32649.54it/s][A
 51%|     | 243281/478625 [00:07<00:07, 31383.87it/s][A09/05/2024 19:42:26 - INFO - opensora.dataset.t2v_datasets - before filter: 478625, after filter: 474899 | motion_score: 474899, cnt_no_motion: 13 | 192077 > 0.95, 0.7 > 65730 Mean: 0.8593367888417824, Var: 0.03075349223473551, Std: 0.17536673639757203, Min: -0.0717548280954361, Max: 1.0

 54%|    | 259627/478625 [00:08<00:07, 31090.50it/s][A
 50%|     | 239445/478625 [00:07<00:07, 32172.78it/s][A
 37%|      | 178182/478625 [00:05<00:09, 31780.04it/s][A
 52%|    | 250063/478625 [00:07<00:07, 32172.25it/s][A
 52%|    | 246557/478625 [00:07<00:07, 31785.56it/s][A
 55%|    | 262975/478625 [00:08<00:06, 31785.49it/s][A
 51%|     | 242665/478625 [00:07<00:07, 31687.89it/s][A
 38%|      | 181416/478625 [00:05<00:09, 31943.32it/s][A
 53%|    | 253442/478625 [00:07<00:06, 32642.12it/s][A
 52%|    | 249843/478625 [00:07<00:07, 32101.64it/s][A
 56%|    | 266278/478625 [00:08<00:06, 32128.88it/s][A
 51%|    | 245962/478625 [00:07<00:07, 32064.00it/s][A
 39%|      | 184613/478625 [00:05<00:09, 31216.98it/s][A09/05/2024 19:42:27 - INFO - opensora.dataset.t2v_datasets - before filter: 478625, after filter: 474899 | aesthetic_score: 478625, cnt_no_aesthetic: 0 | 14374 > 5.75, 4.5 > 113830 Mean: 4.846693657797633, Var: 0.24147353645946146, Std: 0.4913995690468821, Min: 2.685077953338623, Max: 6.742257436116536

 54%|    | 256806/478625 [00:07<00:06, 32935.49it/s][Atime 21.951740264892578

 53%|    | 253056/478625 [00:07<00:07, 31459.42it/s][A
 56%|    | 269496/478625 [00:08<00:06, 31706.50it/s][An_elements: 474899
data length: 474899
09/05/2024 19:42:27 - INFO - __main__ - after train_dataloader
09/05/2024 19:42:27 - INFO - __main__ - before accelerator.prepare
[2024-09-05 19:42:27,082] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown

 52%|    | 249279/478625 [00:07<00:07, 32388.85it/s][A
 39%|      | 187897/478625 [00:05<00:09, 31692.37it/s][A
 54%|    | 260103/478625 [00:07<00:06, 32135.17it/s][A
 54%|    | 256340/478625 [00:08<00:06, 31864.21it/s][A
 57%|    | 272778/478625 [00:08<00:06, 32032.57it/s][A
 53%|    | 252521/478625 [00:07<00:07, 31761.67it/s][A
 40%|      | 191201/478625 [00:06<00:08, 32088.70it/s][A
 55%|    | 263468/478625 [00:08<00:06, 32575.24it/s][A
 58%|    | 275992/478625 [00:08<00:06, 32062.08it/s][A
 54%|    | 259531/478625 [00:08<00:07, 31122.93it/s][A
 53%|    | 255843/478625 [00:07<00:06, 32188.11it/s][A
 41%|      | 194414/478625 [00:06<00:08, 31597.63it/s][A
 56%|    | 266852/478625 [00:08<00:06, 32946.06it/s][A
 55%|    | 262815/478625 [00:08<00:06, 31621.42it/s][A
 58%|    | 279201/478625 [00:08<00:06, 31131.62it/s][A
 54%|    | 259068/478625 [00:08<00:06, 32205.10it/s][A
 41%|     | 197692/478625 [00:06<00:08, 31944.37it/s][A
 56%|    | 270151/478625 [00:08<00:06, 32278.18it/s][A
 56%|    | 266100/478625 [00:08<00:06, 31982.40it/s][A
 59%|    | 282481/478625 [00:08<00:06, 31616.78it/s][A
 55%|    | 262292/478625 [00:08<00:06, 31693.38it/s][A
 42%|     | 200890/478625 [00:06<00:08, 31416.34it/s][A
 57%|    | 273505/478625 [00:08<00:06, 32645.52it/s][A
 56%|    | 269303/478625 [00:08<00:06, 31341.65it/s][A
 60%|    | 285649/478625 [00:09<00:06, 31348.85it/s][A
 55%|    | 265593/478625 [00:08<00:06, 32080.68it/s][A
 43%|     | 204176/478625 [00:06<00:08, 31838.69it/s][A
 58%|    | 276775/478625 [00:08<00:06, 32201.48it/s][A
 57%|    | 272574/478625 [00:08<00:06, 31739.08it/s][A
 60%|    | 288893/478625 [00:09<00:05, 31667.49it/s][A
 56%|    | 268805/478625 [00:08<00:06, 31614.57it/s][A
 43%|     | 207383/478625 [00:06<00:08, 31903.77it/s][A
 59%|    | 280135/478625 [00:08<00:06, 32608.95it/s][A
 58%|    | 275902/478625 [00:08<00:06, 32192.05it/s][A
 61%|    | 292064/478625 [00:09<00:05, 31621.89it/s][A
 57%|    | 272110/478625 [00:08<00:06, 32034.73it/s][A
 44%|     | 210576/478625 [00:06<00:08, 31455.37it/s][A
 59%|    | 283495/478625 [00:08<00:05, 32901.10it/s][A
 58%|    | 279126/478625 [00:08<00:06, 31734.51it/s][A
 62%|   | 295229/478625 [00:09<00:05, 31227.78it/s][A
 58%|    | 275411/478625 [00:08<00:06, 32322.67it/s][A
 45%|     | 213858/478625 [00:06<00:08, 31854.28it/s][A
 60%|    | 286789/478625 [00:08<00:05, 32259.82it/s][A
 59%|    | 282470/478625 [00:08<00:06, 32235.26it/s][A
 62%|   | 298573/478625 [00:09<00:05, 31876.67it/s][A
 58%|    | 278646/478625 [00:08<00:06, 31775.77it/s][A
 45%|     | 217047/478625 [00:06<00:08, 31346.54it/s][A
 61%|    | 290156/478625 [00:08<00:05, 32671.83it/s][A
 60%|    | 285698/478625 [00:09<00:06, 31770.27it/s][A
 63%|   | 301764/478625 [00:09<00:05, 31469.52it/s][A
 59%|    | 281960/478625 [00:08<00:06, 32174.39it/s][A
 46%|     | 220295/478625 [00:06<00:08, 31678.28it/s][A
 61%|   | 293428/478625 [00:09<00:05, 31948.92it/s][A
 60%|    | 289030/478625 [00:09<00:05, 32223.19it/s][A
 64%|   | 305016/478625 [00:09<00:05, 31776.22it/s][A
 60%|    | 285181/478625 [00:08<00:06, 31733.75it/s][A
 47%|     | 223563/478625 [00:07<00:07, 31971.89it/s][A
 62%|   | 296783/478625 [00:09<00:05, 32415.26it/s][A
 61%|    | 292282/478625 [00:09<00:05, 32280.38it/s][A
 64%|   | 308352/478625 [00:09<00:05, 32242.75it/s][A
 60%|    | 288470/478625 [00:09<00:05, 32071.12it/s][A
 47%|     | 226763/478625 [00:07<00:07, 31544.18it/s][A
 63%|   | 300130/478625 [00:09<00:05, 32723.80it/s][A
 62%|   | 295513/478625 [00:09<00:05, 31847.45it/s][A
 65%|   | 311579/478625 [00:09<00:05, 31424.68it/s][A
 61%|    | 291700/478625 [00:09<00:05, 32136.98it/s][A
 48%|     | 230004/478625 [00:07<00:07, 31798.49it/s][A
 63%|   | 303407/478625 [00:09<00:05, 32241.26it/s][A
 62%|   | 298854/478625 [00:09<00:05, 32307.52it/s][A
 66%|   | 314871/478625 [00:09<00:05, 31860.74it/s][A
 62%|   | 294916/478625 [00:09<00:05, 31672.72it/s][A
 49%|     | 233207/478625 [00:07<00:07, 31864.21it/s][A
 64%|   | 306755/478625 [00:09<00:05, 32604.80it/s][A
 63%|   | 302088/478625 [00:09<00:05, 31813.38it/s][A
 66%|   | 318104/478625 [00:10<00:05, 31996.69it/s][A
 62%|   | 298220/478625 [00:09<00:05, 32074.61it/s][A
 49%|     | 236396/478625 [00:07<00:07, 31348.83it/s][A
 65%|   | 310019/478625 [00:09<00:05, 32111.30it/s][A
 64%|   | 305416/478625 [00:09<00:05, 32242.39it/s][A
 67%|   | 321308/478625 [00:10<00:05, 30979.39it/s][A
 63%|   | 301431/478625 [00:09<00:05, 31607.55it/s][A
 50%|     | 239662/478625 [00:07<00:07, 31733.98it/s][A
 65%|   | 313390/478625 [00:09<00:05, 32579.64it/s][A
 65%|   | 308737/478625 [00:09<00:05, 32527.46it/s][A
 68%|   | 324659/478625 [00:10<00:04, 31712.31it/s][A
 64%|   | 304737/478625 [00:09<00:05, 32032.33it/s][A
 51%|     | 242838/478625 [00:07<00:07, 31255.35it/s][A
 66%|   | 316735/478625 [00:09<00:04, 32835.54it/s][A
 65%|   | 311993/478625 [00:09<00:05, 32039.14it/s][A
 68%|   | 327839/478625 [00:10<00:04, 30984.65it/s][A
 64%|   | 308041/478625 [00:09<00:05, 32328.06it/s][A
 51%|    | 246112/478625 [00:07<00:07, 31691.11it/s][A
 67%|   | 320022/478625 [00:09<00:04, 32258.69it/s][A
 66%|   | 315336/478625 [00:09<00:05, 32447.71it/s][A
 69%|   | 331167/478625 [00:10<00:04, 31649.64it/s][A
 65%|   | 311277/478625 [00:09<00:05, 31829.15it/s][A
 52%|    | 249382/478625 [00:07<00:07, 31987.59it/s][A
 68%|   | 323344/478625 [00:09<00:04, 32539.16it/s][A
 67%|   | 318584/478625 [00:10<00:05, 31928.44it/s][A
 70%|   | 334340/478625 [00:10<00:04, 31541.28it/s][A
 66%|   | 314577/478625 [00:09<00:05, 32171.85it/s][A
 53%|    | 252584/478625 [00:07<00:07, 31196.54it/s][A
 68%|   | 326602/478625 [00:10<00:04, 32010.57it/s][A
 67%|   | 321894/478625 [00:10<00:04, 32270.63it/s][A
 71%|   | 337500/478625 [00:10<00:04, 30839.38it/s][A
 66%|   | 317888/478625 [00:09<00:04, 32449.16it/s][A
 53%|    | 255834/478625 [00:08<00:07, 31576.51it/s][A
 69%|   | 329873/478625 [00:10<00:04, 32212.76it/s][A
 68%|   | 325223/478625 [00:10<00:04, 32571.40it/s][A
 71%|   | 340813/478625 [00:10<00:04, 31504.61it/s][A
 67%|   | 321136/478625 [00:10<00:04, 31855.27it/s][A
 54%|    | 258997/478625 [00:08<00:06, 31552.08it/s][A
 70%|   | 333225/478625 [00:10<00:04, 32595.80it/s][A
 69%|   | 328483/478625 [00:10<00:04, 31775.70it/s][A
 72%|  | 343970/478625 [00:10<00:04, 31202.25it/s][A
 68%|   | 324450/478625 [00:10<00:04, 32232.22it/s][A
 55%|    | 262156/478625 [00:08<00:06, 31060.64it/s][A
 70%|   | 336488/478625 [00:10<00:04, 32102.48it/s][A
 69%|   | 331805/478625 [00:10<00:04, 32197.13it/s][A
 73%|  | 347309/478625 [00:10<00:04, 31842.50it/s][A
 68%|   | 327677/478625 [00:10<00:04, 31608.40it/s][A
 55%|    | 265424/478625 [00:08<00:06, 31535.68it/s][A
 71%|   | 339837/478625 [00:10<00:04, 32510.31it/s][A
 70%|   | 335156/478625 [00:10<00:04, 32581.34it/s][A
 73%|  | 350498/478625 [00:11<00:04, 31783.88it/s][A
 69%|   | 330895/478625 [00:10<00:04, 31773.70it/s][A
 56%|    | 268581/478625 [00:08<00:06, 31092.66it/s][A
 72%|  | 343091/478625 [00:10<00:04, 32065.08it/s][A
 71%|   | 338419/478625 [00:10<00:04, 31930.10it/s][A
 74%|  | 353680/478625 [00:11<00:03, 31434.87it/s][A
 70%|   | 334205/478625 [00:10<00:04, 32162.64it/s][A
 57%|    | 271810/478625 [00:08<00:06, 31443.40it/s][A
 72%|  | 346432/478625 [00:10<00:04, 32460.23it/s][A
 71%|  | 341761/478625 [00:10<00:04, 32364.06it/s][A
 75%|  | 357005/478625 [00:11<00:03, 31969.85it/s][A
 70%|   | 337425/478625 [00:10<00:04, 31590.15it/s][A
 57%|    | 275100/478625 [00:08<00:06, 31873.57it/s][A
 73%|  | 349803/478625 [00:10<00:03, 32829.38it/s][A
 72%|  | 345002/478625 [00:10<00:04, 31837.36it/s][A
 75%|  | 360301/478625 [00:11<00:03, 32260.86it/s][A
 71%|   | 340751/478625 [00:10<00:04, 32077.10it/s][A
 58%|    | 278291/478625 [00:08<00:06, 31280.68it/s][A
 74%|  | 353089/478625 [00:10<00:03, 32241.52it/s][A
 73%|  | 348348/478625 [00:10<00:04, 32311.14it/s][A
 76%|  | 363530/478625 [00:11<00:03, 31794.94it/s][A
 72%|  | 343963/478625 [00:10<00:04, 31575.31it/s][A
 59%|    | 281581/478625 [00:08<00:06, 31756.01it/s][A
 74%|  | 356453/478625 [00:10<00:03, 32652.58it/s][A
 73%|  | 351682/478625 [00:11<00:03, 32613.46it/s][A
 77%|  | 366860/478625 [00:11<00:03, 32237.90it/s][A
 73%|  | 347261/478625 [00:10<00:04, 31984.40it/s][A
 59%|    | 284761/478625 [00:08<00:06, 31304.79it/s][A
 75%|  | 359795/478625 [00:11<00:03, 32136.57it/s][A
 74%|  | 354947/478625 [00:11<00:03, 32028.38it/s][A
 77%|  | 370087/478625 [00:11<00:03, 31653.87it/s][A
 73%|  | 350579/478625 [00:10<00:03, 32336.12it/s][A
 60%|    | 288064/478625 [00:09<00:05, 31811.45it/s][A
 76%|  | 363151/478625 [00:11<00:03, 32552.24it/s][A
 75%|  | 358312/478625 [00:11<00:03, 32504.00it/s][A
 78%|  | 373474/478625 [00:11<00:03, 32302.80it/s][A
 74%|  | 353816/478625 [00:11<00:03, 31770.77it/s][A
 61%|    | 291340/478625 [00:09<00:05, 32091.10it/s][A
 77%|  | 366489/478625 [00:11<00:03, 32795.41it/s][A
 76%|  | 361567/478625 [00:11<00:03, 32000.78it/s][A
 79%|  | 376840/478625 [00:11<00:03, 32700.63it/s][A
 75%|  | 357121/478625 [00:11<00:03, 32143.25it/s][A
 62%|   | 294552/478625 [00:09<00:05, 31287.39it/s][A
 77%|  | 369772/478625 [00:11<00:03, 32005.83it/s][A
 76%|  | 364907/478625 [00:11<00:03, 32408.15it/s][A
 79%|  | 380114/478625 [00:11<00:03, 32187.00it/s][A
 75%|  | 360438/478625 [00:11<00:03, 32444.10it/s][A
 62%|   | 297817/478625 [00:09<00:05, 31684.37it/s][A
 78%|  | 373160/478625 [00:11<00:03, 32517.89it/s][A
 77%|  | 368226/478625 [00:11<00:03, 32636.51it/s][A
 80%|  | 383452/478625 [00:12<00:02, 32537.26it/s][A
 76%|  | 363686/478625 [00:11<00:03, 31916.54it/s][A
 63%|   | 301081/478625 [00:09<00:05, 31963.77it/s][A
 79%|  | 376528/478625 [00:11<00:03, 32859.12it/s][A
 78%|  | 371493/478625 [00:11<00:03, 31789.74it/s][A
 81%|  | 386709/478625 [00:12<00:02, 31721.81it/s][A
 77%|  | 366973/478625 [00:11<00:03, 32196.60it/s][A
 64%|   | 304282/478625 [00:09<00:05, 31473.31it/s][A
 79%|  | 379819/478625 [00:11<00:03, 32312.80it/s][A
 78%|  | 374836/478625 [00:11<00:03, 32268.46it/s][A
 81%| | 390057/478625 [00:12<00:02, 32234.30it/s][A
 77%|  | 370196/478625 [00:11<00:03, 31522.76it/s][A
 64%|   | 307552/478625 [00:09<00:05, 31832.77it/s][A
 80%|  | 383203/478625 [00:11<00:02, 32760.73it/s][A
 79%|  | 378069/478625 [00:11<00:03, 31815.18it/s][A
 82%| | 393444/478625 [00:12<00:02, 32713.30it/s][A
 78%|  | 373501/478625 [00:11<00:03, 31969.34it/s][A
 65%|   | 310739/478625 [00:09<00:05, 31306.10it/s][A
 81%|  | 386484/478625 [00:11<00:02, 32263.81it/s][A
 80%|  | 381433/478625 [00:11<00:03, 32347.64it/s][A
 83%| | 396721/478625 [00:12<00:02, 32007.92it/s][A
 79%|  | 376819/478625 [00:11<00:03, 32324.23it/s][A
 66%|   | 314017/478625 [00:09<00:05, 31735.84it/s][A
 81%| | 389847/478625 [00:11<00:02, 32662.21it/s][A
 80%|  | 384771/478625 [00:12<00:02, 32651.13it/s][A
 84%| | 400063/478625 [00:12<00:02, 32419.05it/s][A
 79%|  | 380055/478625 [00:11<00:03, 31836.25it/s][A
 66%|   | 317278/478625 [00:10<00:05, 31992.98it/s][A
 82%| | 393217/478625 [00:12<00:02, 32967.19it/s][A
 81%|  | 388040/478625 [00:12<00:02, 32043.29it/s][A
 84%| | 403311/478625 [00:12<00:02, 32014.36it/s][A
 80%|  | 383371/478625 [00:11<00:02, 32222.74it/s][A
 67%|   | 320481/478625 [00:10<00:05, 31449.04it/s][A
 83%| | 396517/478625 [00:12<00:02, 32348.65it/s][A
 82%| | 391376/478625 [00:12<00:02, 32429.10it/s][A
 85%| | 406647/478625 [00:12<00:02, 32408.23it/s][A
 81%|  | 386597/478625 [00:12<00:02, 31748.76it/s][A
 68%|   | 323761/478625 [00:10<00:04, 31843.82it/s][A
 84%| | 399879/478625 [00:12<00:02, 32720.01it/s][A
 82%| | 394623/478625 [00:12<00:02, 31917.25it/s][A
 86%| | 410010/478625 [00:12<00:02, 32767.41it/s][A
 81%| | 389920/478625 [00:12<00:02, 32183.56it/s][A
 68%|   | 326949/478625 [00:10<00:04, 31310.76it/s][A
 84%| | 403155/478625 [00:12<00:02, 32261.80it/s][A
 83%| | 397946/478625 [00:12<00:02, 32299.44it/s][A
 86%| | 413290/478625 [00:13<00:02, 32037.74it/s][A
 82%| | 393248/478625 [00:12<00:02, 32506.61it/s][A
 69%|   | 330085/478625 [00:10<00:04, 31323.71it/s][A
 85%| | 406511/478625 [00:12<00:02, 32642.08it/s][A
 84%| | 401268/478625 [00:12<00:02, 32570.37it/s][A
 87%| | 416499/478625 [00:13<00:01, 31991.11it/s][A
 83%| | 396502/478625 [00:12<00:02, 31828.57it/s][A
 70%|   | 333306/478625 [00:10<00:04, 31583.74it/s][A
 86%| | 409867/478625 [00:12<00:02, 32904.73it/s][A
 85%| | 404528/478625 [00:12<00:02, 32038.89it/s][A
 88%| | 419702/478625 [00:13<00:01, 31706.86it/s][A
 84%| | 399833/478625 [00:12<00:02, 32262.51it/s][A
 70%|   | 336467/478625 [00:10<00:04, 31132.34it/s][A
 86%| | 413161/478625 [00:12<00:02, 32253.44it/s][A
 85%| | 407845/478625 [00:12<00:02, 32369.35it/s][A
 88%| | 423022/478625 [00:13<00:01, 32144.44it/s][A
 84%| | 403064/478625 [00:12<00:02, 31748.62it/s][A
 71%|   | 339735/478625 [00:10<00:04, 31586.97it/s][A
 87%| | 416414/478625 [00:12<00:01, 32333.40it/s][A
 86%| | 411109/478625 [00:12<00:02, 31865.73it/s][A
 89%| | 426384/478625 [00:13<00:01, 32580.75it/s][A
 85%| | 406371/478625 [00:12<00:02, 32132.84it/s][A
 72%|  | 343004/478625 [00:10<00:04, 31912.65it/s][A
 88%| | 419651/478625 [00:12<00:01, 31940.92it/s][A
 87%| | 414429/478625 [00:13<00:01, 32254.47it/s][A
 90%| | 429645/478625 [00:13<00:01, 32137.38it/s][A
 86%| | 409704/478625 [00:12<00:02, 32482.61it/s][A
 72%|  | 346198/478625 [00:10<00:04, 31317.41it/s][A
 88%| | 423033/478625 [00:13<00:01, 32491.16it/s][A
 87%| | 417658/478625 [00:13<00:01, 32229.71it/s][A
 90%| | 433050/478625 [00:13<00:01, 32699.44it/s][A
 86%| | 412956/478625 [00:12<00:02, 31782.57it/s][A
 73%|  | 349463/478625 [00:11<00:04, 31707.19it/s][A
 89%| | 426384/478625 [00:13<00:01, 32791.42it/s][A
 88%| | 420884/478625 [00:13<00:01, 31861.94it/s][A
 91%| | 436433/478625 [00:13<00:01, 33033.14it/s][A
 87%| | 416163/478625 [00:12<00:01, 31864.25it/s][A
 74%|  | 352638/478625 [00:11<00:04, 31283.86it/s][A
 90%| | 429666/478625 [00:13<00:01, 32272.67it/s][A
 89%| | 424185/478625 [00:13<00:01, 32198.80it/s][A
 92%|| 439739/478625 [00:13<00:01, 32466.97it/s][A
 88%| | 419499/478625 [00:13<00:01, 32303.51it/s][A
 74%|  | 355877/478625 [00:11<00:03, 31606.77it/s][A
 90%| | 433013/478625 [00:13<00:01, 32625.04it/s][A
 89%| | 427511/478625 [00:13<00:01, 32511.89it/s][A
 93%|| 443085/478625 [00:13<00:01, 32757.71it/s][A
 88%| | 422733/478625 [00:13<00:01, 31794.87it/s][A
 75%|  | 359145/478625 [00:11<00:03, 31923.37it/s][A
 91%| | 436279/478625 [00:13<00:01, 32177.29it/s][A
 90%| | 430765/478625 [00:13<00:01, 32062.99it/s][A
 93%|| 446364/478625 [00:14<00:01, 32157.34it/s][A
 89%| | 426045/478625 [00:13<00:01, 32183.87it/s][A
 76%|  | 362340/478625 [00:11<00:03, 31461.63it/s][A
 92%|| 439610/478625 [00:13<00:01, 32509.29it/s][A
 91%| | 434095/478625 [00:13<00:01, 32426.03it/s][A
 94%|| 449722/478625 [00:14<00:00, 32573.29it/s][A
 90%| | 429267/478625 [00:13<00:01, 31735.80it/s][A
 76%|  | 365558/478625 [00:11<00:03, 31670.90it/s][A
 93%|| 442977/478625 [00:13<00:01, 32852.37it/s][A
 91%|| 437340/478625 [00:13<00:01, 31906.13it/s][A
 95%|| 452992/478625 [00:14<00:00, 32608.66it/s][A
 90%| | 432541/478625 [00:13<00:01, 32029.06it/s][A
 77%|  | 368728/478625 [00:11<00:03, 31653.21it/s][A
 93%|| 446265/478625 [00:13<00:01, 32355.73it/s][A
 92%|| 440663/478625 [00:13<00:01, 32275.33it/s][A
 95%|| 456256/478625 [00:14<00:00, 32077.47it/s][A
 91%| | 435870/478625 [00:13<00:01, 32400.61it/s][A
 78%|  | 371895/478625 [00:11<00:03, 31141.81it/s][A
 94%|| 449642/478625 [00:13<00:00, 32770.64it/s][A
 93%|| 444019/478625 [00:13<00:01, 32652.73it/s][A
 96%|| 459641/478625 [00:14<00:00, 32596.54it/s][A
 92%|| 439113/478625 [00:13<00:01, 31888.34it/s][A
 78%|  | 375151/478625 [00:11<00:03, 31558.74it/s][A
 95%|| 452922/478625 [00:13<00:00, 32191.71it/s][A
 93%|| 447287/478625 [00:14<00:00, 32112.93it/s][A
 97%|| 462904/478625 [00:14<00:00, 32164.47it/s][A
 92%|| 442426/478625 [00:13<00:01, 32251.29it/s][A
 79%|  | 378310/478625 [00:11<00:03, 30974.98it/s][A
 95%|| 456295/478625 [00:14<00:00, 32641.81it/s][A
 94%|| 450606/478625 [00:14<00:00, 32428.69it/s][A
 97%|| 466174/478625 [00:14<00:00, 32321.60it/s][A
 93%|| 445654/478625 [00:13<00:01, 31754.84it/s][A
 80%|  | 381607/478625 [00:12<00:03, 31552.36it/s][A
 96%|| 459676/478625 [00:14<00:00, 32954.33it/s][A
 95%|| 453852/478625 [00:14<00:00, 31961.64it/s][A
 98%|| 469409/478625 [00:14<00:00, 32239.04it/s][A
 94%|| 449010/478625 [00:14<00:00, 32283.94it/s][A
 80%|  | 384873/478625 [00:12<00:02, 31877.21it/s][A
 97%|| 462975/478625 [00:14<00:00, 32446.14it/s][A
 96%|| 457161/478625 [00:14<00:00, 32290.24it/s][A
 99%|| 472635/478625 [00:14<00:00, 31873.32it/s][A
 94%|| 452300/478625 [00:14<00:00, 32465.16it/s][A
 81%|  | 388064/478625 [00:12<00:02, 31428.33it/s][A
 97%|| 466314/478625 [00:14<00:00, 32722.70it/s][A
 96%|| 460496/478625 [00:14<00:00, 32602.52it/s][A
 99%|| 476016/478625 [00:14<00:00, 32443.90it/s][A
 95%|| 455550/478625 [00:14<00:00, 31928.21it/s][A
 82%| | 391346/478625 [00:12<00:02, 31836.32it/s][A100%|| 478625/478625 [00:15<00:00, 31831.46it/s]
100%|| 1/1 [00:21<00:00, 21.66s/it]100%|| 1/1 [00:21<00:00, 21.67s/it]

 98%|| 469590/478625 [00:14<00:00, 31876.39it/s][A
 97%|| 463759/478625 [00:14<00:00, 31955.34it/s][A
 96%|| 458879/478625 [00:14<00:00, 32328.25it/s][A
 82%| | 394533/478625 [00:12<00:02, 31360.17it/s][A
 99%|| 472954/478625 [00:14<00:00, 32388.58it/s][A
 98%|| 466989/478625 [00:14<00:00, 32054.67it/s][A
 97%|| 462115/478625 [00:14<00:00, 31820.55it/s][A
 83%| | 397776/478625 [00:12<00:02, 31643.22it/s][A
100%|| 476310/478625 [00:14<00:00, 32706.75it/s][A
 98%|| 470211/478625 [00:14<00:00, 31678.16it/s][A
 97%|| 465374/478625 [00:14<00:00, 32044.35it/s][A
 84%| | 401074/478625 [00:12<00:02, 32036.62it/s][A100%|| 478625/478625 [00:14<00:00, 32527.73it/s]
100%|| 1/1 [00:21<00:00, 21.35s/it]100%|| 1/1 [00:21<00:00, 21.35s/it]

 99%|| 473519/478625 [00:14<00:00, 32089.15it/s][A
 98%|| 468582/478625 [00:14<00:00, 32002.20it/s][A
 84%| | 404281/478625 [00:12<00:02, 31491.29it/s][A
100%|| 476862/478625 [00:14<00:00, 32482.88it/s][A
 99%|| 471785/478625 [00:14<00:00, 31572.72it/s][A100%|| 478625/478625 [00:14<00:00, 31911.82it/s]
100%|| 1/1 [00:21<00:00, 21.65s/it]100%|| 1/1 [00:21<00:00, 21.65s/it]

 85%| | 407540/478625 [00:12<00:02, 31813.55it/s][A
 99%|| 475117/478625 [00:14<00:00, 32086.15it/s][A
 86%| | 410794/478625 [00:12<00:02, 32025.52it/s][A
100%|| 478430/478625 [00:14<00:00, 32394.34it/s][A100%|| 478625/478625 [00:14<00:00, 32041.22it/s]
100%|| 1/1 [00:21<00:00, 21.49s/it]100%|| 1/1 [00:21<00:00, 21.49s/it]
time 22.34464454650879

 86%| | 413999/478625 [00:13<00:02, 31139.77it/s][An_elements: 474899
data length: 474899

 87%| | 417185/478625 [00:13<00:01, 31348.62it/s][A
 88%| | 420325/478625 [00:13<00:01, 30695.11it/s][Atime 21.991446256637573
n_elements: 474899
data length: 474899

 89%| | 423594/478625 [00:13<00:01, 31275.66it/s][A
 89%| | 426811/478625 [00:13<00:01, 31537.75it/s][Atime 22.311201095581055
n_elements: 474899
data length: 474899

 90%| | 429970/478625 [00:13<00:01, 31115.72it/s][A
 91%| | 433213/478625 [00:13<00:01, 31499.94it/s][Atime 22.154942750930786
n_elements: 474899
data length: 474899

 91%| | 436390/478625 [00:13<00:01, 31577.17it/s][A
 92%|| 439551/478625 [00:13<00:01, 31131.97it/s][A
 93%|| 442798/478625 [00:14<00:01, 31526.02it/s][A
 93%|| 445954/478625 [00:14<00:01, 30843.54it/s][A
 94%|| 449222/478625 [00:14<00:00, 31379.24it/s][A
 95%|| 452500/478625 [00:14<00:00, 31788.65it/s][A
 95%|| 455683/478625 [00:14<00:00, 31321.61it/s][A
 96%|| 458980/478625 [00:14<00:00, 31803.59it/s][A
 97%|| 462164/478625 [00:14<00:00, 31254.71it/s][A
 97%|| 465403/478625 [00:14<00:00, 31584.83it/s][A
 98%|| 468565/478625 [00:14<00:00, 31249.25it/s][A
 99%|| 471693/478625 [00:14<00:00, 30515.57it/s][A
 99%|| 474857/478625 [00:15<00:00, 30841.04it/s][A
100%|| 477983/478625 [00:15<00:00, 30961.69it/s][A100%|| 478625/478625 [00:15<00:00, 31575.03it/s]
100%|| 1/1 [00:21<00:00, 21.79s/it]100%|| 1/1 [00:21<00:00, 21.79s/it]
time 22.4544620513916
n_elements: 474899
data length: 474899
[2024-09-05 19:42:54,936] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-09-05 19:42:54,944] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-09-05 19:42:54,944] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-05 19:42:55,093] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-09-05 19:42:55,093] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch_npu.utils._optim.partialclass.<locals>.NewCls'>
[2024-09-05 19:42:55,093] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-09-05 19:42:55,093] [INFO] [stage_1_and_2.py:173:__init__] Reduce bucket size 536870912
[2024-09-05 19:42:55,093] [INFO] [stage_1_and_2.py:174:__init__] Allgather bucket size 536870912
[2024-09-05 19:42:55,093] [INFO] [stage_1_and_2.py:175:__init__] CPU Offload: False
[2024-09-05 19:42:55,093] [INFO] [stage_1_and_2.py:176:__init__] Round robin gradient partitioning: False
zp rank is 0, zp_size=8
zp rank is 5, zp_size=8
zp rank is 6, zp_size=8
zp rank is 1, zp_size=8
zp rank is 4, zp_size=8
zp rank is 7, zp_size=8
zp rank is 2, zp_size=8
zp rank is 3, zp_size=8
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 935, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 766, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 739, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 449, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/module.py", line 351, in mpdl_iter_init
    origin_mpdl_iter_init(self, *args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1039, in __init__
    w.start()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0xffff1ce4d550>: attribute lookup <lambda> on opensora.models.causalvideovae failed
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 935, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 766, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 739, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 449, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/module.py", line 351, in mpdl_iter_init
    origin_mpdl_iter_init(self, *args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1039, in __init__
    w.start()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0xfffefb366a60>: attribute lookup <lambda> on opensora.models.causalvideovae failed
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 935, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 766, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 739, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 449, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/module.py", line 351, in mpdl_iter_init
    origin_mpdl_iter_init(self, *args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1039, in __init__
    w.start()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0xfffef4c2b820>: attribute lookup <lambda> on opensora.models.causalvideovae failed
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 935, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 766, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 739, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 449, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/module.py", line 351, in mpdl_iter_init
    origin_mpdl_iter_init(self, *args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1039, in __init__
    w.start()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0xffff00b67ca0>: attribute lookup <lambda> on opensora.models.causalvideovae failed
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 935, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 766, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 739, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 449, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/module.py", line 351, in mpdl_iter_init
    origin_mpdl_iter_init(self, *args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1039, in __init__
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 935, in <module>
    w.start()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 47, in _launch
        main(args)reduction.dump(process_obj, fp)

  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/reduction.py", line 60, in dump
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 766, in main
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0xffff060eb820>: attribute lookup <lambda> on opensora.models.causalvideovae failed
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 739, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 449, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/module.py", line 351, in mpdl_iter_init
    origin_mpdl_iter_init(self, *args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1039, in __init__
    w.start()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0xfffef0e4e5e0>: attribute lookup <lambda> on opensora.models.causalvideovae failed
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 935, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 766, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 739, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 449, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/module.py", line 351, in mpdl_iter_init
    origin_mpdl_iter_init(self, *args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1039, in __init__
    w.start()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0xffff240ae790>: attribute lookup <lambda> on opensora.models.causalvideovae failed
[2024-09-05 19:43:00,342] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
[2024-09-05 19:43:00,343] [INFO] [utils.py:792:see_memory_usage] MA 17.67 GB         Max_MA 18.33 GB         CA 18.7 GB         Max_CA 19 GB 
[2024-09-05 19:43:00,344] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 195.08 GB, percent = 12.9%
[2024-09-05 19:43:02,243] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
[2024-09-05 19:43:02,245] [INFO] [utils.py:792:see_memory_usage] MA 20.3 GB         Max_MA 24.24 GB         CA 25.27 GB         Max_CA 25 GB 
[2024-09-05 19:43:02,245] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 148.04 GB, percent = 9.8%
[2024-09-05 19:43:02,245] [INFO] [stage_1_and_2.py:552:__init__] optimizer state initialized
[2024-09-05 19:43:04,101] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
[2024-09-05 19:43:04,102] [INFO] [utils.py:792:see_memory_usage] MA 20.3 GB         Max_MA 20.3 GB         CA 25.27 GB         Max_CA 25 GB 
[2024-09-05 19:43:04,102] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 101.25 GB, percent = 6.7%
[2024-09-05 19:43:04,110] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-09-05 19:43:04,111] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-05 19:43:04,111] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-05 19:43:04,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.999)]
[2024-09-05 19:43:04,114] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2024-09-05 19:43:04,114] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-05 19:43:04,114] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-05 19:43:04,114] [INFO] [config.py:988:print]   amp_enabled .................. False
[2024-09-05 19:43:04,114] [INFO] [config.py:988:print]   amp_params ................... False
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0xffff23c3fb50>
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   communication_data_type ...... torch.float32
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   disable_allgather ............ False
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   dump_state ................... False
[2024-09-05 19:43:04,115] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   fp16_enabled ................. False
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   global_rank .................. 0
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2024-09-05 19:43:04,116] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   optimizer_name ............... None
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   optimizer_params ............. None
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   pld_enabled .................. False
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   pld_params ................... False
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   scheduler_name ............... None
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   scheduler_params ............. None
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-05 19:43:04,117] [INFO] [config.py:988:print]   sparse_attention ............. None
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   steps_per_print .............. inf
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   train_batch_size ............. 8
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  1
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   world_size ................... 8
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=536870912 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=536870912 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   zero_enabled ................. True
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-05 19:43:04,118] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2
[2024-09-05 19:43:04,118] [INFO] [config.py:974:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "communication_data_type": "fp32", 
    "gradient_clipping": 1.0, 
    "train_micro_batch_size_per_gpu": 1, 
    "train_batch_size": 8, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "overlap_comm": true, 
        "allgather_bucket_size": 5.368709e+08, 
        "contiguous_gradients": true, 
        "reduce_bucket_size": 5.368709e+08
    }, 
    "steps_per_print": inf, 
    "zero_allow_untested_optimizer": true
}
09/05/2024 19:43:04 - INFO - __main__ - after accelerator.prepare
09/05/2024 19:43:04 - INFO - __main__ - init trackers...
wandb: Currently logged in as: pkuhxy (pkuhxy-Peking University). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/image_data/hxy/Open-Sora-Plan/wandb/run-20240905_194309-hhdazhf4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-morning-5
wandb:  View project at https://wandb.ai/pkuhxy-Peking%20University/inpaint_93x320x320_stage1_swap
wandb:  View run at https://wandb.ai/pkuhxy-Peking%20University/inpaint_93x320x320_stage1_swap/runs/hhdazhf4
09/05/2024 19:43:11 - INFO - __main__ - ***** Running training *****
09/05/2024 19:43:11 - INFO - __main__ -   Model = DeepSpeedEngine(
  (module): OpenSoraInpaint(
    (pos_embed): PatchEmbed2D(
      (proj): Conv2d(8, 2304, kernel_size=(2, 2), stride=(2, 2))
    )
    (transformer_blocks): ModuleList(
      (0-31): 32 x BasicTransformerBlock(
        (norm1): LayerNorm((2304,), eps=1e-06, elementwise_affine=False)
        (attn1): Attention(
          (to_q): Linear(in_features=2304, out_features=2304, bias=True)
          (to_k): Linear(in_features=2304, out_features=2304, bias=True)
          (to_v): Linear(in_features=2304, out_features=2304, bias=True)
          (to_out): ModuleList(
            (0): Linear(in_features=2304, out_features=2304, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((2304,), eps=1e-06, elementwise_affine=False)
        (attn2): Attention(
          (to_q): Linear(in_features=2304, out_features=2304, bias=True)
          (to_k): Linear(in_features=2304, out_features=2304, bias=True)
          (to_v): Linear(in_features=2304, out_features=2304, bias=True)
          (to_out): ModuleList(
            (0): Linear(in_features=2304, out_features=2304, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ff): FeedForward(
          (net): ModuleList(
            (0): GELU(
              (proj): Linear(in_features=2304, out_features=9216, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=9216, out_features=2304, bias=True)
          )
        )
      )
    )
    (norm_out): LayerNorm((2304,), eps=1e-06, elementwise_affine=False)
    (proj_out): Linear(in_features=2304, out_features=32, bias=True)
    (adaln_single): AdaLayerNormSingle(
      (emb): PixArtAlphaCombinedTimestepSizeEmbeddings(
        (time_proj): Timesteps()
        (timestep_embedder): TimestepEmbedding(
          (linear_1): Linear(in_features=256, out_features=2304, bias=True)
          (act): SiLU()
          (linear_2): Linear(in_features=2304, out_features=2304, bias=True)
        )
      )
      (silu): SiLU()
      (linear): Linear(in_features=2304, out_features=13824, bias=True)
    )
    (caption_projection): PixArtAlphaTextProjection(
      (linear_1): Linear(in_features=4096, out_features=2304, bias=True)
      (act_1): GELU(approximate='tanh')
      (linear_2): Linear(in_features=2304, out_features=2304, bias=True)
    )
    (motion_projection): MotionAdaLayerNormSingle(
      (emb): MotionEmbeddings(
        (motion_proj): Timesteps()
        (motion_embedder): TimestepEmbedding(
          (linear_1): Linear(in_features=256, out_features=2304, bias=True)
          (act): SiLU()
          (linear_2): Linear(in_features=2304, out_features=2304, bias=True)
        )
      )
      (silu): SiLU()
      (linear): Linear(in_features=2304, out_features=13824, bias=True)
    )
    (pos_embed_mask): ModuleList(
      (0): PatchEmbed2D(
        (proj): Conv2d(4, 2304, kernel_size=(2, 2), stride=(2, 2))
      )
      (1): Linear(in_features=2304, out_features=2304, bias=False)
    )
    (pos_embed_masked_hidden_states): ModuleList(
      (0): PatchEmbed2D(
        (proj): Conv2d(8, 2304, kernel_size=(2, 2), stride=(2, 2))
      )
      (1): Linear(in_features=2304, out_features=2304, bias=False)
    )
  )
)
09/05/2024 19:43:11 - INFO - __main__ -   Num examples = 474899
09/05/2024 19:43:11 - INFO - __main__ -   Num Epochs = 17
09/05/2024 19:43:11 - INFO - __main__ -   Instantaneous batch size per device = 1
09/05/2024 19:43:11 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
09/05/2024 19:43:11 - INFO - __main__ -   Gradient Accumulation steps = 1
09/05/2024 19:43:11 - INFO - __main__ -   Total optimization steps = 1000000
09/05/2024 19:43:11 - INFO - __main__ -   Total optimization steps (num_update_steps_per_epoch) = 59362
09/05/2024 19:43:11 - INFO - __main__ -   Total trainable parameters = 2.8204808 B
Steps:   0%|          | 0/1000000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 935, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 766, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 739, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 449, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/module.py", line 351, in mpdl_iter_init
    origin_mpdl_iter_init(self, *args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1039, in __init__
    w.start()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0xffff303c7e50>: attribute lookup <lambda> on opensora.models.causalvideovae failed
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 935, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 766, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 739, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 449, in __iter__
    dataloader_iter = super().__iter__()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 438, in __iter__
    return self._get_iterator()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/module.py", line 351, in mpdl_iter_init
    origin_mpdl_iter_init(self, *args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1039, in __init__
    w.start()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0xffff303c7e50>: attribute lookup <lambda> on opensora.models.causalvideovae failed
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploaded[2024-09-05 19:43:18,905] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 398 closing signal SIGTERM
[2024-09-05 19:43:18,905] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 400 closing signal SIGTERM
[2024-09-05 19:43:18,905] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 402 closing signal SIGTERM
wandb: | 0.004 MB of 0.004 MB uploaded[2024-09-05 19:43:19,870] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 399) of binary: /home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9
Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1042, in launch_command
    deepspeed_launcher(args)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/commands/launch.py", line 754, in deepspeed_launcher
    distrib_run.run(args)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
opensora/train/train_inpaint.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-09-05_19:43:18
  host      : bms-sora-910b-0003
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 401)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-09-05_19:43:18
  host      : bms-sora-910b-0003
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 403)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-09-05_19:43:18
  host      : bms-sora-910b-0003
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 404)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-09-05_19:43:18
  host      : bms-sora-910b-0003
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 405)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-05_19:43:18
  host      : bms-sora-910b-0003
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 399)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Process ForkServerProcess-7:
Process ForkServerProcess-8:
Process ForkServerProcess-4:
Process ForkServerProcess-9:
Process ForkServerProcess-3:
Process ForkServerProcess-2:
Process ForkServerProcess-6:
Process ForkServerProcess-5:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "<string>", line 2, in get
  File "<string>", line 2, in get
  File "<string>", line 2, in get
  File "<string>", line 2, in get
  File "<string>", line 2, in get
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "<string>", line 2, in get
EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
EOFError
EOFError
EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "<string>", line 2, in get
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
EOFError
Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "<string>", line 2, in get
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
EOFError
wandb: / 0.020 MB of 0.039 MB uploaded (0.011 MB deduped)/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 42 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
wandb: - 0.029 MB of 0.039 MB uploaded (0.011 MB deduped)wandb: \ 0.039 MB of 0.039 MB uploaded (0.011 MB deduped)wandb:  View run exalted-morning-5 at: https://wandb.ai/pkuhxy-Peking%20University/inpaint_93x320x320_stage1_swap/runs/hhdazhf4
wandb:  View job at https://wandb.ai/pkuhxy-Peking%20University/inpaint_93x320x320_stage1_swap/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjQyMDM4Nzg0MA==/version_details/v1
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240905_194309-hhdazhf4/logs
