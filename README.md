# Open-Sora Plan

[[Project Page]](https://pku-yuangroup.github.io/Open-Sora-Plan/) [[ä¸­æ–‡ä¸»é¡µ]](https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html) [[Discord]](https://discord.gg/fqpmStRX) [[Wechat Group]](https://github.com/PKU-YuanGroup/Open-Sora-Plan/issues/53#issuecomment-1980312563) 

## Goal
This project aims to create a simple and scalable repo, to reproduce [Sora](https://openai.com/sora) (OpenAI, but we prefer to call it "CloseAI" ) and build knowledge about Video-VQVAE (VideoGPT) + DiT at scale. However, we have limited resources, we deeply wish all open-source community can contribute to this project. Pull request are welcome!!!

æœ¬é¡¹ç›®å¸Œæœ›é€šè¿‡å¼€æºç¤¾åŒºçš„åŠ›é‡å¤ç°Soraï¼Œç”±åŒ—å¤§-å…”å±•AIGCè”åˆå®éªŒå®¤å…±åŒå‘èµ·ï¼Œå½“å‰æˆ‘ä»¬èµ„æºæœ‰é™ä»…æ­å»ºäº†åŸºç¡€æ¶æ„ï¼Œæ— æ³•è¿›è¡Œå®Œæ•´è®­ç»ƒï¼Œå¸Œæœ›é€šè¿‡å¼€æºç¤¾åŒºé€æ­¥å¢åŠ æ¨¡å—å¹¶ç­¹é›†èµ„æºè¿›è¡Œè®­ç»ƒï¼Œå½“å‰ç‰ˆæœ¬ç¦»ç›®æ ‡å·®è·å·¨å¤§ï¼Œä»éœ€æŒç»­å®Œå–„å’Œå¿«é€Ÿè¿­ä»£ï¼Œæ¬¢è¿Pull requestï¼ï¼ï¼

Project stages:
- Primary
1. Setup the codebase and train a un-conditional model on landscape dataset.
2. Train models that boost resolution and duration.

- Extensions
3. Conduct text2video experiments on landscape dataset.
4. Train the 1080p model on video2text dataset.
5. Control model with more condition.


<div style="display: flex; justify-content: center;"> 
  <img src="assets/we_want_you.jpg" width=200> 
  <img src="assets/framework.jpg" width=600> 
</div>

  
## News

**[2024.03.07]** We support training with 128 frames (when sample rate = 3, which is about 13 seconds) of 256x256, or 64 frames (which is about 6 seconds) of 512x512.

**[2024.03.05]** See our latest [todo](https://github.com/PKU-YuanGroup/Open-Sora-Plan?tab=readme-ov-file#todo), welcome to pull request.

**[2024.03.04]** We re-organize and modulize our codes and make it easy to [contribute](https://github.com/PKU-YuanGroup/Open-Sora-Plan?tab=readme-ov-file#how-to-contribute-to-the-open-sora-plan-community) to the project, please see the [Repo structure](https://github.com/PKU-YuanGroup/Open-Sora-Plan?tab=readme-ov-file#repo-structure).

**[2024.03.03]** We open some [discussions](https://github.com/PKU-YuanGroup/Open-Sora-Plan/discussions) and clarify several issues.

**[2024.03.01]** Training codes are available now! Learn more in our [project page](https://pku-yuangroup.github.io/Open-Sora-Plan/). Please feel free to watch ğŸ‘€ this repository for the latest updates.


## Todo

#### Setup the codebase and train a unconditional model on landscape dataset
- [x] Fix typo & Update readme. ğŸ¤ Thanks to [@mio2333](https://github.com/mio2333), [@CreamyLong](https://github.com/CreamyLong), [@chg0901](https://github.com/chg0901)
- [x] Setup repo-structure.
- [x] Add Video-VQGAN model, which is borrowed from [VideoGPT](https://github.com/wilson1yan/VideoGPT).
- [x] Support variable aspect ratios, resolutions, durations training on [DiT](https://github.com/facebookresearch/DiT).
- [x] Support Dynamic mask input inspired [FiT](https://github.com/whlzy/FiT).
- [x] Add class-conditioning on embeddings.
- [x] Incorporating [Latte](https://github.com/Vchitect/Latte) as main codebase.
- [x] Add VAE model, which is borrowed from [Stable Diffusion](https://github.com/CompVis/latent-diffusion).
- [x] Joint dynamic mask input with VAE.
- [ ] Add VQVAE from [VQGAN](https://github.com/CompVis/taming-transformers). ğŸ™ **[Need your contribution]**
- [ ] Make the codebase ready for the cluster training. Add SLURM scripts. ğŸ™ **[Need your contribution]**
- [x] Refactor VideoGPT. ğŸ¤ Thanks to [@qqingzheng](https://github.com/qqingzheng), [@luo3300612](https://github.com/luo3300612)
- [x] Add sampling script.
- [x] Incorporate [SiT](https://github.com/willisma/SiT). ğŸ¤ Thanks to [@khan-yin](https://github.com/khan-yin)

#### Train models that boost resolution and duration
- [ ] Add [PI](https://arxiv.org/abs/2306.15595) to support out-of-domain size. ğŸ™ **[Need your contribution]**
- [ ] Add 2D RoPE to improve generalization ability as [FiT](https://github.com/whlzy/FiT). ğŸ™ **[Need your contribution]**
- [x] Extract offline feature.
- [x] Add frame interpolation model. ğŸ¤ Thanks to [@yunyangge](https://github.com/yunyangge)
- [x] Add super resolution model. ğŸ¤ Thanks to [@Linzy19](https://github.com/Linzy19)
- [x] Add accelerate to automatically manage training.
- [ ] Joint training with images. ğŸ™ **[Need your contribution]**
- [ ] Incorporate [NaViT](https://arxiv.org/abs/2307.06304). ğŸ™ **[Need your contribution]**
- [ ] Add [FreeNoise](https://github.com/arthur-qiu/FreeNoise-LaVie) support for training-free longer video generation. ğŸ™ **[Need your contribution]**

#### Conduct text2video experiments on landscape dataset.
- [ ] Finish data loading, pre-processing utils. âŒ› [WIP]
- [ ] Add CLIP and T5 support. âŒ› [WIP]
- [ ] Add text2image training script. âŒ› [WIP]
- [ ] Add prompt captioner. ğŸ™ **[Need your contribution]**

#### Train the 1080p model on video2text dataset
- [ ] Looking for a suitable dataset, welcome to discuss and recommend. ğŸ™ **[Need your contribution]**
- [ ] Finish data loading, pre-processing utils. âŒ› [WIP]
- [ ] Support memory friendly training.
  - [x] Add flash-attention2 from pytorch.
  - [x] Add xformers.
  - [x] Support mixed precision training.
  - [x] Add gradient checkpoint.
  - [x] Support for ReBased and Ring attention. ğŸ¤ Thanks to [@kabachuha](https://github.com/kabachuha)
  - [ ] Train using the deepspeed engine. ğŸ™ **[Need your contribution]**
- [ ] Train with text condition. Here we could conduct different experiments:
  - [ ] Train with T5 conditioning. ğŸš€ **[Require more computation]**
  - [ ] Train with CLIP conditioning. ğŸš€ **[Require more computation]**
  - [ ] Train with CLIP + T5 conditioning (probably costly during training and experiments). ğŸš€ **[Require more computation]**

#### Control model with more condition
- [ ] Load pretrained weight from [PixArt-Î±](https://github.com/PixArt-alpha/PixArt-alpha). âŒ› [WIP]
- [ ] Incorporating [ControlNet](https://github.com/lllyasviel/ControlNet). ğŸ™ **[Need your contribution]**

## Repo structure (WIP)
```
â”œâ”€â”€ README.md
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ Data.md                    -> Datasets description.
â”‚   â”œâ”€â”€ Contribution_Guidelines.md -> Contribution guidelines description.
â”œâ”€â”€ scripts                        -> All scripts.
â”œâ”€â”€ opensora
â”‚Â Â  â”œâ”€â”€ dataset
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ae                     -> Compress videos to latents
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ imagebase
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ vae
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ vqvae
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ videobase
â”‚Â Â  â”‚Â Â  â”‚Â Â      â”œâ”€â”€ vae
â”‚Â Â  â”‚Â Â  â”‚Â Â      â””â”€â”€ vqvae
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ captioner
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ diffusion              -> Denoise latents
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ diffusion         
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dit
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ latte
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ unet
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ frame_interpolation
â”‚Â Â  â”‚Â Â  â””â”€â”€ super_resolution
â”‚Â Â  â”œâ”€â”€ sample
â”‚Â Â  â”œâ”€â”€ train                      -> Training code
â”‚Â Â  â””â”€â”€ utils
```

## Requirements and Installation

The recommended requirements are as follows.

* Python >= 3.8
* CUDA Version >= 11.7
* Install required packages:

```
git clone https://github.com/PKU-YuanGroup/Open-Sora-Plan
cd Open-Sora-Plan
conda create -n opensora python=3.8 -y
conda activate opensora
pip install -e .
```

## Usage

### Datasets
Refer to [Data.md](docs/Data.md)


### Video-VQVAE (VideoGPT)

#### Training

To train VQVAE, run the script:

```
scripts/train_vqvae.sh
```

You can modify the training parameters within the script. For training parameters, please refer to [transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments). Other parameters are explained as follows:

##### VQ-VAE Specific Settings

* `--embedding_dim`: number of dimensions for codebooks embeddings
* `--n_codes 2048`: number of codes in the codebook
* `--n_hiddens 240`: number of hidden features in the residual blocks
* `--n_res_layers 4`: number of residual blocks
* `--downsample "4,4,4"`: T H W downsampling stride of the encoder

##### Dataset Settings
* `--data_path <path>`: path to an `hdf5` file or a folder containing `train` and `test` folders with subdirectories of videos
* `--resolution 128`: spatial resolution to train on 
* `--sequence_length 16`: temporal resolution, or video clip length

#### Reconstructing

```Python
python examples/rec_video.py --video-path "assets/origin_video_0.mp4" --rec-path "rec_video_0.mp4" --num-frames 500 --sample-rate 1
```
```Python
python examples/rec_video.py --video-path "assets/origin_video_1.mp4" --rec-path "rec_video_1.mp4" --resolution 196 --num-frames 600 --sample-rate 1
```

We present four reconstructed videos in this demonstration, arranged from left to right as follows: 

| **3s 596x336** | **10s 256x256** | **18s 196x196**  | **24s 168x96** |
| --- | --- | --- | --- |
| <img src="assets/rec_video_2.gif">  | <img src="assets/rec_video_0.gif">  | <img src="assets/rec_video_1.gif">  | <img src="assets/rec_video_3.gif"> |

#### Others

Please refer to the document [VQVAE](docs/VQVAE.md).

### VideoDiT (DiT)

#### Training
```
sh scripts/train.sh
```

<p align="center">
<img src="assets/loss.jpg" width=60%>
</p>

#### Sampling
```
sh scripts/sample.sh
```

## How to Contribute to the Open-Sora Plan Community
We greatly appreciate your contributions to the Open-Sora Plan open-source community and helping us make it even better than it is now!

For more details, please refer to the [Contribution Guidelines](docs/Contribution_Guidelines.md)


## Acknowledgement
* [Latte](https://github.com/Vchitect/Latte): The **main codebase** we built upon and it is an wonderful video gererated model.
* [DiT](https://github.com/facebookresearch/DiT): Scalable Diffusion Models with Transformers.
* [VideoGPT](https://github.com/wilson1yan/VideoGPT): Video Generation using VQ-VAE and Transformers.
* [FiT](https://github.com/whlzy/FiT): Flexible Vision Transformer for Diffusion Model.
* [Positional Interpolation](https://arxiv.org/abs/2306.15595): Extending Context Window of Large Language Models via Positional Interpolation.

## License
* The service is a research preview intended for non-commercial use only. See [LICENSE](LICENSE) for details.

