[2024-09-09 12:27:09,377] torch.distributed.run: [WARNING] 
[2024-09-09 12:27:09,377] torch.distributed.run: [WARNING] *****************************************
[2024-09-09 12:27:09,377] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-09 12:27:09,377] torch.distributed.run: [WARNING] *****************************************
[2024-09-09 12:27:16,508] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-09 12:27:16,604] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-09 12:27:16,769] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-09 12:27:18,155] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:209: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
[2024-09-09 12:27:18,304] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-09 12:27:18,529] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-09 12:27:18,681] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
[2024-09-09 12:27:19,071] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to npu (auto detect)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.
  warnings.warn(msg, RuntimeWarning)
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
pid 405's current affinity list: 0-191
pid 405's new affinity list: 96-119
pid 406's current affinity list: 0-191
pid 406's new affinity list: 120-143
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
pid 407's current affinity list: 0-191
pid 407's new affinity list: 144-167
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
The npu_config.on_npu is True
pid 401's current affinity list: 0-191
pid 401's new affinity list: 0-23
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _has_inf_or_nan
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
pid 408's current affinity list: 0-191
pid 408's new affinity list: 168-191
pid 403's current affinity list: 0-191
pid 403's new affinity list: 48-71
pid 402's current affinity list: 0-191
pid 402's new affinity list: 24-47
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
skip replace _has_inf_or_nan
skip replace _DeepSpeedEngine__check_params
skip replace __init__
skip replace _change_recovery_script_permissions
skip replace _copy_recovery_script
skip replace _get_expert_ckpt_name
skip replace is_iterable_style_dataset
skip replace is_map_style_dataset
skip replace load_moe_state_dict
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
pid 404's current affinity list: 0-191
pid 404's new affinity list: 72-95
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-5]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=0, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema', yolomodel_pathorname='/home/image_data/hxy/Open-Sora-Plan/opensora/dataset/yolov9c-seg.pt')
[RANK-4]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=0, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema', yolomodel_pathorname='/home/image_data/hxy/Open-Sora-Plan/opensora/dataset/yolov9c-seg.pt')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-09 12:27:27,947] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-09 12:27:27,948] [INFO] [comm.py:637:init_distributed] cdb=None
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-09 12:27:27,951] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-09 12:27:27,952] [INFO] [comm.py:637:init_distributed] cdb=None
09/09/2024 12:27:27 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 5
Local process index: 5
Device: npu:5

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
09/09/2024 12:27:27 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 4
Local process index: 4
Device: npu:4

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/lightning_fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
[RANK-6]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=0, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema', yolomodel_pathorname='/home/image_data/hxy/Open-Sora-Plan/opensora/dataset/yolov9c-seg.pt')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-09 12:27:28,653] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-09 12:27:28,653] [INFO] [comm.py:637:init_distributed] cdb=None
09/09/2024 12:27:28 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 6
Local process index: 6
Device: npu:6

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-0]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=0, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema', yolomodel_pathorname='/home/image_data/hxy/Open-Sora-Plan/opensora/dataset/yolov9c-seg.pt')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-09 12:27:29,451] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-09 12:27:29,451] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-09 12:27:29,451] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
09/09/2024 12:27:29 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 0
Local process index: 0
Device: npu:0

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[RANK-7]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=0, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema', yolomodel_pathorname='/home/image_data/hxy/Open-Sora-Plan/opensora/dataset/yolov9c-seg.pt')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-09 12:27:29,834] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-09 12:27:29,835] [INFO] [comm.py:637:init_distributed] cdb=None
09/09/2024 12:27:29 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 7
Local process index: 7
Device: npu:7

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-2]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=0, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema', yolomodel_pathorname='/home/image_data/hxy/Open-Sora-Plan/opensora/dataset/yolov9c-seg.pt')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-09 12:27:30,043] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-09 12:27:30,043] [INFO] [comm.py:637:init_distributed] cdb=None
09/09/2024 12:27:30 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 2
Local process index: 2
Device: npu:2

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
[RANK-1]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=0, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema', yolomodel_pathorname='/home/image_data/hxy/Open-Sora-Plan/opensora/dataset/yolov9c-seg.pt')
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-09 12:27:30,093] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-09 12:27:30,093] [INFO] [comm.py:637:init_distributed] cdb=None
09/09/2024 12:27:30 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 1
Local process index: 1
Device: npu:1

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.
  deprecate("Transformer2DModelOutput", "0.29", deprecation_message)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.
  deprecate("Transformer2DModel", "0.29", deprecation_message)
[RANK-3]: Namespace(dataset='inpaint', data='scripts/train_data/video_data_debug.txt', sample_rate=1, train_fps=16, drop_short_ratio=0.0, speed_factor=1.0, num_frames=93, max_height=320, max_width=320, use_img_from_vid=False, use_image_num=0, model_max_length=512, cfg=0.1, dataloader_num_workers=0, train_batch_size=1, group_data=True, hw_stride=32, skip_low_resolution=True, force_resolution=False, trained_data_global_step=0, use_decord=True, model='OpenSoraInpaint-L/122', enable_8bit_t5=False, tile_overlap_factor=0.125, enable_tiling=False, compress_kv=False, attention_mode='xformers', use_rope=True, compress_kv_factor=1, interpolation_scale_h=1.0, interpolation_scale_w=1.0, interpolation_scale_t=1.0, downsampler=None, ae='WFVAEModel_D8_4x8x8', ae_path='/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL', text_encoder_name='google/mt5-xxl', cache_dir='../../cache_dir/', pretrained=None, enable_stable_fp32=False, sparse1d=True, sparse2d=False, sparse_n=4, tile_sample_min_size=512, tile_sample_min_size_t=33, adapt_vae=False, use_motion=True, gradient_checkpointing=True, snr_gamma=5.0, use_ema=True, ema_decay=0.9999, ema_start_step=0, noise_offset=0.0, prediction_type='v_prediction', rescale_betas_zero_snr=True, num_sampling_steps=50, guidance_scale=2.5, enable_tracker=False, seed=1234, output_dir='/home/save_dir/runs/inpaint_93x320x320_stage1', checkpoints_total_limit=None, checkpointing_steps=1000, resume_from_checkpoint=None, logging_dir='logs', report_to='wandb', num_train_epochs=100, max_train_steps=1000000, gradient_accumulation_steps=1, optimizer='adamW', learning_rate=1e-05, scale_lr=False, lr_warmup_steps=0, use_8bit_adam=False, adam_beta1=0.9, adam_beta2=0.999, prodigy_decouple=True, adam_weight_decay=0.01, adam_weight_decay_text_encoder=None, adam_epsilon=1e-08, prodigy_use_bias_correction=True, prodigy_safeguard_warmup=True, max_grad_norm=1.0, prodigy_beta3=None, lr_scheduler='constant', allow_tf32=True, mixed_precision='bf16', local_rank=-1, sp_size=1, train_sp_batch_size=1, t2v_ratio=0.0, i2v_ratio=0.0, transition_ratio=0.0, v2v_ratio=0.0, clear_video_ratio=0.0, Semantic_ratio=0.2, bbox_ratio=0.2, background_ratio=0.2, fixed_ratio=0.1, Semantic_expansion_ratio=0.1, fixed_bg_ratio=0.1, min_clear_ratio=0.25, default_text_ratio=0.0, pretrained_transformer_model_path='/home/image_data/captions/vpre_latest_134k/model_ema', yolomodel_pathorname='/home/image_data/hxy/Open-Sora-Plan/opensora/dataset/yolov9c-seg.pt')
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/deepspeed/comm/comm.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  utils.logger.warn("HCCL backend in DeepSpeed not yet implemented")
[2024-09-09 12:27:30,699] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-09-09 12:27:30,699] [INFO] [comm.py:637:init_distributed] cdb=None
09/09/2024 12:27:30 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: hccl
Num processes: 8
Process index: 3
Local process index: 3
Device: npu:3

Mixed precision type: bf16
ds_config: {'fp16': {'enabled': False, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': True}, 'communication_data_type': 'fp32', 'gradient_clipping': 1.0, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'allgather_bucket_size': 536870912, 'contiguous_gradients': True, 'reduce_bucket_size': 536870912}, 'steps_per_print': inf}

/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.load_config(...) followed by <class 'opensora.models.causalvideovae.model.vae.modeling_wfvae.WFVAEModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
init from /home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL/wfvae.ckpt
Load from ema model!
['encoder.wavelet_tranform_3d.h_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_conv.conv.weight', 'encoder.wavelet_tranform_3d.h_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.g_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.hh_v_conv.conv.weight', 'encoder.wavelet_tranform_3d.gh_v_conv.conv.weight', 'encoder.wavelet_tranform_2d.aa', 'encoder.wavelet_tranform_2d.ad', 'encoder.wavelet_tranform_2d.da', 'encoder.wavelet_tranform_2d.dd', 'decoder.inverse_wavelet_tranform_3d.h', 'decoder.inverse_wavelet_tranform_3d.g', 'decoder.inverse_wavelet_tranform_3d.hh', 'decoder.inverse_wavelet_tranform_3d.gh', 'decoder.inverse_wavelet_tranform_3d.h_v', 'decoder.inverse_wavelet_tranform_3d.g_v', 'decoder.inverse_wavelet_tranform_3d.hh_v', 'decoder.inverse_wavelet_tranform_3d.gh_v', 'decoder.inverse_wavelet_tranform_2d.aa', 'decoder.inverse_wavelet_tranform_2d.ad', 'decoder.inverse_wavelet_tranform_2d.da', 'decoder.inverse_wavelet_tranform_2d.dd'] []
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
Loading OpenSoraInpaint pretrained weights...
Loading pretrained model from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors...
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!

  0%|          | 0/478625 [00:00<?, ?it/s][A
  1%|          | 3290/478625 [00:00<00:14, 32890.46it/s][A
  1%|▏         | 6580/478625 [00:00<00:14, 32359.90it/s][A
  2%|▏         | 9978/478625 [00:00<00:14, 33090.22it/s][A
  3%|▎         | 13342/478625 [00:00<00:13, 33303.03it/s][A
  3%|▎         | 16674/478625 [00:00<00:14, 32369.75it/s][A
  4%|▍         | 20078/478625 [00:00<00:13, 32923.79it/s][A
  5%|▍         | 23375/478625 [00:00<00:14, 31833.28it/s][A
  6%|▌         | 26685/478625 [00:00<00:14, 32220.02it/s][A
  6%|▋         | 29961/478625 [00:00<00:13, 32381.91it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
  7%|▋         | 33205/478625 [00:01<00:13, 31880.95it/s][A
  1%|          | 3267/478625 [00:00<00:14, 32628.96it/s][A
  8%|▊         | 36398/478625 [00:01<00:13, 31856.52it/s][A
  1%|▏         | 6530/478625 [00:00<00:14, 32120.76it/s][A
  8%|▊         | 39706/478625 [00:01<00:13, 32222.06it/s][A
  2%|▏         | 9877/478625 [00:00<00:14, 32729.25it/s][A
  9%|▉         | 42932/478625 [00:01<00:13, 31680.96it/s][A
  3%|▎         | 13195/478625 [00:00<00:14, 32905.17it/s][A
 10%|▉         | 46295/478625 [00:01<00:13, 32255.26it/s][A
  3%|▎         | 16487/478625 [00:00<00:14, 32160.17it/s][A
 10%|█         | 49525/478625 [00:01<00:13, 31876.23it/s][A
  4%|▍         | 19836/478625 [00:00<00:14, 32603.45it/s][A
 11%|█         | 52917/478625 [00:01<00:13, 32477.50it/s][A
  5%|▍         | 23100/478625 [00:00<00:14, 31975.37it/s][A
 12%|█▏        | 56300/478625 [00:01<00:12, 32876.94it/s][A
  6%|▌         | 26417/478625 [00:00<00:13, 32344.81it/s][A
 12%|█▏        | 59591/478625 [00:01<00:13, 32016.66it/s][A
  6%|▌         | 29774/478625 [00:00<00:13, 32720.62it/s][A
 13%|█▎        | 63008/478625 [00:01<00:12, 32623.86it/s][A
  7%|▋         | 33049/478625 [00:01<00:13, 32140.13it/s][A
 14%|█▍        | 66277/478625 [00:02<00:12, 32136.34it/s][A
  8%|▊         | 36390/478625 [00:01<00:13, 32520.18it/s][A
 15%|█▍        | 69665/478625 [00:02<00:12, 32646.91it/s][A
  8%|▊         | 39718/478625 [00:01<00:13, 32745.88it/s][A
 15%|█▌        | 73020/478625 [00:02<00:12, 32910.40it/s][A
  9%|▉         | 42996/478625 [00:01<00:13, 32042.14it/s][A
 16%|█▌        | 76315/478625 [00:02<00:12, 32383.21it/s][A
 10%|▉         | 46327/478625 [00:01<00:13, 32414.10it/s][A
 17%|█▋        | 79692/478625 [00:02<00:12, 32790.67it/s][A
 10%|█         | 49573/478625 [00:01<00:13, 31985.27it/s][A
 17%|█▋        | 82975/478625 [00:02<00:12, 32123.33it/s][A
 11%|█         | 52904/478625 [00:01<00:13, 32374.98it/s][A
 18%|█▊        | 86283/478625 [00:02<00:12, 32401.84it/s][A
 12%|█▏        | 56231/478625 [00:01<00:12, 32637.96it/s][A
 19%|█▊        | 89649/478625 [00:02<00:11, 32771.47it/s][A
 12%|█▏        | 59498/478625 [00:01<00:13, 32010.07it/s][A
 19%|█▉        | 92930/478625 [00:02<00:12, 31945.10it/s][A
 13%|█▎        | 62866/478625 [00:01<00:12, 32499.14it/s][A
 20%|██        | 96274/478625 [00:02<00:11, 32380.09it/s][A
 14%|█▍        | 66120/478625 [00:02<00:12, 31928.84it/s][A
 21%|██        | 99518/478625 [00:03<00:11, 31827.18it/s][A
 15%|█▍        | 69476/478625 [00:02<00:12, 32406.14it/s][A
 21%|██▏       | 102745/478625 [00:03<00:11, 31953.98it/s][A
 15%|█▌        | 72801/478625 [00:02<00:12, 32652.12it/s][A
 22%|██▏       | 106051/478625 [00:03<00:11, 32277.06it/s][A
 16%|█▌        | 76070/478625 [00:02<00:12, 32033.91it/s][A
 23%|██▎       | 109283/478625 [00:03<00:11, 31770.33it/s][A
 17%|█▋        | 79399/478625 [00:02<00:12, 32400.89it/s][A
 24%|██▎       | 112668/478625 [00:03<00:11, 32378.03it/s][A
 17%|█▋        | 82643/478625 [00:02<00:12, 31926.37it/s][A
 24%|██▍       | 115910/478625 [00:03<00:11, 31789.64it/s][A
 18%|█▊        | 85964/478625 [00:02<00:12, 32302.63it/s][A
 25%|██▍       | 119253/478625 [00:03<00:11, 32249.49it/s][A
 19%|█▊        | 89266/478625 [00:02<00:11, 32513.03it/s][A
 26%|██▌       | 122665/478625 [00:03<00:10, 32800.78it/s][A
 19%|█▉        | 92520/478625 [00:02<00:12, 31940.96it/s][A
 26%|██▋       | 125949/478625 [00:03<00:10, 32116.25it/s][A
 20%|██        | 95835/478625 [00:02<00:11, 32294.94it/s][A
 27%|██▋       | 129319/478625 [00:03<00:10, 32577.55it/s][A
 21%|██        | 99068/478625 [00:03<00:11, 31805.73it/s][A
 28%|██▊       | 132621/478625 [00:04<00:10, 32077.50it/s][A
 21%|██▏       | 102379/478625 [00:03<00:11, 32181.33it/s][A
 28%|██▊       | 136016/478625 [00:04<00:10, 32623.08it/s][A
 22%|██▏       | 105695/478625 [00:03<00:11, 32466.91it/s][A
 29%|██▉       | 139364/478625 [00:04<00:10, 32872.64it/s][A
 23%|██▎       | 108945/478625 [00:03<00:11, 31955.72it/s][A
 30%|██▉       | 142655/478625 [00:04<00:10, 32228.96it/s][A
 23%|██▎       | 112290/478625 [00:03<00:11, 32393.96it/s][A
 30%|███       | 145921/478625 [00:04<00:10, 32352.47it/s][A
 24%|██▍       | 115610/478625 [00:03<00:11, 32631.92it/s][A09/09/2024 12:29:27 - INFO - __main__ - optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: False
    fused: None
    lr: 1e-05
    maximize: False
    weight_decay: 0.01
)

 31%|███       | 149244/478625 [00:04<00:10, 32610.82it/s][A
 25%|██▍       | 118876/478625 [00:03<00:11, 32013.78it/s][A
 32%|███▏      | 152509/478625 [00:04<00:10, 32020.10it/s][A
 26%|██▌       | 122189/478625 [00:03<00:11, 32340.46it/s][A
 33%|███▎      | 155887/478625 [00:04<00:09, 32535.02it/s][A
 26%|██▌       | 125427/478625 [00:03<00:11, 31873.10it/s][A
 33%|███▎      | 159145/478625 [00:04<00:09, 31979.14it/s][A
 27%|██▋       | 128743/478625 [00:03<00:10, 32248.33it/s][A
 34%|███▍      | 162348/478625 [00:05<00:09, 31908.83it/s][A
 28%|██▊       | 132097/478625 [00:04<00:10, 32626.57it/s][A
 35%|███▍      | 165687/478625 [00:05<00:09, 32342.33it/s][A
 28%|██▊       | 135363/478625 [00:04<00:10, 32041.71it/s][AYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565

 35%|███▌      | 168924/478625 [00:05<00:09, 31812.10it/s][A
 29%|██▉       | 138685/478625 [00:04<00:10, 32386.79it/s][A
 36%|███▌      | 172223/478625 [00:05<00:09, 32157.54it/s][A
 30%|██▉       | 141927/478625 [00:04<00:10, 31877.93it/s][A
 37%|███▋      | 175442/478625 [00:05<00:09, 31759.09it/s][A
 30%|███       | 145239/478625 [00:04<00:10, 32239.14it/s][A
 37%|███▋      | 178785/478625 [00:05<00:09, 32247.38it/s][A
 31%|███       | 148577/478625 [00:04<00:10, 32575.15it/s][A
 38%|███▊      | 182084/478625 [00:05<00:09, 32463.81it/s][A
 32%|███▏      | 151838/478625 [00:04<00:10, 31871.10it/s][A
 39%|███▊      | 185333/478625 [00:05<00:09, 31885.97it/s][A
 32%|███▏      | 155173/478625 [00:04<00:10, 32303.46it/s][A
 39%|███▉      | 188717/478625 [00:05<00:08, 32458.78it/s][A
 33%|███▎      | 158408/478625 [00:04<00:10, 31908.35it/s][A
 40%|████      | 191967/478625 [00:05<00:08, 31868.45it/s][A
 34%|███▍      | 161696/478625 [00:05<00:09, 32191.62it/s][Amissing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!
missing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!

 41%|████      | 195309/478625 [00:06<00:08, 32323.07it/s][A
 34%|███▍      | 165022/478625 [00:05<00:09, 32505.24it/s][A
 42%|████▏     | 198673/478625 [00:06<00:08, 32710.83it/s][A
 35%|███▌      | 168276/478625 [00:05<00:09, 31976.62it/s][A
 42%|████▏     | 201948/478625 [00:06<00:08, 32088.75it/s][A
 36%|███▌      | 171583/478625 [00:05<00:09, 32296.25it/s][A
 43%|████▎     | 205183/478625 [00:06<00:08, 32163.25it/s][A
 37%|███▋      | 174816/478625 [00:05<00:09, 31812.38it/s][A
 44%|████▎     | 208550/478625 [00:06<00:08, 32607.05it/s][A
 37%|███▋      | 178173/478625 [00:05<00:09, 32328.38it/s][A
 44%|████▍     | 211814/478625 [00:06<00:08, 31970.31it/s][A
 38%|███▊      | 181454/478625 [00:05<00:09, 32467.91it/s][A
 45%|████▍     | 215143/478625 [00:06<00:08, 32356.85it/s][A
 39%|███▊      | 184704/478625 [00:05<00:09, 31800.44it/s][A
 46%|████▌     | 218383/478625 [00:06<00:08, 31890.84it/s][A
 39%|███▉      | 188039/478625 [00:05<00:09, 32254.73it/s][A
 46%|████▋     | 221748/478625 [00:06<00:07, 32406.09it/s][A
 40%|███▉      | 191394/478625 [00:05<00:08, 32633.69it/s][A
 47%|████▋     | 225117/478625 [00:06<00:07, 32783.33it/s][A
 41%|████      | 194661/478625 [00:06<00:08, 32053.02it/s][A
 48%|████▊     | 228399/478625 [00:07<00:07, 32066.87it/s][A
 41%|████▏     | 197973/478625 [00:06<00:08, 32363.07it/s][A
 48%|████▊     | 231695/478625 [00:07<00:07, 32326.29it/s][A
 42%|████▏     | 201213/478625 [00:06<00:08, 31842.20it/s][A
 49%|████▉     | 234932/478625 [00:07<00:07, 31829.19it/s][A
 43%|████▎     | 204533/478625 [00:06<00:08, 32238.98it/s][A
 50%|████▉     | 238291/478625 [00:07<00:07, 32344.97it/s][A
 43%|████▎     | 207804/478625 [00:06<00:08, 32377.09it/s][A
 50%|█████     | 241630/478625 [00:07<00:07, 32651.05it/s][A
 44%|████▍     | 211045/478625 [00:06<00:08, 31868.88it/s][A
 51%|█████     | 244899/478625 [00:07<00:07, 32104.98it/s][A
 45%|████▍     | 214326/478625 [00:06<00:08, 32143.61it/s][A
 52%|█████▏    | 248281/478625 [00:07<00:07, 32608.19it/s][A
 45%|████▌     | 217544/478625 [00:06<00:08, 31710.38it/s][A
 53%|█████▎    | 251546/478625 [00:07<00:07, 32038.57it/s][A
 46%|████▌     | 220882/478625 [00:06<00:08, 32195.50it/s][A
 53%|█████▎    | 254754/478625 [00:07<00:06, 32032.79it/s][A
 47%|████▋     | 224220/478625 [00:06<00:07, 32544.21it/s][A
 54%|█████▍    | 258063/478625 [00:07<00:06, 32343.74it/s][A
 48%|████▊     | 227477/478625 [00:07<00:07, 32039.51it/s][A
 48%|████▊     | 230705/478625 [00:07<00:07, 32106.87it/s][A
 55%|█████▍    | 261300/478625 [00:08<00:06, 31778.73it/s][Amissing_keys 4 ['pos_embed_mask.0.proj.weight', 'pos_embed_mask.0.proj.bias', 'pos_embed_mask.1.weight', 'pos_embed_masked_hidden_states.1.weight'], unexpected_keys 0
Successfully load 695/699 keys from /home/image_data/captions/vpre_latest_134k/model_ema/diffusion_pytorch_model.safetensors!

 55%|█████▌    | 264661/478625 [00:08<00:06, 32314.14it/s][A
 49%|████▉     | 233918/478625 [00:07<00:07, 31698.58it/s][A
 50%|████▉     | 237275/478625 [00:07<00:07, 32247.91it/s][A
 56%|█████▌    | 267896/478625 [00:08<00:06, 31850.10it/s][A
 50%|█████     | 240612/478625 [00:07<00:07, 32577.94it/s][A
 57%|█████▋    | 271225/478625 [00:08<00:06, 32270.83it/s][A
 57%|█████▋    | 274544/478625 [00:08<00:06, 32539.68it/s][A
 51%|█████     | 243873/478625 [00:07<00:07, 31938.15it/s][A
 58%|█████▊    | 277801/478625 [00:08<00:06, 32094.51it/s][A
 52%|█████▏    | 247222/478625 [00:07<00:07, 32391.66it/s][A
 59%|█████▊    | 281109/478625 [00:08<00:06, 32384.27it/s][A
 52%|█████▏    | 250544/478625 [00:07<00:06, 32634.87it/s][A
 59%|█████▉    | 284450/478625 [00:08<00:05, 32686.83it/s][A
 53%|█████▎    | 253811/478625 [00:07<00:07, 32079.25it/s][A
 60%|██████    | 287721/478625 [00:08<00:05, 32037.35it/s][A
 54%|█████▎    | 257123/478625 [00:07<00:06, 32382.89it/s][A
 61%|██████    | 291076/478625 [00:09<00:05, 32479.65it/s][A
 54%|█████▍    | 260365/478625 [00:08<00:06, 31682.78it/s][A
 55%|█████▌    | 263691/478625 [00:08<00:06, 32141.73it/s][A
 61%|██████▏   | 294328/478625 [00:09<00:05, 31634.32it/s][A
 56%|█████▌    | 267023/478625 [00:08<00:06, 32486.32it/s][A
 62%|██████▏   | 297661/478625 [00:09<00:05, 32125.66it/s][A
 63%|██████▎   | 301030/478625 [00:09<00:05, 32583.05it/s][A
 56%|█████▋    | 270276/478625 [00:08<00:06, 31951.80it/s][A
 57%|█████▋    | 273596/478625 [00:08<00:06, 32315.60it/s][A
 64%|██████▎   | 304294/478625 [00:09<00:05, 31982.11it/s][A
 64%|██████▍   | 307592/478625 [00:09<00:05, 32271.47it/s][A
 58%|█████▊    | 276832/478625 [00:08<00:06, 31788.76it/s][A
 59%|█████▊    | 280167/478625 [00:08<00:06, 32245.19it/s][A
 65%|██████▍   | 310824/478625 [00:09<00:05, 31691.55it/s][A
 59%|█████▉    | 283499/478625 [00:08<00:05, 32560.29it/s][A
 66%|██████▌   | 314139/478625 [00:09<00:05, 32115.84it/s][A
 66%|██████▋   | 317539/478625 [00:09<00:04, 32670.08it/s][A
 60%|█████▉    | 286759/478625 [00:08<00:05, 32018.29it/s][A
 61%|██████    | 290080/478625 [00:08<00:05, 32368.24it/s][A
 67%|██████▋   | 320810/478625 [00:09<00:04, 32134.32it/s][A
 68%|██████▊   | 324175/478625 [00:10<00:04, 32579.07it/s][A
 61%|██████▏   | 293321/478625 [00:09<00:05, 31673.92it/s][A
 62%|██████▏   | 296638/478625 [00:09<00:05, 32110.55it/s][A
 68%|██████▊   | 327437/478625 [00:10<00:04, 31960.71it/s][A
 63%|██████▎   | 299978/478625 [00:09<00:05, 32489.08it/s][A
 69%|██████▉   | 330733/478625 [00:10<00:04, 32251.74it/s][A
 70%|██████▉   | 333973/478625 [00:10<00:04, 32292.79it/s][A
 63%|██████▎   | 303231/478625 [00:09<00:05, 31926.67it/s][A
 70%|███████   | 337206/478625 [00:10<00:04, 31908.35it/s][A
 64%|██████▍   | 306535/478625 [00:09<00:05, 32250.96it/s][A
 71%|███████   | 340592/478625 [00:10<00:04, 32483.15it/s][A
 65%|██████▍   | 309868/478625 [00:09<00:05, 32566.95it/s][A
 72%|███████▏  | 343844/478625 [00:10<00:04, 31971.38it/s][A
 65%|██████▌   | 313128/478625 [00:09<00:05, 32003.06it/s][A
 73%|███████▎  | 347183/478625 [00:10<00:04, 32386.82it/s][A
 66%|██████▌   | 316443/478625 [00:09<00:05, 32337.81it/s][A
 73%|███████▎  | 350548/478625 [00:10<00:03, 32758.48it/s][A
 67%|██████▋   | 319681/478625 [00:09<00:04, 31897.30it/s][A
 67%|██████▋   | 322984/478625 [00:10<00:04, 32229.80it/s][A
 74%|███████▍  | 353827/478625 [00:10<00:03, 32022.02it/s][A
 68%|██████▊   | 326287/478625 [00:10<00:04, 32463.62it/s][A
 75%|███████▍  | 357185/478625 [00:11<00:03, 32478.38it/s][A/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

 69%|██████▉   | 329536/478625 [00:10<00:04, 31739.08it/s][A
 75%|███████▌  | 360513/478625 [00:11<00:03, 32069.61it/s][A
 70%|██████▉   | 332867/478625 [00:10<00:04, 32198.79it/s][A
 76%|███████▌  | 363910/478625 [00:11<00:03, 32624.16it/s][A
 77%|███████▋  | 367219/478625 [00:11<00:03, 32759.95it/s][A
 70%|███████   | 336092/478625 [00:10<00:04, 31688.47it/s][A
 71%|███████   | 339415/478625 [00:10<00:04, 32137.63it/s][A
 77%|███████▋  | 370499/478625 [00:11<00:03, 31944.28it/s][A
 72%|███████▏  | 342774/478625 [00:10<00:04, 32563.27it/s][A
 78%|███████▊  | 373787/478625 [00:11<00:03, 32215.63it/s][A
 79%|███████▉  | 377110/478625 [00:11<00:03, 32512.47it/s][A
 72%|███████▏  | 346034/478625 [00:10<00:04, 31952.74it/s][A
 73%|███████▎  | 349352/478625 [00:10<00:04, 32310.38it/s][A
 79%|███████▉  | 380366/478625 [00:11<00:03, 31896.59it/s][A
 80%|████████  | 383675/478625 [00:11<00:02, 32244.74it/s][A
 74%|███████▎  | 352587/478625 [00:10<00:03, 31822.47it/s][A
 74%|███████▍  | 355920/478625 [00:11<00:03, 32264.29it/s][A
 81%|████████  | 386904/478625 [00:11<00:02, 31883.00it/s][A
 75%|███████▌  | 359230/478625 [00:11<00:03, 32509.18it/s][A
 82%|████████▏ | 390245/478625 [00:12<00:02, 32330.94it/s][A
 82%|████████▏ | 393541/478625 [00:12<00:02, 32514.48it/s][A
 76%|███████▌  | 362484/478625 [00:11<00:03, 32007.91it/s][A
 76%|███████▋  | 365816/478625 [00:11<00:03, 32392.18it/s][A
 83%|████████▎ | 396795/478625 [00:12<00:02, 32045.21it/s][A
 84%|████████▎ | 400137/478625 [00:12<00:02, 32449.65it/s][A
 77%|███████▋  | 369059/478625 [00:11<00:03, 31539.87it/s][A
 84%|████████▍ | 403385/478625 [00:12<00:02, 32069.96it/s][A
 78%|███████▊  | 372395/478625 [00:11<00:03, 32068.51it/s][A
 85%|████████▍ | 406772/478625 [00:12<00:02, 32598.05it/s][A
 78%|███████▊  | 375711/478625 [00:11<00:03, 32387.53it/s][A  0%|          | 0/1 [00:00<?, ?it/s]09/09/2024 12:29:35 - INFO - opensora.dataset.t2v_datasets - Building /home/image_data/captions/TV01_clips_final_478625_llavanext_217405_aes478625.json...

 86%|████████▌ | 410098/478625 [00:12<00:02, 32792.12it/s][A
 79%|███████▉  | 378955/478625 [00:11<00:03, 31829.12it/s][A
 86%|████████▋ | 413380/478625 [00:12<00:02, 32118.80it/s][A
 80%|███████▉  | 382299/478625 [00:11<00:02, 32300.24it/s][A
 81%|████████  | 385611/478625 [00:11<00:02, 32539.90it/s][A
 87%|████████▋ | 416596/478625 [00:12<00:01, 31643.96it/s][A
 81%|████████  | 388869/478625 [00:12<00:02, 31997.94it/s][A
 88%|████████▊ | 419764/478625 [00:13<00:01, 31408.29it/s][A
 82%|████████▏ | 392196/478625 [00:12<00:02, 32370.76it/s][A
 88%|████████▊ | 423098/478625 [00:13<00:01, 31973.04it/s][A
 89%|████████▉ | 426400/478625 [00:13<00:01, 32279.21it/s][A
 83%|████████▎ | 395437/478625 [00:12<00:02, 31889.33it/s][A

 90%|████████▉ | 429631/478625 [00:13<00:01, 31877.65it/s][A 83%|████████▎ | 398746/478625 [00:12<00:02, 32240.20it/s][A

 90%|█████████ | 432985/478625 [00:13<00:01, 32365.69it/s][A 84%|████████▍ | 402063/478625 [00:12<00:02, 32514.16it/s][A
 91%|█████████ | 436247/478625 [00:13<00:01, 32437.99it/s][A
 85%|████████▍ | 405318/478625 [00:12<00:02, 32026.07it/s][A
 85%|████████▌ | 408632/478625 [00:12<00:02, 32353.36it/s][A
 92%|█████████▏| 439493/478625 [00:13<00:01, 31893.29it/s][A
 93%|█████████▎| 442839/478625 [00:13<00:01, 32353.53it/s][A
 86%|████████▌ | 411871/478625 [00:12<00:02, 31738.26it/s][A
 93%|█████████▎| 446078/478625 [00:13<00:01, 31885.92it/s][A
 87%|████████▋ | 415088/478625 [00:12<00:01, 31862.73it/s][A
 94%|█████████▍| 449462/478625 [00:13<00:00, 32459.97it/s][A
 87%|████████▋ | 418403/478625 [00:12<00:01, 32240.17it/s][A
 95%|█████████▍| 452842/478625 [00:14<00:00, 32855.55it/s][A
 88%|████████▊ | 421630/478625 [00:13<00:01, 31823.66it/s][A
 89%|████████▉ | 424952/478625 [00:13<00:01, 32232.47it/s][A
 95%|█████████▌| 456131/478625 [00:14<00:00, 32210.41it/s][A
 96%|█████████▌| 459463/478625 [00:14<00:00, 32534.59it/s][A
 89%|████████▉ | 428178/478625 [00:13<00:01, 31744.38it/s][A
 90%|█████████ | 431517/478625 [00:13<00:01, 32225.71it/s][A
 97%|█████████▋| 462721/478625 [00:14<00:00, 32086.32it/s][A
 91%|█████████ | 434851/478625 [00:13<00:01, 32554.58it/s][A
 97%|█████████▋| 466059/478625 [00:14<00:00, 32463.32it/s][A
 98%|█████████▊| 469309/478625 [00:14<00:00, 32453.49it/s][A
 92%|█████████▏| 438110/478625 [00:13<00:01, 31989.52it/s][A
 92%|█████████▏| 441445/478625 [00:13<00:01, 32386.80it/s][A
 99%|█████████▊| 472557/478625 [00:14<00:00, 31811.33it/s][A
 93%|█████████▎| 444785/478625 [00:13<00:01, 32685.92it/s][A
 99%|█████████▉| 475801/478625 [00:14<00:00, 31993.39it/s][A100%|██████████| 478625/478625 [00:14<00:00, 32257.96it/s]
100%|██████████| 1/1 [00:21<00:00, 21.19s/it]100%|██████████| 1/1 [00:21<00:00, 21.19s/it]

 94%|█████████▎| 448057/478625 [00:13<00:00, 32159.65it/s][A
 94%|█████████▍| 451372/478625 [00:14<00:00, 32448.56it/s][A
 95%|█████████▍| 454620/478625 [00:14<00:00, 31969.21it/s][A
 96%|█████████▌| 457952/478625 [00:14<00:00, 32364.10it/s][A
 96%|█████████▋| 461284/478625 [00:14<00:00, 32646.32it/s][A
 97%|█████████▋| 464552/478625 [00:14<00:00, 31940.88it/s][A
 98%|█████████▊| 467764/478625 [00:14<00:00, 31991.78it/s][Atime 21.85038995742798
n_elements: 474899
data length: 474899

 98%|█████████▊| 470967/478625 [00:14<00:00, 31573.46it/s][A
 99%|█████████▉| 474318/478625 [00:14<00:00, 32141.47it/s][A
100%|█████████▉| 477628/478625 [00:14<00:00, 32424.24it/s][A100%|██████████| 478625/478625 [00:14<00:00, 32204.01it/s]
100%|██████████| 1/1 [00:21<00:00, 21.25s/it]100%|██████████| 1/1 [00:21<00:00, 21.25s/it]
time 21.90780997276306
n_elements: 474899
data length: 474899
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
  0%|          | 0/1 [00:00<?, ?it/s]/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

  0%|          | 0/478625 [00:00<?, ?it/s][A
  1%|          | 2946/478625 [00:00<00:16, 29455.25it/s][A
  1%|          | 5916/478625 [00:00<00:15, 29595.83it/s][A
  2%|▏         | 8876/478625 [00:00<00:16, 28895.03it/s][A
  2%|▏         | 11768/478625 [00:00<00:16, 28899.26it/s][A
  3%|▎         | 14660/478625 [00:00<00:16, 28381.35it/s][A
  4%|▎         | 17639/478625 [00:00<00:15, 28850.66it/s][A
  4%|▍         | 20585/478625 [00:00<00:15, 29046.53it/s][A
  5%|▍         | 23492/478625 [00:00<00:15, 28486.33it/s][A
  6%|▌         | 26396/478625 [00:00<00:15, 28653.96it/s][A
  6%|▌         | 29340/478625 [00:01<00:15, 28890.36it/s][A
  7%|▋         | 32231/478625 [00:01<00:15, 28402.11it/s][A
  7%|▋         | 35173/478625 [00:01<00:15, 28702.74it/s][A
  8%|▊         | 38178/478625 [00:01<00:15, 29103.39it/s][A
  9%|▊         | 41091/478625 [00:01<00:15, 28610.67it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
  9%|▉         | 44037/478625 [00:01<00:15, 28861.49it/s][A
 10%|▉         | 46964/478625 [00:01<00:14, 28980.54it/s][A
 10%|█         | 49865/478625 [00:01<00:15, 28445.19it/s][A
 11%|█         | 52798/478625 [00:01<00:14, 28703.34it/s][A
 12%|█▏        | 55768/478625 [00:01<00:14, 28997.15it/s][A
 12%|█▏        | 58671/478625 [00:02<00:14, 28548.87it/s][A
 13%|█▎        | 61656/478625 [00:02<00:14, 28930.10it/s][A
 13%|█▎        | 64586/478625 [00:02<00:14, 29038.42it/s][A
 14%|█▍        | 67492/478625 [00:02<00:14, 28577.72it/s][A
 15%|█▍        | 70437/478625 [00:02<00:14, 28834.32it/s][A
 15%|█▌        | 73382/478625 [00:02<00:13, 29014.36it/s][A
 16%|█▌        | 76286/478625 [00:02<00:14, 28632.65it/s][A
 17%|█▋        | 79282/478625 [00:02<00:13, 29022.64it/s][A
 17%|█▋        | 82187/478625 [00:02<00:13, 28523.81it/s][A
 18%|█▊        | 85177/478625 [00:02<00:13, 28926.39it/s][A
 18%|█▊        | 88125/478625 [00:03<00:13, 29087.44it/s][A
 19%|█▉        | 91036/478625 [00:03<00:13, 28550.81it/s][A
 20%|█▉        | 93926/478625 [00:03<00:13, 28652.09it/s][A
 20%|██        | 96923/478625 [00:03<00:13, 29039.26it/s][A
 21%|██        | 99830/478625 [00:03<00:13, 28493.17it/s][A
 21%|██▏       | 102801/478625 [00:03<00:13, 28849.79it/s][A
 22%|██▏       | 105786/478625 [00:03<00:12, 29144.74it/s][A
 23%|██▎       | 108704/478625 [00:03<00:12, 28684.47it/s][A
 23%|██▎       | 111712/478625 [00:03<00:12, 29094.19it/s][A
 24%|██▍       | 114681/478625 [00:03<00:12, 29268.38it/s][A
 25%|██▍       | 117611/478625 [00:04<00:12, 28694.64it/s][A
 25%|██▌       | 120609/478625 [00:04<00:12, 29032.18it/s][A
 26%|██▌       | 123608/478625 [00:04<00:12, 29313.39it/s][A
 26%|██▋       | 126542/478625 [00:04<00:12, 28792.69it/s][A
 27%|██▋       | 129495/478625 [00:04<00:12, 29007.56it/s][A
 28%|██▊       | 132492/478625 [00:04<00:11, 29291.24it/s][A
 28%|██▊       | 135424/478625 [00:04<00:11, 28733.36it/s][A
 29%|██▉       | 138389/478625 [00:04<00:11, 29001.91it/s][A
 30%|██▉       | 141293/478625 [00:04<00:11, 28579.26it/s][A
 30%|███       | 144154/478625 [00:05<00:11, 28499.98it/s][A
 31%|███       | 147099/478625 [00:05<00:11, 28779.30it/s][A
 31%|███▏      | 149979/478625 [00:05<00:11, 28289.98it/s][A
 32%|███▏      | 152955/478625 [00:05<00:11, 28718.93it/s][A
 33%|███▎      | 155932/478625 [00:05<00:11, 29028.41it/s][A
 33%|███▎      | 158838/478625 [00:05<00:11, 28678.23it/s][A
 34%|███▍      | 161739/478625 [00:05<00:11, 28774.38it/s][A
 34%|███▍      | 164707/478625 [00:05<00:10, 29041.75it/s][A
 35%|███▌      | 167613/478625 [00:05<00:10, 28680.46it/s][A
 36%|███▌      | 170521/478625 [00:05<00:10, 28795.91it/s][A
 36%|███▌      | 173493/478625 [00:06<00:10, 29066.83it/s][A
 37%|███▋      | 176401/478625 [00:06<00:10, 28613.32it/s][A
 37%|███▋      | 179404/478625 [00:06<00:10, 29030.22it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
 38%|███▊      | 182322/478625 [00:06<00:10, 29074.16it/s][A
  1%|          | 3292/478625 [00:00<00:14, 32915.64it/s][A
 39%|███▊      | 185232/478625 [00:06<00:10, 28571.99it/s][A
  1%|▏         | 6584/478625 [00:00<00:14, 32425.12it/s][A
 39%|███▉      | 188120/478625 [00:06<00:10, 28662.32it/s][A
  2%|▏         | 9948/478625 [00:00<00:14, 32972.50it/s][A
 40%|███▉      | 191117/478625 [00:06<00:09, 29048.91it/s][A
  3%|▎         | 13308/478625 [00:00<00:14, 33218.40it/s][A/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

 41%|████      | 194024/478625 [00:06<00:09, 28476.31it/s][A
  3%|▎         | 16631/478625 [00:00<00:14, 32314.33it/s][A/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

 41%|████      | 197012/478625 [00:06<00:09, 28888.22it/s][A
  4%|▍         | 19975/478625 [00:00<00:14, 32688.79it/s][A
 42%|████▏     | 199993/478625 [00:06<00:09, 29158.44it/s][A
  5%|▍         | 23248/478625 [00:00<00:14, 32089.96it/s][A
 42%|████▏     | 202912/478625 [00:07<00:09, 28626.32it/s][A
  6%|▌         | 26590/478625 [00:00<00:13, 32501.75it/s][A
 43%|████▎     | 205855/478625 [00:07<00:09, 28862.22it/s][A
  6%|▋         | 29938/478625 [00:00<00:13, 32802.13it/s][A
 44%|████▎     | 208745/478625 [00:07<00:09, 28398.36it/s][A
  7%|▋         | 33222/478625 [00:01<00:13, 32254.48it/s][A
 44%|████▍     | 211623/478625 [00:07<00:09, 28507.87it/s][A
  8%|▊         | 36564/478625 [00:01<00:13, 32601.37it/s][A
 45%|████▍     | 214599/478625 [00:07<00:09, 28874.54it/s][A
  8%|▊         | 39828/478625 [00:01<00:13, 32001.44it/s][A
 45%|████▌     | 217489/478625 [00:07<00:09, 28503.07it/s][A
  9%|▉         | 43186/478625 [00:01<00:13, 32467.59it/s][A
 46%|████▌     | 220488/478625 [00:07<00:08, 28912.35it/s][A
 10%|▉         | 46516/478625 [00:01<00:13, 32713.80it/s][A
 47%|████▋     | 223456/478625 [00:07<00:08, 29138.26it/s][A
 10%|█         | 49791/478625 [00:01<00:13, 32139.77it/s][A
 47%|████▋     | 226372/478625 [00:07<00:08, 28619.50it/s][A
 11%|█         | 53125/478625 [00:01<00:13, 32492.51it/s][A
 48%|████▊     | 229300/478625 [00:07<00:08, 28811.32it/s][A
 12%|█▏        | 56378/478625 [00:01<00:13, 31970.22it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
 49%|████▊     | 232184/478625 [00:08<00:08, 28801.34it/s][A
 12%|█▏        | 59735/478625 [00:01<00:12, 32438.84it/s][A
  1%|          | 3291/478625 [00:00<00:14, 32894.89it/s][A
 49%|████▉     | 235066/478625 [00:08<00:08, 28356.96it/s][A
 13%|█▎        | 63089/478625 [00:01<00:12, 32762.26it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
  1%|▏         | 6581/478625 [00:00<00:14, 32360.89it/s][A
 50%|████▉     | 238050/478625 [00:08<00:08, 28753.03it/s][A
 14%|█▍        | 66369/478625 [00:02<00:12, 32271.00it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
  2%|▏         | 9958/478625 [00:00<00:14, 32996.91it/s][A
 50%|█████     | 240989/478625 [00:08<00:08, 28938.44it/s][A
 15%|█▍        | 69749/478625 [00:02<00:12, 32720.59it/s][A
  3%|▎         | 13291/478625 [00:00<00:14, 33124.44it/s][A
 51%|█████     | 243885/478625 [00:08<00:08, 28403.48it/s][A
 15%|█▌        | 73025/478625 [00:02<00:12, 32110.90it/s][A
  3%|▎         | 16605/478625 [00:00<00:14, 32314.68it/s][A
 52%|█████▏    | 246846/478625 [00:08<00:08, 28756.91it/s][A
 16%|█▌        | 76390/478625 [00:02<00:12, 32559.92it/s][A
  4%|▍         | 19977/478625 [00:00<00:13, 32781.67it/s][A
 52%|█████▏    | 249820/478625 [00:08<00:07, 29046.93it/s][A/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(

 17%|█▋        | 79754/478625 [00:02<00:12, 32850.93it/s][A
  5%|▍         | 23259/478625 [00:00<00:14, 32175.52it/s][A
 53%|█████▎    | 252728/478625 [00:08<00:07, 28474.80it/s][A
 17%|█▋        | 83043/478625 [00:02<00:12, 32284.36it/s][A
  6%|▌         | 26615/478625 [00:00<00:13, 32605.92it/s][A
 53%|█████▎    | 255677/478625 [00:08<00:07, 28771.17it/s][A
 18%|█▊        | 86384/478625 [00:02<00:12, 32612.57it/s][A
  6%|▋         | 29968/478625 [00:00<00:13, 32889.19it/s][A
 54%|█████▍    | 258589/478625 [00:08<00:07, 28872.48it/s][A
 19%|█▊        | 89707/478625 [00:02<00:12, 32084.18it/s][A
  7%|▋         | 33260/478625 [00:01<00:13, 32352.24it/s][A
 55%|█████▍    | 261479/478625 [00:09<00:07, 27999.47it/s][A
 19%|█▉        | 93035/478625 [00:02<00:11, 32431.76it/s][A
  8%|▊         | 36609/478625 [00:01<00:13, 32692.84it/s][A
 55%|█████▌    | 264494/478625 [00:09<00:07, 28624.83it/s][A
 20%|██        | 96358/478625 [00:02<00:11, 32666.33it/s][A
  8%|▊         | 39882/478625 [00:01<00:13, 32116.93it/s][A
 56%|█████▌    | 267460/478625 [00:09<00:07, 28928.68it/s][A
 21%|██        | 99628/478625 [00:03<00:11, 32149.61it/s][A
  9%|▉         | 43257/478625 [00:01<00:13, 32598.28it/s][A
 56%|█████▋    | 270359/478625 [00:09<00:07, 28536.28it/s][A
 22%|██▏       | 102966/478625 [00:03<00:11, 32508.06it/s][A
 10%|▉         | 46605/478625 [00:01<00:13, 32859.95it/s][A
 57%|█████▋    | 273287/478625 [00:09<00:07, 28753.65it/s][A
 22%|██▏       | 106306/478625 [00:03<00:11, 32771.16it/s][A
 10%|█         | 49894/478625 [00:01<00:13, 32169.57it/s][A
 58%|█████▊    | 276166/478625 [00:09<00:07, 28132.05it/s][A
 23%|██▎       | 109586/478625 [00:03<00:11, 32147.17it/s][A
 11%|█         | 53255/478625 [00:01<00:13, 32591.01it/s][A
 58%|█████▊    | 278993/478625 [00:09<00:07, 28170.21it/s][A
 24%|██▎       | 112933/478625 [00:03<00:11, 32535.47it/s][A
 12%|█▏        | 56611/478625 [00:01<00:12, 32874.87it/s][A
 59%|█████▉    | 281922/478625 [00:09<00:06, 28499.40it/s][A
 24%|██▍       | 116191/478625 [00:03<00:11, 32038.73it/s][A
 13%|█▎        | 59902/478625 [00:01<00:12, 32311.60it/s][A
 59%|█████▉    | 284775/478625 [00:09<00:06, 28084.19it/s][A
 25%|██▍       | 119526/478625 [00:03<00:11, 32423.01it/s][A
 13%|█▎        | 63261/478625 [00:01<00:12, 32685.26it/s][A
 60%|██████    | 287782/478625 [00:10<00:06, 28667.53it/s][A
 26%|██▌       | 122867/478625 [00:03<00:10, 32713.20it/s][A
 14%|█▍        | 66534/478625 [00:02<00:12, 32214.85it/s][A
 61%|██████    | 290762/478625 [00:10<00:06, 29000.69it/s][A
 26%|██▋       | 126142/478625 [00:03<00:10, 32117.50it/s][A  0%|          | 0/1 [00:00<?, ?it/s]
 15%|█▍        | 69920/478625 [00:02<00:12, 32696.31it/s][A
 61%|██████▏   | 293665/478625 [00:10<00:06, 27827.88it/s][A
 27%|██▋       | 129464/478625 [00:03<00:10, 32440.40it/s][A
 15%|█▌        | 73257/478625 [00:02<00:12, 32894.33it/s][A
 62%|██████▏   | 296592/478625 [00:10<00:06, 28242.67it/s][A
 28%|██▊       | 132712/478625 [00:04<00:10, 32021.49it/s][A
 16%|█▌        | 76550/478625 [00:02<00:12, 32333.89it/s][A
 63%|██████▎   | 299595/478625 [00:10<00:06, 28762.96it/s][A
 28%|██▊       | 136083/478625 [00:04<00:10, 32514.57it/s][A
 17%|█▋        | 79890/478625 [00:02<00:12, 32646.49it/s][A
 63%|██████▎   | 302480/478625 [00:10<00:06, 28490.75it/s][A
 29%|██▉       | 139418/478625 [00:04<00:10, 32761.17it/s][A
 17%|█▋        | 83158/478625 [00:02<00:12, 32166.64it/s][A
 64%|██████▍   | 305461/478625 [00:10<00:05, 28876.43it/s][A
 30%|██▉       | 142697/478625 [00:04<00:10, 32160.11it/s][A
 18%|█▊        | 86501/478625 [00:02<00:12, 32535.66it/s][A
 64%|██████▍   | 308425/478625 [00:10<00:05, 29101.10it/s][A
 31%|███       | 146018/478625 [00:04<00:10, 32466.18it/s][A
 19%|█▉        | 89847/478625 [00:02<00:11, 32807.16it/s][A
 65%|██████▌   | 311339/478625 [00:10<00:05, 28653.24it/s][A
 31%|███       | 149268/478625 [00:04<00:10, 32070.55it/s][A
 19%|█▉        | 93131/478625 [00:02<00:11, 32263.39it/s][A
 66%|██████▌   | 314311/478625 [00:10<00:05, 28965.25it/s][A
 32%|███▏      | 152608/478625 [00:04<00:10, 32459.16it/s][A
 20%|██        | 96480/478625 [00:02<00:11, 32622.77it/s][A
 66%|██████▋   | 317305/478625 [00:11<00:05, 29251.55it/s][A
 33%|███▎      | 155955/478625 [00:04<00:09, 32756.03it/s][A
 21%|██        | 99746/478625 [00:03<00:11, 32140.08it/s][A
 67%|██████▋   | 320233/478625 [00:11<00:05, 28700.00it/s][A
 33%|███▎      | 159233/478625 [00:04<00:09, 32267.92it/s][A
 22%|██▏       | 103075/478625 [00:03<00:11, 32475.26it/s][A
 68%|██████▊   | 323166/478625 [00:11<00:05, 28884.39it/s][A
 34%|███▍      | 162508/478625 [00:05<00:09, 32408.85it/s][A
 22%|██▏       | 106439/478625 [00:03<00:11, 32817.87it/s][A
 68%|██████▊   | 326146/478625 [00:11<00:05, 29154.17it/s][A
 35%|███▍      | 165752/478625 [00:05<00:09, 31979.90it/s][A
 23%|██▎       | 109724/478625 [00:03<00:11, 32272.00it/s][A
 69%|██████▉   | 329064/478625 [00:11<00:05, 28428.58it/s][A
 35%|███▌      | 169109/478625 [00:05<00:09, 32445.10it/s][A
 24%|██▎       | 113072/478625 [00:03<00:11, 32626.89it/s][A
 69%|██████▉   | 332005/478625 [00:11<00:05, 28712.36it/s][A
 36%|███▌      | 172444/478625 [00:05<00:09, 32711.06it/s][A
 24%|██▍       | 116338/478625 [00:03<00:11, 32118.16it/s][A
 70%|██████▉   | 334881/478625 [00:11<00:05, 28717.49it/s][A
 37%|███▋      | 175718/478625 [00:05<00:09, 32104.14it/s][A
 25%|██▌       | 119674/478625 [00:03<00:11, 32481.05it/s][A
 71%|███████   | 337756/478625 [00:11<00:05, 27852.54it/s][A
 37%|███▋      | 179059/478625 [00:05<00:09, 32484.52it/s][A
 26%|██▌       | 123025/478625 [00:03<00:10, 32782.61it/s][A
 71%|███████   | 340723/478625 [00:11<00:04, 28378.87it/s][A
 38%|███▊      | 182335/478625 [00:05<00:09, 32564.17it/s][A
 26%|██▋       | 126306/478625 [00:03<00:10, 32201.00it/s][A
 72%|███████▏  | 343568/478625 [00:11<00:04, 28086.85it/s][A
 39%|███▉      | 185594/478625 [00:05<00:09, 31938.62it/s][A
 27%|██▋       | 129655/478625 [00:03<00:10, 32578.53it/s][A
 72%|███████▏  | 346534/478625 [00:12<00:04, 28547.44it/s][A
 39%|███▉      | 188921/478625 [00:05<00:08, 32326.90it/s][A
 28%|██▊       | 132917/478625 [00:04<00:10, 32133.47it/s][A
 73%|███████▎  | 349498/478625 [00:12<00:04, 28867.14it/s][A
 40%|████      | 192158/478625 [00:05<00:08, 31947.49it/s][A
 28%|██▊       | 136277/478625 [00:04<00:10, 32562.43it/s][A
 74%|███████▎  | 352389/478625 [00:12<00:04, 28346.13it/s][A
 41%|████      | 195462/478625 [00:06<00:08, 32266.49it/s][A
 29%|██▉       | 139636/478625 [00:04<00:10, 32863.63it/s][A
 74%|███████▍  | 355394/478625 [00:12<00:04, 28845.56it/s][A
 42%|████▏     | 198796/478625 [00:06<00:08, 32582.47it/s][A
 30%|██▉       | 142925/478625 [00:04<00:10, 32312.98it/s][A
 75%|███████▍  | 358352/478625 [00:12<00:04, 29059.76it/s][A
 42%|████▏     | 202057/478625 [00:06<00:08, 32071.38it/s][A
 31%|███       | 146258/478625 [00:04<00:10, 32611.58it/s][A
 75%|███████▌  | 361262/478625 [00:12<00:04, 28620.47it/s][A
 43%|████▎     | 205329/478625 [00:06<00:08, 32260.81it/s][A
 31%|███       | 149523/478625 [00:04<00:10, 32138.80it/s][A
 76%|███████▌  | 364258/478625 [00:12<00:03, 29013.16it/s][A
 44%|████▎     | 208558/478625 [00:06<00:08, 31820.56it/s][A
 32%|███▏      | 152883/478625 [00:04<00:10, 32567.49it/s][A
 77%|███████▋  | 367241/478625 [00:12<00:03, 29254.54it/s][A
 44%|████▍     | 211900/478625 [00:06<00:08, 32291.05it/s][A
 33%|███▎      | 156242/478625 [00:04<00:09, 32866.93it/s][A
 77%|███████▋  | 370169/478625 [00:12<00:03, 28401.12it/s][A
 45%|████▍     | 215225/478625 [00:06<00:08, 32572.00it/s][A
 33%|███▎      | 159532/478625 [00:04<00:09, 32311.80it/s][A
 78%|███████▊  | 373159/478625 [00:12<00:03, 28836.68it/s][A
 46%|████▌     | 218485/478625 [00:06<00:08, 32052.54it/s][A
 34%|███▍      | 162858/478625 [00:05<00:09, 32589.65it/s][A
 79%|███████▊  | 376095/478625 [00:13<00:03, 28987.51it/s][A
 46%|████▋     | 221855/478625 [00:06<00:07, 32535.05it/s][A
 35%|███▍      | 166242/478625 [00:05<00:09, 32956.03it/s][A
 79%|███████▉  | 378999/478625 [00:13<00:03, 28582.15it/s][A
 47%|████▋     | 225112/478625 [00:06<00:07, 32077.06it/s][A
 35%|███▌      | 169541/478625 [00:05<00:09, 32410.65it/s][A
 80%|███████▉  | 382032/478625 [00:13<00:03, 29093.59it/s][A
 48%|████▊     | 228420/478625 [00:07<00:07, 32371.37it/s][A
 36%|███▌      | 172893/478625 [00:05<00:09, 32734.91it/s][A
 80%|████████  | 384964/478625 [00:13<00:03, 29160.08it/s][A
 48%|████▊     | 231694/478625 [00:07<00:07, 32478.36it/s][A
 37%|███▋      | 176170/478625 [00:05<00:09, 32250.52it/s][A
 81%|████████  | 387883/478625 [00:13<00:03, 28619.72it/s][A
 49%|████▉     | 234944/478625 [00:07<00:07, 31978.17it/s][A
 38%|███▊      | 179526/478625 [00:05<00:09, 32634.73it/s][A
 82%|████████▏ | 390869/478625 [00:13<00:03, 28982.98it/s][A
 50%|████▉     | 238306/478625 [00:07<00:07, 32461.47it/s][A
 38%|███▊      | 182824/478625 [00:05<00:09, 32736.05it/s][A
 82%|████████▏ | 393820/478625 [00:13<00:02, 29135.69it/s][A
 50%|█████     | 241608/478625 [00:07<00:07, 31968.69it/s][A
 39%|███▉      | 186100/478625 [00:05<00:09, 32212.04it/s][A
 83%|████████▎ | 396737/478625 [00:13<00:02, 28598.91it/s][A
 51%|█████     | 244958/478625 [00:07<00:07, 32415.03it/s][A
 40%|███▉      | 189500/478625 [00:05<00:08, 32737.66it/s][A
 84%|████████▎ | 399720/478625 [00:13<00:02, 28958.65it/s][A
 52%|█████▏    | 248294/478625 [00:07<00:07, 32690.98it/s][A
 40%|████      | 192777/478625 [00:05<00:08, 32138.23it/s][A
 84%|████████▍ | 402620/478625 [00:14<00:02, 28374.12it/s][A
 53%|█████▎    | 251567/478625 [00:07<00:07, 32126.84it/s][A
 41%|████      | 196132/478625 [00:06<00:08, 32551.22it/s][A
 85%|████████▍ | 405589/478625 [00:14<00:02, 28758.81it/s][A
 53%|█████▎    | 254923/478625 [00:07<00:06, 32545.54it/s][A
 42%|████▏     | 199487/478625 [00:06<00:08, 32845.00it/s][A
 85%|████████▌ | 408518/478625 [00:14<00:02, 28912.07it/s][A
 54%|█████▍    | 258240/478625 [00:07<00:06, 32728.60it/s][A
 42%|████▏     | 202775/478625 [00:06<00:08, 32348.75it/s][A
 86%|████████▌ | 411413/478625 [00:14<00:02, 28426.35it/s][A
 55%|█████▍    | 261516/478625 [00:08<00:06, 31987.83it/s][A
 43%|████▎     | 206067/478625 [00:06<00:08, 32514.54it/s][A
 87%|████████▋ | 414395/478625 [00:14<00:02, 28835.33it/s][A
 55%|█████▌    | 264848/478625 [00:08<00:06, 32377.85it/s][A
 44%|████▎     | 209322/478625 [00:06<00:08, 31994.04it/s][A
 87%|████████▋ | 417282/478625 [00:14<00:02, 28491.85it/s][A
 56%|█████▌    | 268091/478625 [00:08<00:06, 31866.92it/s][A
 44%|████▍     | 212685/478625 [00:06<00:08, 32474.06it/s][A
 88%|████████▊ | 420134/478625 [00:14<00:02, 28106.69it/s][A
 57%|█████▋    | 271422/478625 [00:08<00:06, 32287.34it/s][A
 45%|████▌     | 216046/478625 [00:06<00:08, 32808.03it/s][A
 88%|████████▊ | 422999/478625 [00:14<00:01, 28239.28it/s][A
 57%|█████▋    | 274753/478625 [00:08<00:06, 32587.16it/s][A
 46%|████▌     | 219330/478625 [00:06<00:08, 32282.13it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
 89%|████████▉ | 425990/478625 [00:14<00:01, 28730.75it/s][A
 58%|█████▊    | 278015/478625 [00:08<00:06, 32078.42it/s][A
 47%|████▋     | 222691/478625 [00:06<00:07, 32670.32it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A
  1%|          | 3235/478625 [00:00<00:14, 32345.56it/s][A
 90%|████████▉ | 428866/478625 [00:14<00:01, 28330.01it/s][A
 59%|█████▉    | 281364/478625 [00:08<00:06, 32492.10it/s][A
 47%|████▋     | 225962/478625 [00:06<00:07, 32193.66it/s][A
  1%|          | 3330/478625 [00:00<00:14, 33280.27it/s][A
  1%|▏         | 6470/478625 [00:00<00:15, 31102.47it/s][A
 90%|█████████ | 431846/478625 [00:15<00:01, 28761.78it/s][A
 59%|█████▉    | 284617/478625 [00:08<00:06, 32018.15it/s][A
 48%|████▊     | 229292/478625 [00:07<00:07, 32517.37it/s][A
  1%|▏         | 6659/478625 [00:00<00:14, 32635.55it/s][A
  2%|▏         | 9772/478625 [00:00<00:14, 31958.37it/s][A
 91%|█████████ | 434835/478625 [00:15<00:01, 29094.50it/s][A
 60%|██████    | 287985/478625 [00:08<00:05, 32503.93it/s][A
 49%|████▊     | 232573/478625 [00:07<00:07, 32603.32it/s][A
  2%|▏         | 10059/478625 [00:00<00:14, 33251.02it/s][A
  3%|▎         | 13026/478625 [00:00<00:14, 32181.84it/s][A
 91%|█████████▏| 437747/478625 [00:15<00:01, 28596.84it/s][A
 61%|██████    | 291335/478625 [00:08<00:05, 32797.62it/s][A
 49%|████▉     | 235836/478625 [00:07<00:07, 32189.60it/s][A
  3%|▎         | 13471/478625 [00:00<00:13, 33587.62it/s][A
  3%|▎         | 16247/478625 [00:00<00:14, 31507.13it/s][A
 92%|█████████▏| 440742/478625 [00:15<00:01, 28993.01it/s][A
 62%|██████▏   | 294618/478625 [00:09<00:05, 31965.93it/s][A
 50%|████▉     | 239195/478625 [00:07<00:07, 32602.54it/s][A
  4%|▎         | 16831/478625 [00:00<00:14, 32608.57it/s][A
  4%|▍         | 19549/478625 [00:00<00:14, 32008.31it/s][A
 93%|█████████▎| 443717/478625 [00:15<00:01, 29215.63it/s][A
 62%|██████▏   | 297973/478625 [00:09<00:05, 32427.97it/s][A
 51%|█████     | 242458/478625 [00:07<00:07, 32092.70it/s][A
  4%|▍         | 20209/478625 [00:00<00:13, 32994.70it/s][A
  5%|▍         | 22754/478625 [00:00<00:14, 31618.51it/s][A
 93%|█████████▎| 446641/478625 [00:15<00:01, 28660.34it/s][A
 63%|██████▎   | 301222/478625 [00:09<00:05, 32026.69it/s][A
 51%|█████▏    | 245822/478625 [00:07<00:07, 32545.82it/s][A
  5%|▍         | 23513/478625 [00:00<00:14, 32393.66it/s][A
  5%|▌         | 25919/478625 [00:00<00:14, 31045.81it/s][A
 94%|█████████▍| 449633/478625 [00:15<00:00, 29028.49it/s][A
 64%|██████▎   | 304533/478625 [00:09<00:05, 32343.93it/s][A
 52%|█████▏    | 249173/478625 [00:07<00:06, 32828.58it/s][A
  6%|▌         | 26916/478625 [00:00<00:13, 32901.89it/s][A
  6%|▌         | 29143/478625 [00:00<00:14, 31408.97it/s][A
 95%|█████████▍| 452601/478625 [00:15<00:00, 29218.72it/s][A
 64%|██████▍   | 307875/478625 [00:09<00:05, 32660.32it/s][A
 53%|█████▎    | 252459/478625 [00:07<00:07, 32262.97it/s][A
  6%|▋         | 30326/478625 [00:00<00:13, 33267.39it/s][A
  7%|▋         | 32287/478625 [00:01<00:14, 30875.09it/s][A
 95%|█████████▌| 455526/478625 [00:15<00:00, 28623.73it/s][A
 65%|██████▌   | 311145/478625 [00:09<00:05, 32164.84it/s][A
 53%|█████▎    | 255821/478625 [00:07<00:06, 32660.32it/s][A
  7%|▋         | 33657/478625 [00:01<00:13, 32604.68it/s][A
  7%|▋         | 35437/478625 [00:01<00:14, 31060.17it/s][A
 96%|█████████▌| 458513/478625 [00:15<00:00, 28987.62it/s][A
 66%|██████▌   | 314492/478625 [00:09<00:05, 32547.03it/s][A
 54%|█████▍    | 259093/478625 [00:07<00:06, 32674.84it/s][A
  8%|▊         | 37033/478625 [00:01<00:13, 32949.47it/s][A
  8%|▊         | 38643/478625 [00:01<00:14, 31357.18it/s][A
 96%|█████████▋| 461521/478625 [00:16<00:00, 29309.64it/s][A
 66%|██████▋   | 317750/478625 [00:09<00:05, 32062.49it/s][A
 55%|█████▍    | 262363/478625 [00:08<00:06, 32107.73it/s][A
  8%|▊         | 40332/478625 [00:01<00:13, 32421.38it/s][A
  9%|▊         | 41782/478625 [00:01<00:14, 30991.20it/s][A
 97%|█████████▋| 464455/478625 [00:16<00:00, 28454.01it/s][A
 67%|██████▋   | 321107/478625 [00:09<00:04, 32503.09it/s][A
 56%|█████▌    | 265719/478625 [00:08<00:06, 32532.58it/s][A
  9%|▉         | 43711/478625 [00:01<00:13, 32823.88it/s][A
  9%|▉         | 44964/478625 [00:01<00:13, 31235.62it/s][A
 98%|█████████▊| 467308/478625 [00:16<00:00, 28474.15it/s][A
 68%|██████▊   | 324469/478625 [00:10<00:04, 32833.06it/s][A
 56%|█████▌    | 268976/478625 [00:08<00:06, 32061.60it/s][A
 10%|▉         | 47120/478625 [00:01<00:12, 33196.90it/s][A
 10%|█         | 48191/478625 [00:01<00:13, 30846.52it/s][A
 98%|█████████▊| 470161/478625 [00:16<00:00, 28171.37it/s][A
 68%|██████▊   | 327755/478625 [00:10<00:04, 32182.32it/s][A
 57%|█████▋    | 272343/478625 [00:08<00:06, 32533.15it/s][A
 11%|█         | 50443/478625 [00:01<00:13, 32505.36it/s][A
 11%|█         | 51419/478625 [00:01<00:13, 31265.93it/s][A
 99%|█████████▉| 473166/478625 [00:16<00:00, 28720.90it/s][A
 69%|██████▉   | 331014/478625 [00:10<00:04, 32301.48it/s][A
 58%|█████▊    | 275702/478625 [00:08<00:06, 32845.32it/s][A
 11%|█▏        | 53846/478625 [00:01<00:12, 32953.50it/s][A
 11%|█▏        | 54685/478625 [00:01<00:13, 31676.35it/s][A
 99%|█████████▉| 476196/478625 [00:16<00:00, 29185.44it/s][A
 70%|██████▉   | 334388/478625 [00:10<00:04, 32724.55it/s][A
 58%|█████▊    | 278990/478625 [00:08<00:06, 32251.97it/s][A
 12%|█▏        | 57146/478625 [00:01<00:13, 32378.42it/s][A100%|██████████| 478625/478625 [00:16<00:00, 28743.92it/s]
100%|██████████| 1/1 [00:23<00:00, 23.27s/it]100%|██████████| 1/1 [00:23<00:00, 23.27s/it]

 12%|█▏        | 57856/478625 [00:01<00:13, 31149.31it/s][A
 71%|███████   | 337664/478625 [00:10<00:04, 32050.76it/s][A
 59%|█████▉    | 282364/478625 [00:08<00:06, 32687.66it/s][A
 13%|█▎        | 60527/478625 [00:01<00:12, 32797.63it/s][A
  0%|          | 0/478625 [00:00<?, ?it/s][A09/09/2024 12:29:59 - INFO - opensora.dataset.t2v_datasets - no_cap: 0, too_long: 3711, too_short: 2, no_resolution: 0, resolution_mismatch: 0, Counter(sample_size): Counter({'93x160x320': 84930, '29x160x320': 73201, '45x160x320': 68295, '61x160x320': 44578, '77x160x320': 38630, '93x128x320': 17805, '29x128x320': 16948, '93x224x320': 16403, '93x192x320': 15259, '45x128x320': 14788, '61x128x320': 9795, '29x224x320': 8615, '29x192x320': 8528, '45x224x320': 8477, '45x192x320': 8309, '77x128x320': 7730, '61x224x320': 6211, '61x192x320': 5983, '77x224x320': 5788, '77x192x320': 5268, '93x256x320': 3164, '45x256x320': 1510, '29x256x320': 1480, '61x256x320': 1152, '77x256x320': 1090, '93x96x320': 282, '45x96x320': 200, '29x96x320': 169, '61x96x320': 163, '77x96x320': 148}), cnt_movie: 0, cnt_img: 0, before filter: 478625, after filter: 474899

 13%|█▎        | 61020/478625 [00:01<00:13, 31292.89it/s][A
 71%|███████▏  | 341023/478625 [00:10<00:04, 32500.88it/s][A
 60%|█████▉    | 285637/478625 [00:08<00:05, 32205.63it/s][A
 13%|█▎        | 63921/478625 [00:01<00:12, 33134.46it/s][A
  1%|          | 3251/478625 [00:00<00:14, 32497.63it/s][A
 13%|█▎        | 64236/478625 [00:02<00:13, 31548.45it/s][A
 72%|███████▏  | 344278/478625 [00:10<00:04, 31975.72it/s][A
 60%|██████    | 288987/478625 [00:08<00:05, 32584.91it/s][A
 14%|█▍        | 67238/478625 [00:02<00:12, 32576.42it/s][A
  1%|▏         | 6501/478625 [00:00<00:14, 31842.45it/s][A
 14%|█▍        | 67394/478625 [00:02<00:13, 31066.56it/s][A
 73%|███████▎  | 347610/478625 [00:10<00:04, 32367.96it/s][A09/09/2024 12:29:59 - INFO - opensora.dataset.t2v_datasets - before filter: 478625, after filter: 474899 | motion_score: 474899, cnt_no_motion: 13 | 192077 > 0.95, 0.7 > 65730 Mean: 0.8593367888417824, Var: 0.03075349223473551, Std: 0.17536673639757203, Min: -0.0717548280954361, Max: 1.0

 61%|██████    | 292275/478625 [00:08<00:05, 32670.01it/s][A
 15%|█▍        | 70634/478625 [00:02<00:12, 32980.80it/s][A
  2%|▏         | 9829/478625 [00:00<00:14, 32489.52it/s][A
 15%|█▍        | 70572/478625 [00:02<00:13, 31275.86it/s][A
 73%|███████▎  | 350948/478625 [00:10<00:03, 32664.04it/s][A
 62%|██████▏   | 295545/478625 [00:09<00:05, 32194.25it/s][A
  3%|▎         | 13145/478625 [00:00<00:14, 32751.51it/s][A
 15%|█▌        | 73936/478625 [00:02<00:12, 32395.45it/s][A
 15%|█▌        | 73702/478625 [00:02<00:13, 30343.73it/s][A
 74%|███████▍  | 354218/478625 [00:10<00:03, 32127.59it/s][A
 62%|██████▏   | 298893/478625 [00:09<00:05, 32571.40it/s][A
 16%|█▌        | 77323/478625 [00:02<00:12, 32827.27it/s][A
  3%|▎         | 16422/478625 [00:00<00:14, 31960.71it/s][A
 16%|█▌        | 76744/478625 [00:02<00:13, 30282.47it/s][A
 75%|███████▍  | 357585/478625 [00:11<00:03, 32580.51it/s][A09/09/2024 12:29:59 - INFO - opensora.dataset.t2v_datasets - before filter: 478625, after filter: 474899 | aesthetic_score: 478625, cnt_no_aesthetic: 0 | 14374 > 5.75, 4.5 > 113830 Mean: 4.846693657797633, Var: 0.24147353645946146, Std: 0.4913995690468821, Min: 2.685077953338623, Max: 6.742257436116536
time 23.925863027572632
n_elements: 474899
data length: 474899

 63%|██████▎   | 302153/478625 [00:09<00:05, 32097.18it/s][A
 17%|█▋        | 80700/478625 [00:02<00:12, 33104.99it/s][A
  4%|▍         | 19743/478625 [00:00<00:14, 32376.70it/s][A
 17%|█▋        | 79847/478625 [00:02<00:13, 30498.16it/s][A
 75%|███████▌  | 360847/478625 [00:11<00:03, 32157.38it/s][A
 64%|██████▍   | 305530/478625 [00:09<00:05, 32586.70it/s][A
 18%|█▊        | 84014/478625 [00:02<00:12, 32559.18it/s][A
  5%|▍         | 22984/478625 [00:00<00:14, 31713.46it/s][A
 17%|█▋        | 82901/478625 [00:02<00:13, 30304.13it/s][A
 76%|███████▌  | 364209/478625 [00:11<00:03, 32585.64it/s][A
 65%|██████▍   | 308878/478625 [00:09<00:05, 32848.91it/s][A
 18%|█▊        | 87393/478625 [00:02<00:11, 32920.35it/s][A
  5%|▌         | 26315/478625 [00:00<00:14, 32207.09it/s][A
 18%|█▊        | 86178/478625 [00:02<00:12, 30990.72it/s][A
 77%|███████▋  | 367539/478625 [00:11<00:03, 32796.03it/s][A
 65%|██████▌   | 312166/478625 [00:09<00:05, 32305.85it/s][A
  6%|▌         | 29624/478625 [00:00<00:13, 32475.59it/s][A
 19%|█▉        | 90689/478625 [00:02<00:11, 32366.08it/s][A
 19%|█▊        | 89439/478625 [00:02<00:12, 31469.46it/s][A
 77%|███████▋  | 370822/478625 [00:11<00:03, 32081.89it/s][A
 66%|██████▌   | 315514/478625 [00:09<00:04, 32648.99it/s][A
 20%|█▉        | 94054/478625 [00:02<00:11, 32742.07it/s][A
  7%|▋         | 32875/478625 [00:01<00:13, 31915.09it/s][A
 19%|█▉        | 92589/478625 [00:02<00:12, 30866.67it/s][A
 78%|███████▊  | 374161/478625 [00:11<00:03, 32464.42it/s][A
 67%|██████▋   | 318782/478625 [00:09<00:04, 32194.17it/s][A
 20%|██        | 97443/478625 [00:02<00:11, 33035.04it/s][A
  8%|▊         | 36203/478625 [00:01<00:13, 32322.10it/s][A
 20%|██        | 95744/478625 [00:03<00:12, 31065.98it/s][A
 79%|███████▉  | 377412/478625 [00:11<00:03, 32011.14it/s][A
 67%|██████▋   | 322140/478625 [00:09<00:04, 32599.61it/s][A
  8%|▊         | 39500/478625 [00:01<00:13, 32515.71it/s][A
 21%|██        | 100750/478625 [00:03<00:11, 32536.23it/s][A
 21%|██        | 98854/478625 [00:03<00:12, 30515.81it/s][A
 80%|███████▉  | 380796/478625 [00:11<00:03, 32546.90it/s][A
 68%|██████▊   | 325480/478625 [00:10<00:04, 32835.15it/s][A
 22%|██▏       | 104107/478625 [00:03<00:11, 32838.95it/s][A
  9%|▉         | 42755/478625 [00:01<00:13, 31883.97it/s][A
 21%|██▏       | 101950/478625 [00:03<00:12, 30644.10it/s][A
 80%|████████  | 384155/478625 [00:11<00:02, 32854.57it/s][A09/09/2024 12:30:00 - INFO - __main__ - after train_dataloader
09/09/2024 12:30:00 - INFO - __main__ - before accelerator.prepare
[2024-09-09 12:30:00,669] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown

 69%|██████▊   | 328766/478625 [00:10<00:04, 32071.06it/s][A
 10%|▉         | 46072/478625 [00:01<00:13, 32263.35it/s][A
 22%|██▏       | 107394/478625 [00:03<00:11, 32227.00it/s][A
 22%|██▏       | 105118/478625 [00:03<00:12, 30947.63it/s][A
 81%|████████  | 387444/478625 [00:11<00:02, 32283.28it/s][A
 69%|██████▉   | 332119/478625 [00:10<00:04, 32498.06it/s][A
 23%|██▎       | 110805/478625 [00:03<00:11, 32776.62it/s][A
 10%|█         | 49302/478625 [00:01<00:13, 31753.48it/s][A
 23%|██▎       | 108216/478625 [00:03<00:12, 30538.17it/s][A
 82%|████████▏ | 390792/478625 [00:12<00:02, 32633.04it/s][A
 70%|███████   | 335374/478625 [00:10<00:04, 31977.01it/s][A
 24%|██▍       | 114182/478625 [00:03<00:11, 33067.47it/s][A
 11%|█         | 52614/478625 [00:01<00:13, 32155.47it/s][A
 23%|██▎       | 111319/478625 [00:03<00:11, 30680.78it/s][A
 82%|████████▏ | 394059/478625 [00:12<00:02, 32196.78it/s][A
 71%|███████   | 338726/478625 [00:10<00:04, 32427.76it/s][A
 12%|█▏        | 55936/478625 [00:01<00:13, 32469.82it/s][A
 25%|██▍       | 117492/478625 [00:03<00:11, 32423.89it/s][A
 24%|██▍       | 114591/478625 [00:03<00:11, 31283.58it/s][A
 83%|████████▎ | 397397/478625 [00:12<00:02, 32542.75it/s][A
 71%|███████▏  | 342114/478625 [00:10<00:04, 32855.24it/s][A
 25%|██▌       | 120879/478625 [00:03<00:10, 32845.82it/s][A
 12%|█▏        | 59186/478625 [00:01<00:13, 31899.09it/s][A
 25%|██▍       | 117722/478625 [00:03<00:11, 30432.47it/s][A
 84%|████████▎ | 400747/478625 [00:12<00:02, 32823.67it/s][A
 72%|███████▏  | 345404/478625 [00:10<00:04, 32300.34it/s][A
 13%|█▎        | 62489/478625 [00:01<00:12, 32215.27it/s][A
 26%|██▌       | 124168/478625 [00:03<00:10, 32332.51it/s][A
 25%|██▌       | 120772/478625 [00:03<00:11, 30305.17it/s][A
 84%|████████▍ | 404032/478625 [00:12<00:02, 32362.26it/s][A
 73%|███████▎  | 348759/478625 [00:10<00:03, 32666.20it/s][A
 27%|██▋       | 127569/478625 [00:03<00:10, 32821.51it/s][A
 14%|█▎        | 65714/478625 [00:02<00:12, 31765.77it/s][A
 26%|██▌       | 124044/478625 [00:03<00:11, 31012.52it/s][A
 85%|████████▌ | 407382/478625 [00:12<00:02, 32695.76it/s][A
 74%|███████▎  | 352055/478625 [00:10<00:03, 32182.95it/s][A
 27%|██▋       | 130945/478625 [00:03<00:10, 33097.18it/s][A
 14%|█▍        | 69037/478625 [00:02<00:12, 32195.04it/s][A
 27%|██▋       | 127150/478625 [00:04<00:11, 30702.51it/s][A
 86%|████████▌ | 410655/478625 [00:12<00:02, 32099.21it/s][A
 74%|███████▍  | 355442/478625 [00:10<00:03, 32675.36it/s][A
 15%|█▌        | 72342/478625 [00:02<00:12, 32445.80it/s][A
 28%|██▊       | 134258/478625 [00:04<00:10, 32481.43it/s][A
 27%|██▋       | 130439/478625 [00:04<00:11, 31345.23it/s][A
 86%|████████▋ | 413969/478625 [00:12<00:01, 32403.64it/s][A
 75%|███████▍  | 358810/478625 [00:11<00:03, 32970.64it/s][A
 29%|██▉       | 137608/478625 [00:04<00:10, 32779.84it/s][A
 16%|█▌        | 75590/478625 [00:02<00:12, 31945.29it/s][A
 28%|██▊       | 133578/478625 [00:04<00:11, 30705.50it/s][A
 87%|████████▋ | 417225/478625 [00:12<00:01, 32447.70it/s][A
 76%|███████▌  | 362111/478625 [00:11<00:03, 32404.42it/s][A
 29%|██▉       | 141000/478625 [00:04<00:10, 33114.39it/s][A
 16%|█▋        | 78912/478625 [00:02<00:12, 32318.47it/s][A
 29%|██▊       | 136654/478625 [00:04<00:11, 30609.96it/s][A
 88%|████████▊ | 420473/478625 [00:12<00:01, 31973.44it/s][A
 76%|███████▋  | 365475/478625 [00:11<00:03, 32766.55it/s][A
 30%|███       | 144315/478625 [00:04<00:10, 32352.02it/s][A
 17%|█▋        | 82147/478625 [00:02<00:12, 31783.12it/s][A
 29%|██▉       | 139920/478625 [00:04<00:10, 31210.68it/s][A
 89%|████████▊ | 423827/478625 [00:13<00:01, 32432.88it/s][A
 77%|███████▋  | 368756/478625 [00:11<00:03, 32695.44it/s][A
 31%|███       | 147608/478625 [00:04<00:10, 32518.75it/s][A
 18%|█▊        | 85465/478625 [00:02<00:12, 32192.77it/s][A
 30%|██▉       | 143045/478625 [00:04<00:10, 30574.60it/s][A
 89%|████████▉ | 427182/478625 [00:13<00:01, 32762.50it/s][A
 78%|███████▊  | 372028/478625 [00:11<00:03, 32118.03it/s][A
 19%|█▊        | 88784/478625 [00:02<00:12, 32485.82it/s][A
 32%|███▏      | 150865/478625 [00:04<00:10, 32078.67it/s][A
 31%|███       | 146177/478625 [00:04<00:10, 30792.03it/s][A
 90%|████████▉ | 430461/478625 [00:13<00:01, 32211.38it/s][A
 78%|███████▊  | 375388/478625 [00:11<00:03, 32550.32it/s][A
 32%|███▏      | 154241/478625 [00:04<00:09, 32571.06it/s][A
 19%|█▉        | 92036/478625 [00:02<00:12, 31860.54it/s][A
 31%|███       | 149449/478625 [00:04<00:10, 30686.27it/s][A
 91%|█████████ | 433803/478625 [00:13<00:01, 32567.01it/s][A
 79%|███████▉  | 378647/478625 [00:11<00:03, 32104.55it/s][A
 33%|███▎      | 157607/478625 [00:04<00:09, 32891.87it/s][A
 20%|█▉        | 95333/478625 [00:02<00:11, 32185.72it/s][A
 32%|███▏      | 152748/478625 [00:04<00:10, 31356.95it/s][A
 91%|█████████▏| 437063/478625 [00:13<00:01, 32135.62it/s][A
 80%|███████▉  | 382017/478625 [00:11<00:02, 32572.91it/s][A
 21%|██        | 98663/478625 [00:03<00:11, 32512.19it/s][A
 34%|███▎      | 160900/478625 [00:04<00:09, 32310.38it/s][A
 33%|███▎      | 156033/478625 [00:05<00:10, 31794.97it/s][A
 92%|█████████▏| 440417/478625 [00:13<00:01, 32548.52it/s][A
 81%|████████  | 385373/478625 [00:11<00:02, 32862.87it/s][A
 34%|███▍      | 164253/478625 [00:05<00:09, 32639.09it/s][A
 21%|██▏       | 101918/478625 [00:03<00:11, 31889.47it/s][A
 33%|███▎      | 159217/478625 [00:05<00:10, 31045.86it/s][A
 93%|█████████▎| 443776/478625 [00:13<00:01, 32855.74it/s][A
 81%|████████  | 388662/478625 [00:11<00:02, 32308.14it/s][A
 22%|██▏       | 105232/478625 [00:03<00:11, 32254.98it/s][A
 35%|███▌      | 167521/478625 [00:05<00:09, 32213.57it/s][A
 34%|███▍      | 162328/478625 [00:05<00:10, 30622.17it/s][A
 93%|█████████▎| 447064/478625 [00:13<00:00, 32210.33it/s][A
 82%|████████▏ | 392034/478625 [00:12<00:02, 32720.89it/s][A
 36%|███▌      | 170871/478625 [00:05<00:09, 32590.30it/s][A
 23%|██▎       | 108462/478625 [00:03<00:11, 31880.42it/s][A
 35%|███▍      | 165423/478625 [00:05<00:10, 30716.30it/s][A
 94%|█████████▍| 450415/478625 [00:13<00:00, 32591.36it/s][A
 83%|████████▎ | 395310/478625 [00:12<00:02, 32194.77it/s][A
 36%|███▋      | 174212/478625 [00:05<00:09, 32830.74it/s][A
 23%|██▎       | 111794/478625 [00:03<00:11, 32303.04it/s][A
 35%|███▌      | 168498/478625 [00:05<00:10, 30557.13it/s][A
 95%|█████████▍| 453678/478625 [00:14<00:00, 32050.17it/s][A
 83%|████████▎ | 398674/478625 [00:12<00:02, 32616.82it/s][A
 24%|██▍       | 115108/478625 [00:03<00:11, 32548.88it/s][A
 37%|███▋      | 177498/478625 [00:05<00:09, 32340.42it/s][A
 36%|███▌      | 171571/478625 [00:05<00:10, 30606.28it/s][A
 95%|█████████▌| 457036/478625 [00:14<00:00, 32497.53it/s][A
 84%|████████▍ | 402041/478625 [00:12<00:02, 32926.55it/s][A
 38%|███▊      | 180836/478625 [00:05<00:09, 32645.08it/s][A
 25%|██▍       | 118366/478625 [00:03<00:11, 32011.70it/s][A
 37%|███▋      | 174773/478625 [00:05<00:09, 30385.71it/s][A
 96%|█████████▌| 460399/478625 [00:14<00:00, 32831.55it/s][A
 85%|████████▍ | 405337/478625 [00:12<00:02, 32353.14it/s][A
 25%|██▌       | 121693/478625 [00:03<00:11, 32380.00it/s][A
 38%|███▊      | 184104/478625 [00:05<00:09, 32022.34it/s][A
 37%|███▋      | 177963/478625 [00:05<00:09, 30828.71it/s][A
 97%|█████████▋| 463686/478625 [00:14<00:00, 32188.29it/s][A
 85%|████████▌ | 408699/478625 [00:12<00:02, 32723.38it/s][A
 39%|███▉      | 187489/478625 [00:05<00:08, 32557.58it/s][A
 26%|██▌       | 124934/478625 [00:03<00:11, 31833.03it/s][A
 38%|███▊      | 181145/478625 [00:05<00:09, 31119.14it/s][A
 98%|█████████▊| 466919/478625 [00:14<00:00, 32226.91it/s][A
 86%|████████▌ | 411975/478625 [00:12<00:02, 32077.58it/s][A
 40%|███▉      | 190894/478625 [00:05<00:08, 32997.05it/s][A
 27%|██▋       | 128246/478625 [00:03<00:10, 32209.58it/s][A
 38%|███▊      | 184260/478625 [00:05<00:09, 30251.68it/s][A
 98%|█████████▊| 470145/478625 [00:14<00:00, 31902.09it/s][A
 87%|████████▋ | 415246/478625 [00:12<00:01, 32259.68it/s][A
 27%|██▋       | 131590/478625 [00:04<00:10, 32571.12it/s][A
 41%|████      | 194198/478625 [00:05<00:08, 32486.82it/s][A
 39%|███▉      | 187554/478625 [00:06<00:09, 31034.66it/s][A
 99%|█████████▉| 473492/478625 [00:14<00:00, 32361.91it/s][A
 87%|████████▋ | 418611/478625 [00:12<00:01, 32668.04it/s][A
 41%|████▏     | 197568/478625 [00:06<00:08, 32841.03it/s][A
 28%|██▊       | 134850/478625 [00:04<00:10, 32015.38it/s][A
100%|█████████▉| 476855/478625 [00:14<00:00, 32735.33it/s][A
 40%|███▉      | 190664/478625 [00:06<00:09, 30176.30it/s][A
 88%|████████▊ | 421881/478625 [00:12<00:01, 32174.99it/s][A100%|██████████| 478625/478625 [00:14<00:00, 32373.29it/s]
100%|██████████| 1/1 [00:21<00:00, 21.43s/it]100%|██████████| 1/1 [00:21<00:00, 21.43s/it]

 29%|██▉       | 138154/478625 [00:04<00:10, 32315.57it/s][A
 42%|████▏     | 200856/478625 [00:06<00:08, 32345.47it/s][A
 40%|████      | 193690/478625 [00:06<00:09, 30145.80it/s][A
 89%|████████▉ | 425240/478625 [00:13<00:01, 32589.37it/s][A
 43%|████▎     | 204218/478625 [00:06<00:08, 32717.94it/s][A
 30%|██▉       | 141389/478625 [00:04<00:10, 31844.98it/s][A
 41%|████      | 196976/478625 [00:06<00:09, 30938.41it/s][A
 90%|████████▉ | 428503/478625 [00:13<00:01, 32176.84it/s][A
 43%|████▎     | 207530/478625 [00:06<00:08, 32835.85it/s][A
 30%|███       | 144670/478625 [00:04<00:10, 32126.00it/s][A
 42%|████▏     | 200076/478625 [00:06<00:09, 30933.18it/s][A
 90%|█████████ | 431881/478625 [00:13<00:01, 32647.62it/s][A
 31%|███       | 148020/478625 [00:04<00:10, 32530.27it/s][A
 44%|████▍     | 210816/478625 [00:06<00:08, 32255.23it/s][A
 42%|████▏     | 203174/478625 [00:06<00:09, 29831.31it/s][A
 91%|█████████ | 435255/478625 [00:13<00:01, 32969.39it/s][A
 45%|████▍     | 214186/478625 [00:06<00:08, 32679.14it/s][A
 32%|███▏      | 151276/478625 [00:04<00:10, 31916.22it/s][A
 43%|████▎     | 206386/478625 [00:06<00:08, 30493.66it/s][A
 92%|█████████▏| 438555/478625 [00:13<00:01, 32425.44it/s][A
 32%|███▏      | 154603/478625 [00:04<00:10, 32312.29it/s][A
 45%|████▌     | 217458/478625 [00:06<00:08, 32215.11it/s][A
 44%|████▍     | 209445/478625 [00:06<00:08, 30020.73it/s][A
 92%|█████████▏| 441906/478625 [00:13<00:01, 32744.28it/s][A
 46%|████▌     | 220837/478625 [00:06<00:07, 32676.95it/s][A
 33%|███▎      | 157913/478625 [00:04<00:10, 31839.04it/s][Atime 22.08217716217041

 44%|████▍     | 212676/478625 [00:06<00:08, 30686.09it/s][An_elements: 474899
data length: 474899

 93%|█████████▎| 445184/478625 [00:13<00:01, 32187.44it/s][A
 47%|████▋     | 224224/478625 [00:06<00:07, 33028.69it/s][A
 34%|███▎      | 161246/478625 [00:05<00:09, 32273.74it/s][A
 45%|████▌     | 215862/478625 [00:06<00:08, 31030.35it/s][A
 94%|█████████▎| 448578/478625 [00:13<00:00, 32700.10it/s][A

 34%|███▍      | 164529/478625 [00:05<00:09, 32435.86it/s][A 48%|████▊     | 227530/478625 [00:06<00:07, 32549.27it/s][A
 46%|████▌     | 218971/478625 [00:07<00:08, 30850.80it/s][A
 94%|█████████▍| 451941/478625 [00:13<00:00, 32950.86it/s][A
 48%|████▊     | 230845/478625 [00:07<00:07, 32723.69it/s][A
 35%|███▌      | 167776/478625 [00:05<00:09, 31982.89it/s][A
 46%|████▋     | 222255/478625 [00:07<00:08, 31436.14it/s][A
 95%|█████████▌| 455239/478625 [00:14<00:00, 32452.80it/s][A
 36%|███▌      | 171115/478625 [00:05<00:09, 32394.45it/s][A
 49%|████▉     | 234120/478625 [00:07<00:07, 32216.68it/s][A
 47%|████▋     | 225436/478625 [00:07<00:08, 31163.70it/s][A
 96%|█████████▌| 458602/478625 [00:14<00:00, 32796.74it/s][A
 36%|███▋      | 174462/478625 [00:05<00:09, 32710.10it/s][A
 50%|████▉     | 237486/478625 [00:07<00:07, 32638.66it/s][A
 48%|████▊     | 228715/478625 [00:07<00:07, 31642.01it/s][A
 97%|█████████▋| 461885/478625 [00:14<00:00, 32297.70it/s][A
 50%|█████     | 240873/478625 [00:07<00:07, 33002.53it/s][A
 37%|███▋      | 177736/478625 [00:05<00:09, 32075.42it/s][A
 48%|████▊     | 231883/478625 [00:07<00:07, 31341.72it/s][A
 97%|█████████▋| 465236/478625 [00:14<00:00, 32651.57it/s][A

 38%|███▊      | 181062/478625 [00:05<00:09, 32422.75it/s][A 51%|█████     | 244176/478625 [00:07<00:07, 32399.23it/s][A
 49%|████▉     | 235020/478625 [00:07<00:08, 30446.48it/s][A
 98%|█████████▊| 468505/478625 [00:14<00:00, 32591.75it/s][A
 52%|█████▏    | 247546/478625 [00:07<00:07, 32780.10it/s][A
 39%|███▊      | 184308/478625 [00:05<00:09, 31798.47it/s][A
 50%|████▉     | 238206/478625 [00:07<00:07, 30856.19it/s][A
 99%|█████████▊| 471767/478625 [00:14<00:00, 32132.46it/s][A
 52%|█████▏    | 250828/478625 [00:07<00:07, 32205.20it/s][A
 39%|███▉      | 187636/478625 [00:05<00:09, 32230.03it/s][A
 50%|█████     | 241392/478625 [00:07<00:07, 31148.37it/s][A
 99%|█████████▉| 475135/478625 [00:14<00:00, 32584.85it/s][A
 53%|█████▎    | 254231/478625 [00:07<00:06, 32738.26it/s][A
 40%|███▉      | 190982/478625 [00:05<00:08, 32589.83it/s][A
 51%|█████     | 244512/478625 [00:07<00:07, 30593.37it/s][A
100%|█████████▉| 478488/478625 [00:14<00:00, 32863.89it/s][A100%|██████████| 478625/478625 [00:14<00:00, 32509.51it/s]
100%|██████████| 1/1 [00:21<00:00, 21.30s/it]100%|██████████| 1/1 [00:21<00:00, 21.30s/it]

 54%|█████▍    | 257586/478625 [00:07<00:06, 32975.39it/s][A
 41%|████      | 194245/478625 [00:06<00:08, 32081.45it/s][A
 52%|█████▏    | 247825/478625 [00:08<00:07, 31333.63it/s][A
 41%|████▏     | 197562/478625 [00:06<00:08, 32400.66it/s][A
 55%|█████▍    | 260887/478625 [00:07<00:06, 32224.93it/s][A
 52%|█████▏    | 250964/478625 [00:08<00:07, 30721.95it/s][A
 55%|█████▌    | 264237/478625 [00:08<00:06, 32595.85it/s][A
 42%|████▏     | 200806/478625 [00:06<00:08, 31936.43it/s][A
 53%|█████▎    | 254181/478625 [00:08<00:07, 31143.11it/s][A
 56%|█████▌    | 267608/478625 [00:08<00:06, 32922.60it/s][A
 43%|████▎     | 204104/478625 [00:06<00:08, 32242.43it/s][A
 54%|█████▍    | 257342/478625 [00:08<00:07, 31279.80it/s][A
 43%|████▎     | 207379/478625 [00:06<00:08, 32390.76it/s][A
 57%|█████▋    | 270905/478625 [00:08<00:06, 32290.56it/s][A
 54%|█████▍    | 260474/478625 [00:08<00:07, 30591.65it/s][A
 57%|█████▋    | 274249/478625 [00:08<00:06, 32626.78it/s][A
 44%|████▍     | 210621/478625 [00:06<00:08, 31918.08it/s][A
 55%|█████▌    | 263576/478625 [00:08<00:07, 30715.13it/s][A
 45%|████▍     | 213921/478625 [00:06<00:08, 32234.74it/s][A
 58%|█████▊    | 277516/478625 [00:08<00:06, 32114.13it/s][Atime 21.969990730285645

 56%|█████▌    | 266825/478625 [00:08<00:06, 31237.75it/s][An_elements: 474899
data length: 474899

 59%|█████▊    | 280890/478625 [00:08<00:06, 32587.94it/s][A
 45%|████▌     | 217147/478625 [00:06<00:08, 31756.58it/s][A
 56%|█████▋    | 269953/478625 [00:08<00:06, 30723.18it/s][A
 59%|█████▉    | 284250/478625 [00:08<00:05, 32885.40it/s][A
 46%|████▌     | 220488/478625 [00:06<00:08, 32241.98it/s][A
 57%|█████▋    | 273250/478625 [00:08<00:06, 31352.81it/s][A
 47%|████▋     | 223814/478625 [00:06<00:07, 32542.53it/s][A
 60%|██████    | 287542/478625 [00:08<00:05, 32349.07it/s][A
 58%|█████▊    | 276390/478625 [00:08<00:06, 30655.46it/s][A
 61%|██████    | 290888/478625 [00:08<00:05, 32675.45it/s][A
 47%|████▋     | 227071/478625 [00:07<00:07, 31965.37it/s][A
 58%|█████▊    | 279565/478625 [00:09<00:06, 30973.26it/s][A
 48%|████▊     | 230324/478625 [00:07<00:07, 32130.96it/s][A
 61%|██████▏   | 294159/478625 [00:09<00:05, 31924.08it/s][A
 59%|█████▉    | 282867/478625 [00:09<00:06, 31574.59it/s][A
 49%|████▉     | 233638/478625 [00:07<00:07, 32427.42it/s][A
 62%|██████▏   | 297528/478625 [00:09<00:05, 32438.64it/s][A
 60%|█████▉    | 286029/478625 [00:09<00:06, 30439.45it/s][A
 49%|████▉     | 236884/478625 [00:07<00:07, 31934.13it/s][A
 63%|██████▎   | 300886/478625 [00:09<00:05, 32772.04it/s][A
 60%|██████    | 289084/478625 [00:09<00:06, 30420.85it/s][A
 50%|█████     | 240203/478625 [00:07<00:07, 32302.98it/s][A
 64%|██████▎   | 304168/478625 [00:09<00:05, 32321.29it/s][A
 61%|██████    | 292146/478625 [00:09<00:06, 30477.64it/s][A
 51%|█████     | 243437/478625 [00:07<00:07, 31835.66it/s][A
 64%|██████▍   | 307541/478625 [00:09<00:05, 32734.56it/s][A
 62%|██████▏   | 295200/478625 [00:09<00:06, 30203.82it/s][A
 52%|█████▏    | 246751/478625 [00:07<00:07, 32218.78it/s][A
 65%|██████▍   | 310818/478625 [00:09<00:05, 32213.17it/s][A
 62%|██████▏   | 298514/478625 [00:09<00:05, 31066.72it/s][A
 52%|█████▏    | 250069/478625 [00:07<00:07, 32501.74it/s][A
 66%|██████▌   | 314193/478625 [00:09<00:05, 32663.26it/s][A
 63%|██████▎   | 301626/478625 [00:09<00:05, 30811.74it/s][A
 53%|█████▎    | 253322/478625 [00:07<00:07, 31920.31it/s][A
 66%|██████▋   | 317576/478625 [00:09<00:04, 33005.57it/s][A
 64%|██████▎   | 304916/478625 [00:09<00:05, 31417.10it/s][A
 54%|█████▎    | 256646/478625 [00:07<00:06, 32305.75it/s][A
 67%|██████▋   | 320880/478625 [00:09<00:04, 32313.63it/s][A
 64%|██████▍   | 308094/478625 [00:09<00:05, 31521.14it/s][A
 54%|█████▍    | 259880/478625 [00:08<00:06, 31605.79it/s][A
 68%|██████▊   | 324260/478625 [00:09<00:04, 32748.30it/s][A
 65%|██████▌   | 311249/478625 [00:10<00:05, 30230.60it/s][A
 55%|█████▍    | 263215/478625 [00:08<00:06, 32114.04it/s][A
 68%|██████▊   | 327540/478625 [00:10<00:04, 31945.40it/s][A
 66%|██████▌   | 314362/478625 [00:10<00:05, 30489.24it/s][A
 56%|█████▌    | 266549/478625 [00:08<00:06, 32472.84it/s][A
 69%|██████▉   | 330855/478625 [00:10<00:04, 32295.74it/s][A
 66%|██████▋   | 317574/478625 [00:10<00:05, 30930.12it/s][A
 56%|█████▋    | 269801/478625 [00:08<00:06, 31881.22it/s][A
 70%|██████▉   | 334227/478625 [00:10<00:04, 32713.22it/s][A
 67%|██████▋   | 320675/478625 [00:10<00:05, 30419.13it/s][A
 57%|█████▋    | 273136/478625 [00:08<00:06, 32311.70it/s][A
 71%|███████   | 337503/478625 [00:10<00:04, 32154.69it/s][A
 68%|██████▊   | 323966/478625 [00:10<00:04, 31145.57it/s][A
 58%|█████▊    | 276372/478625 [00:08<00:06, 31816.92it/s][A
 71%|███████   | 340892/478625 [00:10<00:04, 32661.69it/s][A
 68%|██████▊   | 327087/478625 [00:10<00:04, 30718.78it/s][A
 58%|█████▊    | 279695/478625 [00:08<00:06, 32228.35it/s][A
 72%|███████▏  | 344163/478625 [00:10<00:04, 32213.41it/s][A
 69%|██████▉   | 330293/478625 [00:10<00:04, 31109.81it/s][A
 59%|█████▉    | 283034/478625 [00:08<00:06, 32569.13it/s][A
 73%|███████▎  | 347539/478625 [00:10<00:04, 32664.61it/s][A
 70%|██████▉   | 333589/478625 [00:10<00:04, 31652.85it/s][A
 60%|█████▉    | 286295/478625 [00:08<00:06, 32008.61it/s][A
 73%|███████▎  | 350924/478625 [00:10<00:03, 33013.69it/s][A
 70%|███████   | 336759/478625 [00:10<00:04, 31174.99it/s][A
 61%|██████    | 289605/478625 [00:09<00:05, 32327.93it/s][A
 74%|███████▍  | 354229/478625 [00:10<00:03, 32496.34it/s][A
 71%|███████   | 340069/478625 [00:10<00:04, 31740.35it/s][A
 61%|██████    | 292852/478625 [00:09<00:05, 32368.50it/s][A
 75%|███████▍  | 357627/478625 [00:10<00:03, 32932.53it/s][A
 72%|███████▏  | 343355/478625 [00:11<00:04, 32069.10it/s][A
 62%|██████▏   | 296092/478625 [00:09<00:05, 31899.56it/s][A
 75%|███████▌  | 360924/478625 [00:11<00:03, 32411.09it/s][A
 72%|███████▏  | 346566/478625 [00:11<00:04, 31409.93it/s][A
 63%|██████▎   | 299416/478625 [00:09<00:05, 32292.01it/s][A
 76%|███████▌  | 364311/478625 [00:11<00:03, 32839.21it/s][A
 73%|███████▎  | 349872/478625 [00:11<00:04, 31893.19it/s][A
 63%|██████▎   | 302648/478625 [00:09<00:05, 31841.38it/s][A
 77%|███████▋  | 367631/478625 [00:11<00:03, 32942.73it/s][A
 74%|███████▍  | 353066/478625 [00:11<00:04, 31214.54it/s][A
 64%|██████▍   | 305963/478625 [00:09<00:05, 32225.27it/s][A
 77%|███████▋  | 370928/478625 [00:11<00:03, 32237.88it/s][A
 74%|███████▍  | 356369/478625 [00:11<00:03, 31744.37it/s][A
 65%|██████▍   | 309280/478625 [00:09<00:05, 32504.39it/s][A
 78%|███████▊  | 374323/478625 [00:11<00:03, 32739.24it/s][A
 75%|███████▌  | 359683/478625 [00:11<00:03, 32154.92it/s][A
 65%|██████▌   | 312533/478625 [00:09<00:05, 31980.26it/s][A
 79%|███████▉  | 377602/478625 [00:11<00:03, 32257.87it/s][A
 76%|███████▌  | 362903/478625 [00:11<00:03, 31504.61it/s][A
 66%|██████▌   | 315855/478625 [00:09<00:05, 32344.91it/s][A
 80%|███████▉  | 380982/478625 [00:11<00:02, 32708.68it/s][A
 77%|███████▋  | 366157/478625 [00:11<00:03, 31804.75it/s][A
 67%|██████▋   | 319093/478625 [00:09<00:05, 31842.45it/s][A
 80%|████████  | 384385/478625 [00:11<00:02, 33098.39it/s][A
 77%|███████▋  | 369342/478625 [00:11<00:03, 30842.49it/s][A
 67%|██████▋   | 322424/478625 [00:10<00:04, 32273.42it/s][A
 81%|████████  | 387699/478625 [00:11<00:02, 32568.60it/s][A
 78%|███████▊  | 372486/478625 [00:12<00:03, 31014.49it/s][A
 68%|██████▊   | 325718/478625 [00:10<00:04, 32469.99it/s][A
 82%|████████▏ | 391083/478625 [00:11<00:02, 32940.20it/s][A
 78%|███████▊  | 375709/478625 [00:12<00:03, 31369.09it/s][A
 69%|██████▊   | 328968/478625 [00:10<00:04, 31634.38it/s][A
 82%|████████▏ | 394381/478625 [00:12<00:02, 32452.21it/s][A
 79%|███████▉  | 378852/478625 [00:12<00:03, 30868.88it/s][A
 69%|██████▉   | 332300/478625 [00:10<00:04, 32124.48it/s][A
 83%|████████▎ | 397769/478625 [00:12<00:02, 32870.30it/s][A
 80%|███████▉  | 381944/478625 [00:12<00:03, 30880.44it/s][A
 70%|███████   | 335518/478625 [00:10<00:04, 31674.95it/s][A
 84%|████████▍ | 401159/478625 [00:12<00:02, 33171.64it/s][A
 80%|████████  | 385150/478625 [00:12<00:02, 31226.34it/s][A
 71%|███████   | 338834/478625 [00:10<00:04, 32108.89it/s][A
 85%|████████▍ | 404479/478625 [00:12<00:02, 32634.74it/s][A
 81%|████████  | 388276/478625 [00:12<00:02, 30624.76it/s][A
 71%|███████▏  | 342178/478625 [00:10<00:04, 32500.28it/s][A
 85%|████████▌ | 407830/478625 [00:12<00:02, 32890.31it/s][A
 82%|████████▏ | 391565/478625 [00:12<00:02, 31288.38it/s][A
 72%|███████▏  | 345432/478625 [00:10<00:04, 31913.67it/s][A
 86%|████████▌ | 411141/478625 [00:12<00:02, 32408.27it/s][A
 82%|████████▏ | 394699/478625 [00:12<00:02, 29844.56it/s][A
 73%|███████▎  | 348752/478625 [00:10<00:04, 32289.12it/s][A
 87%|████████▋ | 414512/478625 [00:12<00:01, 32788.65it/s][A
 83%|████████▎ | 397980/478625 [00:12<00:02, 30693.88it/s][A
 87%|████████▋ | 417794/478625 [00:12<00:01, 32751.00it/s][A
 74%|███████▎  | 352050/478625 [00:10<00:03, 31809.27it/s][A
 84%|████████▍ | 401201/478625 [00:12<00:02, 31133.52it/s][A
 74%|███████▍  | 355388/478625 [00:11<00:03, 32266.60it/s][A
 88%|████████▊ | 421072/478625 [00:12<00:01, 32325.93it/s][A
 84%|████████▍ | 404327/478625 [00:13<00:02, 30912.55it/s][A
 75%|███████▍  | 358702/478625 [00:11<00:03, 32521.40it/s][A
 89%|████████▊ | 424476/478625 [00:13<00:01, 32830.49it/s][A
 85%|████████▌ | 407595/478625 [00:13<00:02, 31430.61it/s][A
 89%|████████▉ | 427853/478625 [00:13<00:01, 33105.71it/s][A
 76%|███████▌  | 361958/478625 [00:11<00:03, 31976.32it/s][A
 86%|████████▌ | 410901/478625 [00:13<00:02, 31909.86it/s][A
 76%|███████▋  | 365302/478625 [00:11<00:03, 32405.67it/s][A
 90%|█████████ | 431166/478625 [00:13<00:01, 32553.36it/s][A
 87%|████████▋ | 414098/478625 [00:13<00:02, 31173.72it/s][A
 77%|███████▋  | 368595/478625 [00:11<00:03, 32557.94it/s][A
 91%|█████████ | 434542/478625 [00:13<00:01, 32907.62it/s][A
 87%|████████▋ | 417291/478625 [00:13<00:01, 31393.50it/s][A
 78%|███████▊  | 371854/478625 [00:11<00:03, 31737.56it/s][A
 91%|█████████▏| 437836/478625 [00:13<00:01, 32439.01it/s][A
 88%|████████▊ | 420436/478625 [00:13<00:01, 30704.51it/s][A
 78%|███████▊  | 375208/478625 [00:11<00:03, 32263.85it/s][A
 92%|█████████▏| 441237/478625 [00:13<00:01, 32898.65it/s][A
 88%|████████▊ | 423513/478625 [00:13<00:01, 30675.73it/s][A
 93%|█████████▎| 444600/478625 [00:13<00:01, 33112.17it/s][A
 79%|███████▉  | 378440/478625 [00:11<00:03, 31749.21it/s][A
 89%|████████▉ | 426654/478625 [00:13<00:01, 30889.34it/s][A

 94%|█████████▎| 447914/478625 [00:13<00:00, 32606.97it/s][A 80%|███████▉  | 381771/478625 [00:11<00:03, 32203.61it/s][A
 90%|████████▉ | 429747/478625 [00:13<00:01, 30315.30it/s][A
 94%|█████████▍| 451279/478625 [00:13<00:00, 32911.14it/s][A
 80%|████████  | 385102/478625 [00:11<00:02, 32493.94it/s][A
 90%|█████████ | 432904/478625 [00:13<00:01, 30682.25it/s][A
 95%|█████████▍| 454573/478625 [00:13<00:00, 32471.51it/s][A
 81%|████████  | 388355/478625 [00:12<00:02, 32011.67it/s][A
 91%|█████████ | 436187/478625 [00:14<00:01, 31312.81it/s][A
 96%|█████████▌| 457956/478625 [00:14<00:00, 32869.67it/s][A
 82%|████████▏ | 391675/478625 [00:12<00:02, 32358.69it/s][A
 92%|█████████▏| 439322/478625 [00:14<00:01, 30531.38it/s][A
 96%|█████████▋| 461358/478625 [00:14<00:00, 33207.92it/s][A
 83%|████████▎ | 394915/478625 [00:12<00:02, 31913.21it/s][A
 92%|█████████▏| 442505/478625 [00:14<00:01, 30910.28it/s][A
 83%|████████▎ | 398229/478625 [00:12<00:02, 32270.22it/s][A
 97%|█████████▋| 464682/478625 [00:14<00:00, 32504.19it/s][A
 93%|█████████▎| 445602/478625 [00:14<00:01, 30123.89it/s][A
 84%|████████▍ | 401554/478625 [00:12<00:02, 32559.14it/s][A
 98%|█████████▊| 467977/478625 [00:14<00:00, 32633.15it/s][A
 94%|█████████▍| 448929/478625 [00:14<00:00, 31037.89it/s][A
 98%|█████████▊| 471244/478625 [00:14<00:00, 32221.56it/s][A
 85%|████████▍ | 404813/478625 [00:12<00:02, 31968.75it/s][A
 94%|█████████▍| 452103/478625 [00:14<00:00, 31242.15it/s][A
 99%|█████████▉| 474638/478625 [00:14<00:00, 32725.06it/s][A
 85%|████████▌ | 408142/478625 [00:12<00:02, 32355.08it/s][A
 95%|█████████▌| 455234/478625 [00:14<00:00, 30807.49it/s][A
100%|█████████▉| 478023/478625 [00:14<00:00, 33055.34it/s][A
 86%|████████▌ | 411381/478625 [00:12<00:02, 31820.24it/s][A100%|██████████| 478625/478625 [00:14<00:00, 32664.25it/s]
100%|██████████| 1/1 [00:21<00:00, 21.22s/it]100%|██████████| 1/1 [00:21<00:00, 21.22s/it]

 96%|█████████▌| 458461/478625 [00:14<00:00, 31233.26it/s][A
 87%|████████▋ | 414675/478625 [00:12<00:01, 32146.69it/s][A
 96%|█████████▋| 461706/478625 [00:14<00:00, 31589.68it/s][A
 87%|████████▋ | 417898/478625 [00:12<00:01, 32169.48it/s][A
 97%|█████████▋| 464869/478625 [00:15<00:00, 30953.41it/s][A
 88%|████████▊ | 421118/478625 [00:13<00:01, 31746.88it/s][A
 98%|█████████▊| 468072/478625 [00:15<00:00, 31267.45it/s][A
 89%|████████▊ | 424460/478625 [00:13<00:01, 32238.44it/s][A
 98%|█████████▊| 471203/478625 [00:15<00:00, 30954.06it/s][A
 89%|████████▉ | 427770/478625 [00:13<00:01, 32490.40it/s][A
 99%|█████████▉| 474474/478625 [00:15<00:00, 31470.14it/s][A
 90%|█████████ | 431022/478625 [00:13<00:01, 31919.50it/s][Atime 21.885778665542603

100%|█████████▉| 477625/478625 [00:15<00:00, 31215.95it/s][An_elements: 474899
data length: 474899

 91%|█████████ | 434331/478625 [00:13<00:01, 32251.39it/s][A100%|██████████| 478625/478625 [00:15<00:00, 30975.81it/s]
100%|██████████| 1/1 [00:22<00:00, 22.07s/it]100%|██████████| 1/1 [00:22<00:00, 22.07s/it]

 91%|█████████▏| 437560/478625 [00:13<00:01, 31807.88it/s][A
 92%|█████████▏| 440879/478625 [00:13<00:01, 32212.08it/s][A
 93%|█████████▎| 444182/478625 [00:13<00:01, 32426.16it/s][A
 93%|█████████▎| 447427/478625 [00:13<00:00, 31964.33it/s][A
 94%|█████████▍| 450728/478625 [00:14<00:00, 32268.89it/s][A
 95%|█████████▍| 453958/478625 [00:14<00:00, 31771.57it/s][Atime 22.72363829612732
n_elements: 474899
data length: 474899

 96%|█████████▌| 457269/478625 [00:14<00:00, 32162.75it/s][A
 96%|█████████▌| 460590/478625 [00:14<00:00, 32470.05it/s][A
 97%|█████████▋| 463840/478625 [00:14<00:00, 31817.88it/s][A
 98%|█████████▊| 467049/478625 [00:14<00:00, 31893.51it/s][A
 98%|█████████▊| 470242/478625 [00:14<00:00, 31562.72it/s][A
 99%|█████████▉| 473571/478625 [00:14<00:00, 32070.16it/s][A
100%|█████████▉| 476892/478625 [00:14<00:00, 32406.93it/s][A100%|██████████| 478625/478625 [00:14<00:00, 32162.17it/s]
100%|██████████| 1/1 [00:21<00:00, 21.45s/it]100%|██████████| 1/1 [00:21<00:00, 21.45s/it]
time 22.10815191268921
n_elements: 474899
data length: 474899
[2024-09-09 12:30:33,533] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-09-09 12:30:33,538] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-09-09 12:30:33,538] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
zp rank is 6, zp_size=8
zp rank is 3, zp_size=8
zp rank is 7, zp_size=8
zp rank is 2, zp_size=8
zp rank is 1, zp_size=8
zp rank is 4, zp_size=8
zp rank is 5, zp_size=8
[2024-09-09 12:30:33,689] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-09-09 12:30:33,690] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch_npu.utils._optim.partialclass.<locals>.NewCls'>
[2024-09-09 12:30:33,690] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-09-09 12:30:33,690] [INFO] [stage_1_and_2.py:173:__init__] Reduce bucket size 536870912
[2024-09-09 12:30:33,690] [INFO] [stage_1_and_2.py:174:__init__] Allgather bucket size 536870912
[2024-09-09 12:30:33,690] [INFO] [stage_1_and_2.py:175:__init__] CPU Offload: False
[2024-09-09 12:30:33,690] [INFO] [stage_1_and_2.py:176:__init__] Round robin gradient partitioning: False
zp rank is 0, zp_size=8
[] -> [195210]
[] -> [195210]
[] -> [195210]
[] -> [195210]
[] -> [164918]
[2024-09-09 12:30:39,057] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
[2024-09-09 12:30:39,058] [INFO] [utils.py:792:see_memory_usage] MA 17.78 GB         Max_MA 18.44 GB         CA 18.78 GB         Max_CA 19 GB 
[2024-09-09 12:30:39,059] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 202.78 GB, percent = 13.4%
[] -> [195210]
[] -> [184079]
[] -> [164918]
[] -> [195210]
[] -> [184079]
[] -> [164918]
[] -> [164918]
[] -> [164918]
[] -> [164918]
[] -> [184079]
[] -> [195210]
[] -> [184079]
[] -> [164918]
[] -> [184079]
[] -> [195210]
[] -> [164918]
[] -> [195210]
[] -> [184079]
[] -> [184079]
[] -> [164918]
[] -> [184079]
[] -> [184079]
[] -> [184079]
[] -> [184079]
[] -> [184079]
[] -> [184079]
[] -> [184079]
[] -> [184079]
[] -> [184079]
[] -> [184079]
[2024-09-09 12:30:40,996] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
[2024-09-09 12:30:40,997] [INFO] [utils.py:792:see_memory_usage] MA 20.41 GB         Max_MA 24.35 GB         CA 25.36 GB         Max_CA 25 GB 
[2024-09-09 12:30:40,997] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 203.0 GB, percent = 13.4%
[2024-09-09 12:30:40,997] [INFO] [stage_1_and_2.py:552:__init__] optimizer state initialized
shuffled_megabatches 59363
have been trained idx: 0
after shuffled_megabatches 59363
shuffled_megabatches 59363
have been trained idx: 0
after shuffled_megabatches 59363
shuffled_megabatches 59363
have been trained idx: 0
after shuffled_megabatches 59363
shuffled_megabatches 59363
have been trained idx: 0
after shuffled_megabatches 59363
shuffled_megabatches 59363
have been trained idx: 0
after shuffled_megabatches 59363
shuffled_megabatches 59363
have been trained idx: 0
after shuffled_megabatches 59363
shuffled_megabatches 59363
have been trained idx: 0
after shuffled_megabatches 59363
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 941, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 770, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 743, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/dataset_utils.py", line 94, in __call__
    masked_video,video,mask = self.mask_processor(pixed_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 535, in __call__
    masked_pixel_values,mask = self.mask_functions[mask_func_name](pixel_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 470, in fixed_mask
    return get_mask_tensor(video_tensor,MaskType.fixed_mask,self.yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 382, in get_mask_tensor
    masked_video_container,masks_container = get_mask(video_tensor,mask_type,yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 209, in get_mask
    result = tracker.track(frame_tensor,save=False, retina_masks=True, agnostic_nms=True,half=True,verbose=False)
NameError: name 'tracker' is not defined
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 941, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 770, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 743, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/dataset_utils.py", line 94, in __call__
    masked_video,video,mask = self.mask_processor(pixed_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 535, in __call__
    masked_pixel_values,mask = self.mask_functions[mask_func_name](pixel_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 476, in fixed_bg_mask
    return get_mask_tensor(video_tensor,MaskType.fixed_bg_mask,self.yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 382, in get_mask_tensor
    masked_video_container,masks_container = get_mask(video_tensor,mask_type,yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 209, in get_mask
    result = tracker.track(frame_tensor,save=False, retina_masks=True, agnostic_nms=True,half=True,verbose=False)
NameError: name 'tracker' is not defined
[2024-09-09 12:30:42,898] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
[2024-09-09 12:30:42,899] [INFO] [utils.py:792:see_memory_usage] MA 20.41 GB         Max_MA 20.41 GB         CA 25.36 GB         Max_CA 25 GB 
[2024-09-09 12:30:42,899] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 195.77 GB, percent = 13.0%
[2024-09-09 12:30:42,908] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-09-09 12:30:42,908] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-09-09 12:30:42,909] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-09 12:30:42,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.999)]
[2024-09-09 12:30:42,912] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2024-09-09 12:30:42,912] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-09-09 12:30:42,912] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-09-09 12:30:42,912] [INFO] [config.py:988:print]   amp_enabled .................. False
[2024-09-09 12:30:42,912] [INFO] [config.py:988:print]   amp_params ................... False
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0xffff0e7a7970>
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   communication_data_type ...... torch.float32
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2024-09-09 12:30:42,913] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   disable_allgather ............ False
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   dump_state ................... False
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   fp16_enabled ................. False
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   global_rank .................. 0
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2024-09-09 12:30:42,914] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   optimizer_name ............... None
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   optimizer_params ............. None
[2024-09-09 12:30:42,915] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   pld_enabled .................. False
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   pld_params ................... False
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   scheduler_name ............... None
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   scheduler_params ............. None
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   sparse_attention ............. None
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   steps_per_print .............. inf
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   train_batch_size ............. 8
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  1
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   world_size ................... 8
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=536870912 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=536870912 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-09 12:30:42,916] [INFO] [config.py:988:print]   zero_enabled ................. True
[2024-09-09 12:30:42,917] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-09 12:30:42,917] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2
[2024-09-09 12:30:42,917] [INFO] [config.py:974:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "communication_data_type": "fp32", 
    "gradient_clipping": 1.0, 
    "train_micro_batch_size_per_gpu": 1, 
    "train_batch_size": 8, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "overlap_comm": true, 
        "allgather_bucket_size": 5.368709e+08, 
        "contiguous_gradients": true, 
        "reduce_bucket_size": 5.368709e+08
    }, 
    "steps_per_print": inf, 
    "zero_allow_untested_optimizer": true
}
09/09/2024 12:30:42 - INFO - __main__ - after accelerator.prepare
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 941, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 770, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 743, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/dataset_utils.py", line 94, in __call__
    masked_video,video,mask = self.mask_processor(pixed_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 535, in __call__
    masked_pixel_values,mask = self.mask_functions[mask_func_name](pixel_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 476, in fixed_bg_mask
    return get_mask_tensor(video_tensor,MaskType.fixed_bg_mask,self.yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 382, in get_mask_tensor
    masked_video_container,masks_container = get_mask(video_tensor,mask_type,yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 209, in get_mask
    result = tracker.track(frame_tensor,save=False, retina_masks=True, agnostic_nms=True,half=True,verbose=False)
NameError: name 'tracker' is not defined
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 941, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 770, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 743, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/dataset_utils.py", line 94, in __call__
    masked_video,video,mask = self.mask_processor(pixed_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 535, in __call__
    masked_pixel_values,mask = self.mask_functions[mask_func_name](pixel_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 467, in background_mask
    return get_mask_tensor(video_tensor,MaskType.background_mask,self.yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 382, in get_mask_tensor
    masked_video_container,masks_container = get_mask(video_tensor,mask_type,yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 209, in get_mask
    result = tracker.track(frame_tensor,save=False, retina_masks=True, agnostic_nms=True,half=True,verbose=False)
NameError: name 'tracker' is not defined
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 941, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 770, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 743, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/dataset_utils.py", line 94, in __call__
    masked_video,video,mask = self.mask_processor(pixed_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 535, in __call__
    masked_pixel_values,mask = self.mask_functions[mask_func_name](pixel_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 461, in Semantic_mask
    return get_mask_tensor(video_tensor,MaskType.Semantic_mask,self.yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 382, in get_mask_tensor
    masked_video_container,masks_container = get_mask(video_tensor,mask_type,yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 209, in get_mask
    result = tracker.track(frame_tensor,save=False, retina_masks=True, agnostic_nms=True,half=True,verbose=False)
NameError: name 'tracker' is not defined
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 941, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 770, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 743, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/dataset_utils.py", line 94, in __call__
    masked_video,video,mask = self.mask_processor(pixed_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 535, in __call__
    masked_pixel_values,mask = self.mask_functions[mask_func_name](pixel_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 464, in bbox_mask
    return get_mask_tensor(video_tensor,MaskType.bbox_mask,self.yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 382, in get_mask_tensor
    masked_video_container,masks_container = get_mask(video_tensor,mask_type,yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 209, in get_mask
    result = tracker.track(frame_tensor,save=False, retina_masks=True, agnostic_nms=True,half=True,verbose=False)
NameError: name 'tracker' is not defined
09/09/2024 12:30:43 - INFO - __main__ - init trackers...
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 941, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 770, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 743, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/dataset_utils.py", line 94, in __call__
    masked_video,video,mask = self.mask_processor(pixed_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 535, in __call__
    masked_pixel_values,mask = self.mask_functions[mask_func_name](pixel_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 476, in fixed_bg_mask
    return get_mask_tensor(video_tensor,MaskType.fixed_bg_mask,self.yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 382, in get_mask_tensor
    masked_video_container,masks_container = get_mask(video_tensor,mask_type,yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 209, in get_mask
    result = tracker.track(frame_tensor,save=False, retina_masks=True, agnostic_nms=True,half=True,verbose=False)
NameError: name 'tracker' is not defined
wandb: Currently logged in as: pkuhxy (pkuhxy-Peking University). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/image_data/hxy/Open-Sora-Plan/wandb/run-20240909_123048-adfh19sk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-firefly-2
wandb: ⭐️ View project at https://wandb.ai/pkuhxy-Peking%20University/inpaint_93x320x320_stage1
wandb: 🚀 View run at https://wandb.ai/pkuhxy-Peking%20University/inpaint_93x320x320_stage1/runs/adfh19sk
09/09/2024 12:30:50 - INFO - __main__ - ***** Running training *****
09/09/2024 12:30:50 - INFO - __main__ -   Model = DeepSpeedEngine(
  (module): OpenSoraInpaint(
    (pos_embed): PatchEmbed2D(
      (proj): Conv2d(8, 2304, kernel_size=(2, 2), stride=(2, 2))
    )
    (transformer_blocks): ModuleList(
      (0-31): 32 x BasicTransformerBlock(
        (norm1): LayerNorm((2304,), eps=1e-06, elementwise_affine=False)
        (attn1): Attention(
          (to_q): Linear(in_features=2304, out_features=2304, bias=True)
          (to_k): Linear(in_features=2304, out_features=2304, bias=True)
          (to_v): Linear(in_features=2304, out_features=2304, bias=True)
          (to_out): ModuleList(
            (0): Linear(in_features=2304, out_features=2304, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((2304,), eps=1e-06, elementwise_affine=False)
        (attn2): Attention(
          (to_q): Linear(in_features=2304, out_features=2304, bias=True)
          (to_k): Linear(in_features=2304, out_features=2304, bias=True)
          (to_v): Linear(in_features=2304, out_features=2304, bias=True)
          (to_out): ModuleList(
            (0): Linear(in_features=2304, out_features=2304, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ff): FeedForward(
          (net): ModuleList(
            (0): GELU(
              (proj): Linear(in_features=2304, out_features=9216, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=9216, out_features=2304, bias=True)
          )
        )
      )
    )
    (norm_out): LayerNorm((2304,), eps=1e-06, elementwise_affine=False)
    (proj_out): Linear(in_features=2304, out_features=32, bias=True)
    (adaln_single): AdaLayerNormSingle(
      (emb): PixArtAlphaCombinedTimestepSizeEmbeddings(
        (time_proj): Timesteps()
        (timestep_embedder): TimestepEmbedding(
          (linear_1): Linear(in_features=256, out_features=2304, bias=True)
          (act): SiLU()
          (linear_2): Linear(in_features=2304, out_features=2304, bias=True)
        )
      )
      (silu): SiLU()
      (linear): Linear(in_features=2304, out_features=13824, bias=True)
    )
    (caption_projection): PixArtAlphaTextProjection(
      (linear_1): Linear(in_features=4096, out_features=2304, bias=True)
      (act_1): GELU(approximate='tanh')
      (linear_2): Linear(in_features=2304, out_features=2304, bias=True)
    )
    (motion_projection): MotionAdaLayerNormSingle(
      (emb): MotionEmbeddings(
        (motion_proj): Timesteps()
        (motion_embedder): TimestepEmbedding(
          (linear_1): Linear(in_features=256, out_features=2304, bias=True)
          (act): SiLU()
          (linear_2): Linear(in_features=2304, out_features=2304, bias=True)
        )
      )
      (silu): SiLU()
      (linear): Linear(in_features=2304, out_features=13824, bias=True)
    )
    (pos_embed_mask): ModuleList(
      (0): PatchEmbed2D(
        (proj): Conv2d(4, 2304, kernel_size=(2, 2), stride=(2, 2))
      )
      (1): Linear(in_features=2304, out_features=2304, bias=False)
    )
    (pos_embed_masked_hidden_states): ModuleList(
      (0): PatchEmbed2D(
        (proj): Conv2d(8, 2304, kernel_size=(2, 2), stride=(2, 2))
      )
      (1): Linear(in_features=2304, out_features=2304, bias=False)
    )
  )
)
09/09/2024 12:30:50 - INFO - __main__ -   Num examples = 474899
09/09/2024 12:30:50 - INFO - __main__ -   Num Epochs = 17
09/09/2024 12:30:50 - INFO - __main__ -   Instantaneous batch size per device = 1
09/09/2024 12:30:50 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
09/09/2024 12:30:50 - INFO - __main__ -   Gradient Accumulation steps = 1
09/09/2024 12:30:50 - INFO - __main__ -   Total optimization steps = 1000000
09/09/2024 12:30:50 - INFO - __main__ -   Total optimization steps (num_update_steps_per_epoch) = 59362
09/09/2024 12:30:50 - INFO - __main__ -   Total trainable parameters = 2.8204808 B
Steps:   0%|          | 0/1000000 [00:00<?, ?it/s][] -> [195210]
[] -> [195210]
[] -> [195210]
[] -> [195210]
[] -> [195210]
shuffled_megabatches 59363
have been trained idx: 0
after shuffled_megabatches 59363
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 941, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 770, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 743, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/dataset_utils.py", line 94, in __call__
    masked_video,video,mask = self.mask_processor(pixed_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 535, in __call__
    masked_pixel_values,mask = self.mask_functions[mask_func_name](pixel_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 476, in fixed_bg_mask
    return get_mask_tensor(video_tensor,MaskType.fixed_bg_mask,self.yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 382, in get_mask_tensor
    masked_video_container,masks_container = get_mask(video_tensor,mask_type,yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 209, in get_mask
    result = tracker.track(frame_tensor,save=False, retina_masks=True, agnostic_nms=True,half=True,verbose=False)
NameError: name 'tracker' is not defined
Traceback (most recent call last):
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 941, in <module>
    main(args)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 770, in main
    train_one_epoch()
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/train/train_inpaint.py", line 743, in train_one_epoch
    for step, data_item in enumerate(train_dataloader):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/dataset_utils.py", line 94, in __call__
    masked_video,video,mask = self.mask_processor(pixed_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 535, in __call__
    masked_pixel_values,mask = self.mask_functions[mask_func_name](pixel_values)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 476, in fixed_bg_mask
    return get_mask_tensor(video_tensor,MaskType.fixed_bg_mask,self.yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 382, in get_mask_tensor
    masked_video_container,masks_container = get_mask(video_tensor,mask_type,yolomodel)
  File "/home/image_data/hxy/Open-Sora-Plan/opensora/utils/mask_utils.py", line 209, in get_mask
    result = tracker.track(frame_tensor,save=False, retina_masks=True, agnostic_nms=True,half=True,verbose=False)
NameError: name 'tracker' is not defined
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.020 MB of 0.039 MB uploadedwandb: | 0.020 MB of 0.039 MB uploaded[2024-09-09 12:31:04,770] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 401 closing signal SIGTERM
[2024-09-09 12:31:04,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 403 closing signal SIGTERM
[2024-09-09 12:31:04,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 405 closing signal SIGTERM
wandb: / 0.034 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedProcess ForkServerProcess-2:
Process ForkServerProcess-6:
Process ForkServerProcess-8:
Process ForkServerProcess-3:
Process ForkServerProcess-4:
Process ForkServerProcess-9:
Process ForkServerProcess-5:
Process ForkServerProcess-7:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "<string>", line 2, in get
  File "<string>", line 2, in get
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
EOFError
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
EOFError
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
Traceback (most recent call last):
  File "<string>", line 2, in get
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "<string>", line 2, in get
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
EOFError
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 65, in wrapper
    raise exp
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 62, in wrapper
    func(*args, **kwargs)
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "<string>", line 2, in get
  File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py", line 262, in task_distribute
    key, func_name, detail = resource_proxy[TASK_QUEUE].get()
  File "<string>", line 2, in get
  File "<string>", line 2, in get
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "<string>", line 2, in get
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/managers.py", line 810, in _callmethod
    kind, result = conn.recv()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 255, in recv
    buf = self._recv_bytes()
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
EOFError
EOFError
EOFError
EOFError
scripts/text_condition/npu/train_inpaint_sparse1d_newmodel_motion.sh: line 81:   203 Killed                  accelerate launch --config_file scripts/accelerate_configs/multi_node_example_by_deepspeed.yaml --machine_rank=${MACHINE_RANK} --main_process_ip=${MAIN_PROCESS_IP_VALUE} opensora/train/train_inpaint.py --model OpenSoraInpaint-L/122 --text_encoder_name google/mt5-xxl --cache_dir "../../cache_dir/" --dataset inpaint --data "scripts/train_data/video_data_debug.txt" --ae WFVAEModel_D8_4x8x8 --ae_path "/home/image_data/lb/Open-Sora-Plan/WFVAE_DISTILL_FORMAL" --sample_rate 1 --num_frames 93 --max_height 320 --max_width 320 --interpolation_scale_t 1.0 --interpolation_scale_h 1.0 --interpolation_scale_w 1.0 --attention_mode xformers --gradient_checkpointing --train_batch_size=1 --dataloader_num_workers 0 --gradient_accumulation_steps=1 --max_train_steps=1000000 --learning_rate=1e-5 --lr_scheduler="constant" --lr_warmup_steps=0 --mixed_precision="bf16" --report_to="wandb" --checkpointing_steps=1000 --allow_tf32 --model_max_length 512 --use_image_num 0 --snr_gamma 5.0 --use_ema --ema_start_step 0 --cfg 0.1 --noise_offset 0.0 --use_rope --skip_low_resolution --speed_factor 1.0 --ema_decay 0.9999 --drop_short_ratio 0.0 --hw_stride 32 --sparse1d --sparse_n 4 --use_motion --train_fps 16 --seed 1234 --trained_data_global_step 0 --group_data --use_decord --prediction_type "v_prediction" --rescale_betas_zero_snr --t2v_ratio 0.0 --i2v_ratio 0.0 --transition_ratio 0.0 --v2v_ratio 0.0 --Semantic_ratio 0.2 --bbox_ratio 0.2 --background_ratio 0.2 --fixed_ratio 0.1 --Semantic_expansion_ratio 0.1 --fixed_bg_ratio 0.1 --clear_video_ratio 0.0 --min_clear_ratio 0.25 --default_text_ratio 0.0 --output_dir /home/save_dir/runs/$PROJECT --pretrained_transformer_model_path "/home/image_data/captions/vpre_latest_134k/model_ema" --yolomodel_pathorname "/home/image_data/hxy/Open-Sora-Plan/opensora/dataset/yolov9c-seg.pt"
